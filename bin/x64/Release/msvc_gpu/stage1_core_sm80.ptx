//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	sieve_kernel_trans_pp32_r32

.visible .entry sieve_kernel_trans_pp32_r32(
	.param .u64 sieve_kernel_trans_pp32_r32_param_0,
	.param .u32 sieve_kernel_trans_pp32_r32_param_1,
	.param .u64 sieve_kernel_trans_pp32_r32_param_2,
	.param .u32 sieve_kernel_trans_pp32_r32_param_3,
	.param .u64 sieve_kernel_trans_pp32_r32_param_4,
	.param .u64 sieve_kernel_trans_pp32_r32_param_5,
	.param .u64 sieve_kernel_trans_pp32_r32_param_6,
	.param .u32 sieve_kernel_trans_pp32_r32_param_7,
	.param .u32 sieve_kernel_trans_pp32_r32_param_8,
	.param .u32 sieve_kernel_trans_pp32_r32_param_9,
	.param .u32 sieve_kernel_trans_pp32_r32_param_10,
	.param .u32 sieve_kernel_trans_pp32_r32_param_11
)
{
	.reg .pred 	%p<159>;
	.reg .b32 	%r<964>;
	.reg .b64 	%rd<188>;


	ld.param.u64 	%rd34, [sieve_kernel_trans_pp32_r32_param_0];
	ld.param.u32 	%r228, [sieve_kernel_trans_pp32_r32_param_1];
	ld.param.u64 	%rd35, [sieve_kernel_trans_pp32_r32_param_2];
	ld.param.u32 	%r229, [sieve_kernel_trans_pp32_r32_param_3];
	ld.param.u64 	%rd36, [sieve_kernel_trans_pp32_r32_param_4];
	ld.param.u64 	%rd37, [sieve_kernel_trans_pp32_r32_param_5];
	ld.param.u64 	%rd38, [sieve_kernel_trans_pp32_r32_param_6];
	ld.param.u32 	%r230, [sieve_kernel_trans_pp32_r32_param_7];
	ld.param.u32 	%r231, [sieve_kernel_trans_pp32_r32_param_8];
	ld.param.u32 	%r232, [sieve_kernel_trans_pp32_r32_param_9];
	ld.param.u32 	%r234, [sieve_kernel_trans_pp32_r32_param_11];
	cvta.to.global.u64 	%rd1, %rd36;
	cvta.to.global.u64 	%rd2, %rd35;
	cvta.to.global.u64 	%rd3, %rd38;
	cvta.to.global.u64 	%rd4, %rd37;
	mov.u32 	%r235, %ntid.x;
	mov.u32 	%r236, %ctaid.x;
	mov.u32 	%r237, %tid.x;
	mad.lo.s32 	%r1, %r236, %r235, %r237;
	setp.ge.u32 	%p5, %r1, %r228;
	@%p5 bra 	$L__BB0_109;

	cvta.to.global.u64 	%rd39, %rd34;
	mul.wide.u32 	%rd40, %r1, 4;
	add.s64 	%rd41, %rd39, %rd40;
	ld.global.u32 	%r2, [%rd41];
	mul.lo.s32 	%r3, %r2, %r2;
	add.s32 	%r241, %r3, 2;
	mad.lo.s32 	%r242, %r241, %r3, 2;
	mul.lo.s32 	%r243, %r242, %r241;
	mad.lo.s32 	%r244, %r243, %r3, 2;
	mul.lo.s32 	%r245, %r244, %r243;
	mad.lo.s32 	%r246, %r245, %r3, 2;
	mul.lo.s32 	%r247, %r246, %r245;
	mad.lo.s32 	%r248, %r247, %r3, 2;
	mul.lo.s32 	%r4, %r248, %r247;
	mov.u32 	%r249, %ctaid.y;
	mul.lo.s32 	%r933, %r249, %r231;
	add.s32 	%r250, %r933, %r231;
	min.u32 	%r6, %r250, %r230;
	mul.lo.s32 	%r7, %r232, %r230;
	mad.lo.s32 	%r935, %r933, %r232, %r1;
	mul.wide.u32 	%rd42, %r933, 24;
	add.s64 	%rd182, %rd3, %rd42;
	setp.ge.u32 	%p7, %r933, %r6;
	mov.pred 	%p157, -1;
	mov.u32 	%r877, 0;
	mov.u32 	%r879, %r877;
	@%p7 bra 	$L__BB0_12;

	cvt.u64.u32 	%rd6, %r3;
	mov.u32 	%r252, 0;
	mov.u32 	%r877, %r252;

$L__BB0_3:
	ld.global.u32 	%r13, [%rd182];
	setp.eq.s32 	%p8, %r877, %r13;
	mov.u32 	%r879, %r252;
	@%p8 bra 	$L__BB0_11;

	mov.u32 	%r875, %r2;
	mov.u32 	%r876, %r13;

$L__BB0_5:
	neg.s32 	%r254, %r876;
	and.b32  	%r255, %r876, %r254;
	clz.b32 	%r256, %r255;
	mov.u32 	%r257, 31;
	sub.s32 	%r258, %r257, %r256;
	shr.u32 	%r259, %r876, %r258;
	min.u32 	%r16, %r875, %r259;
	max.u32 	%r260, %r875, %r259;
	sub.s32 	%r876, %r260, %r16;
	setp.ne.s32 	%p9, %r876, 0;
	mov.u32 	%r875, %r16;
	@%p9 bra 	$L__BB0_5;

	setp.ne.s32 	%p10, %r16, 1;
	mov.u32 	%r877, %r13;
	mov.u32 	%r879, %r252;
	@%p10 bra 	$L__BB0_11;

	ld.global.u64 	%rd8, [%rd182+8];
	and.b64  	%rd43, %rd8, -4294967296;
	setp.eq.s64 	%p11, %rd43, 0;
	@%p11 bra 	$L__BB0_9;

	rem.u64 	%rd181, %rd8, %rd6;
	bra.uni 	$L__BB0_10;

$L__BB0_9:
	cvt.u32.u64 	%r262, %rd6;
	cvt.u32.u64 	%r263, %rd8;
	rem.u32 	%r264, %r263, %r262;
	cvt.u64.u32 	%rd181, %r264;

$L__BB0_10:
	cvt.u32.u64 	%r893, %rd181;
	mov.u32 	%r877, %r13;
	mov.u32 	%r879, %r893;

$L__BB0_11:
	mul.wide.u32 	%rd44, %r935, 4;
	add.s64 	%rd45, %rd4, %rd44;
	st.global.u32 	[%rd45], %r879;
	add.s32 	%r935, %r935, %r232;
	add.s64 	%rd182, %rd182, 24;
	add.s32 	%r933, %r933, 1;
	setp.lt.u32 	%p12, %r933, %r6;
	setp.eq.s32 	%p157, %r879, 0;
	and.pred  	%p13, %p12, %p157;
	@%p13 bra 	$L__BB0_3;

$L__BB0_12:
	@%p157 bra 	$L__BB0_109;

	add.s32 	%r29, %r933, -1;
	setp.ge.u32 	%p14, %r933, %r6;
	@%p14 bra 	$L__BB0_24;

	cvt.u64.u32 	%rd14, %r3;

$L__BB0_15:
	ld.global.u32 	%r35, [%rd182];
	setp.eq.s32 	%p15, %r877, %r35;
	@%p15 bra 	$L__BB0_23;

	mov.u32 	%r890, %r2;
	mov.u32 	%r891, %r35;

$L__BB0_17:
	neg.s32 	%r265, %r891;
	and.b32  	%r266, %r891, %r265;
	clz.b32 	%r267, %r266;
	mov.u32 	%r268, 31;
	sub.s32 	%r269, %r268, %r267;
	shr.u32 	%r270, %r891, %r269;
	min.u32 	%r38, %r890, %r270;
	max.u32 	%r271, %r890, %r270;
	sub.s32 	%r891, %r271, %r38;
	setp.ne.s32 	%p16, %r891, 0;
	mov.u32 	%r890, %r38;
	@%p16 bra 	$L__BB0_17;

	setp.ne.s32 	%p17, %r38, 1;
	mov.u32 	%r879, 0;
	mov.u32 	%r877, %r35;
	@%p17 bra 	$L__BB0_23;

	ld.global.u64 	%rd16, [%rd182+8];
	and.b64  	%rd46, %rd16, -4294967296;
	setp.eq.s64 	%p18, %rd46, 0;
	@%p18 bra 	$L__BB0_21;

	rem.u64 	%rd184, %rd16, %rd14;
	bra.uni 	$L__BB0_22;

$L__BB0_21:
	cvt.u32.u64 	%r273, %rd14;
	cvt.u32.u64 	%r274, %rd16;
	rem.u32 	%r275, %r274, %r273;
	cvt.u64.u32 	%rd184, %r275;

$L__BB0_22:
	cvt.u32.u64 	%r285, %rd184;
	mul.lo.s32 	%r276, %r893, %r285;
	mul.hi.u32 	%r277, %r893, %r285;
	mul.lo.s32 	%r286, %r276, %r4;
	mul.lo.s32 	%r279, %r286, %r3;
	mul.hi.u32 	%r280, %r286, %r3;
	mov.u32 	%r278, 0;
	// begin inline asm
	add.cc.u32 %r276, %r276, %r279;   /* inline */   
	addc.cc.u32 %r277, %r277, %r280;   /* inline */   
	addc.u32 %r278, %r278, %r278;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p19, %r278, 0;
	setp.ge.u32 	%p20, %r277, %r3;
	or.pred  	%p21, %p20, %p19;
	selp.b32 	%r287, %r3, 0, %p21;
	sub.s32 	%r893, %r277, %r287;
	mov.u32 	%r877, %r35;
	mov.u32 	%r879, %r893;

$L__BB0_23:
	mul.wide.u32 	%rd47, %r935, 4;
	add.s64 	%rd48, %rd4, %rd47;
	st.global.u32 	[%rd48], %r879;
	add.s32 	%r935, %r935, %r232;
	add.s64 	%rd182, %rd182, 24;
	add.s32 	%r933, %r933, 1;
	setp.lt.u32 	%p22, %r933, %r6;
	@%p22 bra 	$L__BB0_15;

$L__BB0_24:
	setp.lt.u32 	%p24, %r893, 2;
	mov.pred 	%p158, -1;
	mov.u32 	%r905, 1;
	@%p24 bra 	$L__BB0_38;

	mov.u32 	%r905, 1;
	mov.u32 	%r898, 0;
	mov.u32 	%r904, %r893;
	mov.u32 	%r901, %r898;
	mov.u32 	%r902, %r3;

$L__BB0_26:
	mov.u32 	%r51, %r902;
	mov.u32 	%r50, %r901;
	mov.u32 	%r901, %r905;
	mov.u32 	%r902, %r904;
	mov.u32 	%r49, %r898;
	sub.s32 	%r904, %r51, %r902;
	sub.s32 	%r55, %r904, %r902;
	setp.lt.u32 	%p25, %r904, %r902;
	mov.u32 	%r903, %r901;
	@%p25 bra 	$L__BB0_36;

	shl.b32 	%r903, %r901, 1;
	sub.s32 	%r57, %r55, %r902;
	setp.lt.u32 	%p26, %r55, %r902;
	mov.u32 	%r904, %r55;
	@%p26 bra 	$L__BB0_36;

	mul.lo.s32 	%r903, %r901, 3;
	sub.s32 	%r59, %r57, %r902;
	setp.lt.u32 	%p27, %r57, %r902;
	mov.u32 	%r904, %r57;
	@%p27 bra 	$L__BB0_36;

	shl.b32 	%r903, %r901, 2;
	sub.s32 	%r61, %r59, %r902;
	setp.lt.u32 	%p28, %r59, %r902;
	mov.u32 	%r904, %r59;
	@%p28 bra 	$L__BB0_36;

	mul.lo.s32 	%r903, %r901, 5;
	sub.s32 	%r63, %r61, %r902;
	setp.lt.u32 	%p29, %r61, %r902;
	mov.u32 	%r904, %r61;
	@%p29 bra 	$L__BB0_36;

	mul.lo.s32 	%r903, %r901, 6;
	sub.s32 	%r65, %r63, %r902;
	setp.lt.u32 	%p30, %r63, %r902;
	mov.u32 	%r904, %r63;
	@%p30 bra 	$L__BB0_36;

	mul.lo.s32 	%r903, %r901, 7;
	sub.s32 	%r67, %r65, %r902;
	setp.lt.u32 	%p31, %r65, %r902;
	mov.u32 	%r904, %r65;
	@%p31 bra 	$L__BB0_36;

	shl.b32 	%r903, %r901, 3;
	sub.s32 	%r69, %r67, %r902;
	setp.lt.u32 	%p32, %r67, %r902;
	mov.u32 	%r904, %r67;
	@%p32 bra 	$L__BB0_36;

	mul.lo.s32 	%r903, %r901, 9;
	setp.lt.u32 	%p33, %r69, %r902;
	mov.u32 	%r904, %r69;
	@%p33 bra 	$L__BB0_36;

	div.u32 	%r292, %r51, %r902;
	mul.lo.s32 	%r293, %r292, %r902;
	sub.s32 	%r904, %r51, %r293;
	mul.lo.s32 	%r903, %r292, %r901;

$L__BB0_36:
	add.s32 	%r905, %r903, %r50;
	not.b32 	%r898, %r49;
	setp.gt.u32 	%p34, %r904, 1;
	@%p34 bra 	$L__BB0_26;

	setp.eq.s32 	%p158, %r49, -1;

$L__BB0_38:
	sub.s32 	%r307, %r3, %r905;
	selp.b32 	%r308, %r905, %r307, %p158;
	cvt.u64.u32 	%rd21, %r3;
	mov.u64 	%rd49, -9223372036854775808;
	rem.u64 	%rd50, %rd49, %rd21;
	cvt.u32.u64 	%r309, %rd50;
	shl.b32 	%r310, %r309, 1;
	setp.lt.u32 	%p35, %r310, %r309;
	selp.b32 	%r311, %r3, 0, %p35;
	mov.u32 	%r300, 0;
	sub.s32 	%r295, %r310, %r311;
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r294, %r295, %r3;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r294, %r294, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r298, %r294, %r308;
	mul.hi.u32 	%r299, %r308, %r294;
	mul.lo.s32 	%r312, %r298, %r4;
	mul.lo.s32 	%r301, %r312, %r3;
	mul.hi.u32 	%r302, %r312, %r3;
	// begin inline asm
	add.cc.u32 %r298, %r298, %r301;   /* inline */   
	addc.cc.u32 %r299, %r299, %r302;   /* inline */   
	addc.u32 %r300, %r300, %r300;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p36, %r300, 0;
	setp.ge.u32 	%p37, %r299, %r3;
	or.pred  	%p38, %p37, %p36;
	selp.b32 	%r313, %r3, 0, %p38;
	sub.s32 	%r937, %r299, %r313;

$L__BB0_39:
	mov.u32 	%r80, %r933;
	sub.s32 	%r935, %r935, %r232;
	add.s32 	%r933, %r80, -1;
	setp.le.u32 	%p39, %r933, %r29;
	@%p39 bra 	$L__BB0_41;

	mul.wide.u32 	%rd51, %r935, 4;
	add.s64 	%rd52, %rd4, %rd51;
	ld.global.u32 	%r314, [%rd52];
	setp.eq.s32 	%p40, %r314, 0;
	@%p40 bra 	$L__BB0_39;

$L__BB0_41:
	add.s32 	%r910, %r80, -2;
	setp.lt.s32 	%p41, %r910, %r29;
	@%p41 bra 	$L__BB0_79;

	shr.u32 	%r84, %r3, 1;
	mov.u32 	%r936, 1;
	shr.u32 	%r316, %r234, 1;
	mul.lo.s32 	%r85, %r3, %r316;
	add.s32 	%r86, %r234, -1;
	add.s32 	%r87, %r229, -1;
	and.b32  	%r88, %r234, 3;
	sub.s32 	%r89, %r234, %r88;
	and.b32  	%r90, %r229, 3;
	sub.s32 	%r91, %r229, %r90;
	mov.u32 	%r908, %r935;

$L__BB0_43:
	sub.s32 	%r908, %r908, %r232;
	mul.wide.u32 	%rd53, %r908, 4;
	add.s64 	%rd54, %rd4, %rd53;
	ld.global.u32 	%r100, [%rd54];
	setp.eq.s32 	%p42, %r100, 0;
	@%p42 bra 	$L__BB0_78;

	setp.eq.s32 	%p43, %r100, %r893;
	@%p43 bra 	$L__BB0_77;
	bra.uni 	$L__BB0_45;

$L__BB0_77:
	add.s32 	%r936, %r936, 1;
	bra.uni 	$L__BB0_78;

$L__BB0_45:
	mul.lo.s32 	%r317, %r100, %r937;
	mul.lo.s32 	%r326, %r317, %r4;
	mul.lo.s32 	%r320, %r326, %r3;
	mul.hi.u32 	%r321, %r326, %r3;
	mul.hi.u32 	%r318, %r100, %r937;
	mov.u32 	%r319, 0;
	// begin inline asm
	add.cc.u32 %r317, %r317, %r320;   /* inline */   
	addc.cc.u32 %r318, %r318, %r321;   /* inline */   
	addc.u32 %r319, %r319, %r319;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p44, %r319, 0;
	setp.ge.u32 	%p45, %r318, %r3;
	or.pred  	%p46, %p45, %p44;
	selp.b32 	%r327, %r3, 0, %p46;
	sub.s32 	%r101, %r318, %r327;
	mul.wide.u32 	%rd55, %r933, 24;
	add.s64 	%rd56, %rd3, %rd55;
	ld.global.u64 	%rd22, [%rd56+8];
	and.b64  	%rd57, %rd22, -4294967296;
	setp.eq.s64 	%p47, %rd57, 0;
	@%p47 bra 	$L__BB0_47;

	rem.u64 	%rd185, %rd22, %rd21;
	bra.uni 	$L__BB0_48;

$L__BB0_47:
	cvt.u32.u64 	%r328, %rd21;
	cvt.u32.u64 	%r329, %rd22;
	rem.u32 	%r330, %r329, %r328;
	cvt.u64.u32 	%rd185, %r330;

$L__BB0_48:
	cvt.u32.u64 	%r340, %rd185;
	mul.lo.s32 	%r331, %r937, %r340;
	mul.hi.u32 	%r332, %r937, %r340;
	mul.lo.s32 	%r341, %r331, %r4;
	mul.lo.s32 	%r334, %r341, %r3;
	mul.hi.u32 	%r335, %r341, %r3;
	mov.u32 	%r333, 0;
	// begin inline asm
	add.cc.u32 %r331, %r331, %r334;   /* inline */   
	addc.cc.u32 %r332, %r332, %r335;   /* inline */   
	addc.u32 %r333, %r333, %r333;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p48, %r333, 0;
	setp.ge.u32 	%p49, %r332, %r3;
	or.pred  	%p50, %p49, %p48;
	selp.b32 	%r342, %r3, 0, %p50;
	sub.s32 	%r937, %r332, %r342;
	mov.u32 	%r915, %r933;
	mov.u32 	%r916, %r935;
	mov.u32 	%r917, %r936;

$L__BB0_49:
	mul.wide.u32 	%rd58, %r915, 24;
	add.s64 	%rd59, %rd3, %rd58;
	ld.global.u64 	%rd26, [%rd59+16];
	and.b64  	%rd60, %rd26, -4294967296;
	setp.eq.s64 	%p51, %rd60, 0;
	@%p51 bra 	$L__BB0_51;

	rem.u64 	%rd186, %rd26, %rd21;
	bra.uni 	$L__BB0_52;

$L__BB0_51:
	cvt.u32.u64 	%r343, %rd21;
	cvt.u32.u64 	%r344, %rd26;
	rem.u32 	%r345, %r344, %r343;
	cvt.u64.u32 	%rd186, %r345;

$L__BB0_52:
	cvt.u32.u64 	%r106, %rd186;
	setp.eq.s32 	%p52, %r229, 0;
	@%p52 bra 	$L__BB0_76;

	ld.param.u32 	%r870, [sieve_kernel_trans_pp32_r32_param_10];
	shl.b32 	%r346, %r915, %r870;
	or.b32  	%r107, %r346, %r2;
	setp.eq.s32 	%p53, %r234, 1;
	@%p53 bra 	$L__BB0_60;

	setp.ne.s32 	%p54, %r234, 0;
	@%p54 bra 	$L__BB0_67;

	setp.lt.u32 	%p55, %r87, 3;
	@%p55 bra 	$L__BB0_58;

	mov.u32 	%r918, %r1;
	mov.u32 	%r919, %r91;

$L__BB0_57:
	add.s32 	%r399, %r918, %r228;
	add.s32 	%r400, %r399, %r228;
	add.s32 	%r401, %r400, %r228;
	add.s32 	%r918, %r401, %r228;
	add.s32 	%r919, %r919, -4;
	setp.ne.s32 	%p56, %r919, 0;
	@%p56 bra 	$L__BB0_57;

$L__BB0_58:
	setp.eq.s32 	%p57, %r90, 0;
	@%p57 bra 	$L__BB0_76;

	setp.eq.s32 	%p58, %r90, 1;
	bra.uni 	$L__BB0_76;

$L__BB0_60:
	setp.lt.u32 	%p60, %r87, 3;
	mov.u32 	%r923, %r1;
	mov.u32 	%r924, %r916;
	@%p60 bra 	$L__BB0_63;

	mov.u32 	%r923, %r1;
	mov.u32 	%r924, %r916;
	mov.u32 	%r922, %r91;

$L__BB0_62:
	mul.wide.u32 	%rd61, %r923, 4;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.u32 	%r442, [%rd62];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r441, %r442, %r106;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r441, %r441, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r445, %r441, %r101;
	mul.hi.u32 	%r446, %r441, %r101;
	mul.lo.s32 	%r493, %r445, %r4;
	mul.lo.s32 	%r448, %r493, %r3;
	mul.hi.u32 	%r449, %r493, %r3;
	mov.u32 	%r486, 0;
	mov.u32 	%r447, %r486;
	// begin inline asm
	add.cc.u32 %r445, %r445, %r448;   /* inline */   
	addc.cc.u32 %r446, %r446, %r449;   /* inline */   
	addc.u32 %r447, %r447, %r447;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p61, %r447, 0;
	setp.ge.u32 	%p62, %r446, %r3;
	or.pred  	%p63, %p62, %p61;
	selp.b32 	%r494, %r3, 0, %p63;
	sub.s32 	%r495, %r446, %r494;
	setp.gt.u32 	%p64, %r495, %r84;
	selp.b32 	%r496, %r3, 0, %p64;
	sub.s32 	%r497, %r495, %r496;
	mul.wide.u32 	%rd63, %r924, 4;
	add.s64 	%rd64, %rd1, %rd63;
	st.global.u32 	[%rd64], %r107;
	add.s64 	%rd65, %rd4, %rd63;
	st.global.u32 	[%rd65], %r497;
	add.s32 	%r498, %r923, %r228;
	mul.wide.u32 	%rd66, %r498, 4;
	add.s64 	%rd67, %rd2, %rd66;
	ld.global.u32 	%r455, [%rd67];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r454, %r455, %r106;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r454, %r454, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r458, %r454, %r101;
	mul.hi.u32 	%r459, %r454, %r101;
	mul.lo.s32 	%r499, %r458, %r4;
	mul.lo.s32 	%r461, %r499, %r3;
	mul.hi.u32 	%r462, %r499, %r3;
	mov.u32 	%r460, %r486;
	// begin inline asm
	add.cc.u32 %r458, %r458, %r461;   /* inline */   
	addc.cc.u32 %r459, %r459, %r462;   /* inline */   
	addc.u32 %r460, %r460, %r460;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p65, %r460, 0;
	setp.ge.u32 	%p66, %r459, %r3;
	or.pred  	%p67, %p66, %p65;
	selp.b32 	%r500, %r3, 0, %p67;
	sub.s32 	%r501, %r459, %r500;
	setp.gt.u32 	%p68, %r501, %r84;
	selp.b32 	%r502, %r3, 0, %p68;
	sub.s32 	%r503, %r501, %r502;
	add.s32 	%r504, %r924, %r228;
	mul.wide.u32 	%rd68, %r504, 4;
	add.s64 	%rd69, %rd1, %rd68;
	st.global.u32 	[%rd69], %r107;
	add.s64 	%rd70, %rd4, %rd68;
	st.global.u32 	[%rd70], %r503;
	add.s32 	%r505, %r504, %r228;
	add.s32 	%r506, %r498, %r228;
	mul.wide.u32 	%rd71, %r506, 4;
	add.s64 	%rd72, %rd2, %rd71;
	ld.global.u32 	%r468, [%rd72];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r467, %r468, %r106;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r467, %r467, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r471, %r467, %r101;
	mul.hi.u32 	%r472, %r467, %r101;
	mul.lo.s32 	%r507, %r471, %r4;
	mul.lo.s32 	%r474, %r507, %r3;
	mul.hi.u32 	%r475, %r507, %r3;
	mov.u32 	%r473, %r486;
	// begin inline asm
	add.cc.u32 %r471, %r471, %r474;   /* inline */   
	addc.cc.u32 %r472, %r472, %r475;   /* inline */   
	addc.u32 %r473, %r473, %r473;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p69, %r473, 0;
	setp.ge.u32 	%p70, %r472, %r3;
	or.pred  	%p71, %p70, %p69;
	selp.b32 	%r508, %r3, 0, %p71;
	sub.s32 	%r509, %r472, %r508;
	setp.gt.u32 	%p72, %r509, %r84;
	selp.b32 	%r510, %r3, 0, %p72;
	sub.s32 	%r511, %r509, %r510;
	mul.wide.u32 	%rd73, %r505, 4;
	add.s64 	%rd74, %rd1, %rd73;
	st.global.u32 	[%rd74], %r107;
	add.s64 	%rd75, %rd4, %rd73;
	st.global.u32 	[%rd75], %r511;
	add.s32 	%r512, %r505, %r228;
	add.s32 	%r513, %r506, %r228;
	mul.wide.u32 	%rd76, %r513, 4;
	add.s64 	%rd77, %rd2, %rd76;
	ld.global.u32 	%r481, [%rd77];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r480, %r481, %r106;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r480, %r480, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r484, %r480, %r101;
	mul.hi.u32 	%r485, %r480, %r101;
	mul.lo.s32 	%r514, %r484, %r4;
	mul.lo.s32 	%r487, %r514, %r3;
	mul.hi.u32 	%r488, %r514, %r3;
	// begin inline asm
	add.cc.u32 %r484, %r484, %r487;   /* inline */   
	addc.cc.u32 %r485, %r485, %r488;   /* inline */   
	addc.u32 %r486, %r486, %r486;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p73, %r486, 0;
	setp.ge.u32 	%p74, %r485, %r3;
	or.pred  	%p75, %p74, %p73;
	selp.b32 	%r515, %r3, 0, %p75;
	sub.s32 	%r516, %r485, %r515;
	setp.gt.u32 	%p76, %r516, %r84;
	selp.b32 	%r517, %r3, 0, %p76;
	sub.s32 	%r518, %r516, %r517;
	mul.wide.u32 	%rd78, %r512, 4;
	add.s64 	%rd79, %rd1, %rd78;
	st.global.u32 	[%rd79], %r107;
	add.s64 	%rd80, %rd4, %rd78;
	st.global.u32 	[%rd80], %r518;
	add.s32 	%r924, %r512, %r228;
	add.s32 	%r923, %r513, %r228;
	add.s32 	%r922, %r922, -4;
	setp.ne.s32 	%p77, %r922, 0;
	@%p77 bra 	$L__BB0_62;

$L__BB0_63:
	setp.eq.s32 	%p78, %r90, 0;
	@%p78 bra 	$L__BB0_76;

	setp.eq.s32 	%p79, %r90, 1;
	mul.wide.u32 	%rd81, %r923, 4;
	add.s64 	%rd82, %rd2, %rd81;
	ld.global.u32 	%r520, [%rd82];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r519, %r520, %r106;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r519, %r519, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r523, %r519, %r101;
	mul.hi.u32 	%r524, %r519, %r101;
	mul.lo.s32 	%r532, %r523, %r4;
	mul.lo.s32 	%r526, %r532, %r3;
	mul.hi.u32 	%r527, %r532, %r3;
	mov.u32 	%r543, 0;
	mov.u32 	%r525, %r543;
	// begin inline asm
	add.cc.u32 %r523, %r523, %r526;   /* inline */   
	addc.cc.u32 %r524, %r524, %r527;   /* inline */   
	addc.u32 %r525, %r525, %r525;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p80, %r525, 0;
	setp.ge.u32 	%p81, %r524, %r3;
	or.pred  	%p82, %p81, %p80;
	selp.b32 	%r533, %r3, 0, %p82;
	sub.s32 	%r534, %r524, %r533;
	setp.gt.u32 	%p83, %r534, %r84;
	selp.b32 	%r535, %r3, 0, %p83;
	sub.s32 	%r536, %r534, %r535;
	mul.wide.u32 	%rd83, %r924, 4;
	add.s64 	%rd84, %rd1, %rd83;
	st.global.u32 	[%rd84], %r107;
	add.s64 	%rd85, %rd4, %rd83;
	st.global.u32 	[%rd85], %r536;
	@%p79 bra 	$L__BB0_76;

	add.s32 	%r122, %r924, %r228;
	add.s32 	%r123, %r923, %r228;
	setp.eq.s32 	%p84, %r90, 2;
	mul.wide.u32 	%rd86, %r123, 4;
	add.s64 	%rd87, %rd2, %rd86;
	ld.global.u32 	%r538, [%rd87];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r537, %r538, %r106;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r537, %r537, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r541, %r537, %r101;
	mul.hi.u32 	%r542, %r537, %r101;
	mul.lo.s32 	%r550, %r541, %r4;
	mul.lo.s32 	%r544, %r550, %r3;
	mul.hi.u32 	%r545, %r550, %r3;
	// begin inline asm
	add.cc.u32 %r541, %r541, %r544;   /* inline */   
	addc.cc.u32 %r542, %r542, %r545;   /* inline */   
	addc.u32 %r543, %r543, %r543;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p85, %r543, 0;
	setp.ge.u32 	%p86, %r542, %r3;
	or.pred  	%p87, %p86, %p85;
	selp.b32 	%r551, %r3, 0, %p87;
	sub.s32 	%r552, %r542, %r551;
	setp.gt.u32 	%p88, %r552, %r84;
	selp.b32 	%r553, %r3, 0, %p88;
	sub.s32 	%r554, %r552, %r553;
	mul.wide.u32 	%rd88, %r122, 4;
	add.s64 	%rd89, %rd1, %rd88;
	st.global.u32 	[%rd89], %r107;
	add.s64 	%rd90, %rd4, %rd88;
	st.global.u32 	[%rd90], %r554;
	@%p84 bra 	$L__BB0_76;

	add.s32 	%r568, %r123, %r228;
	mul.wide.u32 	%rd91, %r568, 4;
	add.s64 	%rd92, %rd2, %rd91;
	ld.global.u32 	%r556, [%rd92];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r555, %r556, %r106;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r555, %r555, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r559, %r555, %r101;
	mul.hi.u32 	%r560, %r555, %r101;
	mul.lo.s32 	%r569, %r559, %r4;
	mul.lo.s32 	%r562, %r569, %r3;
	mul.hi.u32 	%r563, %r569, %r3;
	mov.u32 	%r561, 0;
	// begin inline asm
	add.cc.u32 %r559, %r559, %r562;   /* inline */   
	addc.cc.u32 %r560, %r560, %r563;   /* inline */   
	addc.u32 %r561, %r561, %r561;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p89, %r561, 0;
	setp.ge.u32 	%p90, %r560, %r3;
	or.pred  	%p91, %p90, %p89;
	selp.b32 	%r570, %r3, 0, %p91;
	sub.s32 	%r571, %r560, %r570;
	setp.gt.u32 	%p92, %r571, %r84;
	selp.b32 	%r572, %r3, 0, %p92;
	sub.s32 	%r573, %r571, %r572;
	add.s32 	%r574, %r122, %r228;
	mul.wide.u32 	%rd93, %r574, 4;
	add.s64 	%rd94, %rd1, %rd93;
	st.global.u32 	[%rd94], %r107;
	add.s64 	%rd95, %rd4, %rd93;
	st.global.u32 	[%rd95], %r573;
	bra.uni 	$L__BB0_76;

$L__BB0_67:
	mov.u32 	%r925, 0;
	mov.u32 	%r926, %r1;
	mov.u32 	%r927, %r916;

$L__BB0_68:
	mov.u32 	%r582, 0;
	mul.wide.u32 	%rd96, %r926, 4;
	add.s64 	%rd97, %rd2, %rd96;
	ld.global.u32 	%r577, [%rd97];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r576, %r577, %r106;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r576, %r576, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r580, %r576, %r101;
	mul.hi.u32 	%r581, %r576, %r101;
	mul.lo.s32 	%r589, %r580, %r4;
	mul.lo.s32 	%r583, %r589, %r3;
	mul.hi.u32 	%r584, %r589, %r3;
	// begin inline asm
	add.cc.u32 %r580, %r580, %r583;   /* inline */   
	addc.cc.u32 %r581, %r581, %r584;   /* inline */   
	addc.u32 %r582, %r582, %r582;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p93, %r582, 0;
	setp.ge.u32 	%p94, %r581, %r3;
	or.pred  	%p95, %p94, %p93;
	selp.b32 	%r590, %r3, 0, %p95;
	sub.s32 	%r591, %r581, %r590;
	sub.s32 	%r932, %r591, %r85;
	setp.lt.u32 	%p96, %r86, 3;
	mov.u32 	%r931, %r927;
	@%p96 bra 	$L__BB0_71;

	mov.u32 	%r931, %r927;
	mov.u32 	%r930, %r89;

$L__BB0_70:
	mul.wide.u32 	%rd98, %r931, 4;
	add.s64 	%rd99, %rd1, %rd98;
	st.global.u32 	[%rd99], %r107;
	add.s64 	%rd100, %rd4, %rd98;
	st.global.u32 	[%rd100], %r932;
	add.s32 	%r592, %r931, %r7;
	mul.wide.u32 	%rd101, %r592, 4;
	add.s64 	%rd102, %rd1, %rd101;
	st.global.u32 	[%rd102], %r107;
	add.s64 	%rd103, %rd4, %rd101;
	add.s32 	%r593, %r932, %r3;
	st.global.u32 	[%rd103], %r593;
	add.s32 	%r594, %r592, %r7;
	add.s32 	%r595, %r593, %r3;
	mul.wide.u32 	%rd104, %r594, 4;
	add.s64 	%rd105, %rd1, %rd104;
	st.global.u32 	[%rd105], %r107;
	add.s64 	%rd106, %rd4, %rd104;
	st.global.u32 	[%rd106], %r595;
	add.s32 	%r596, %r594, %r7;
	add.s32 	%r597, %r595, %r3;
	mul.wide.u32 	%rd107, %r596, 4;
	add.s64 	%rd108, %rd1, %rd107;
	st.global.u32 	[%rd108], %r107;
	add.s64 	%rd109, %rd4, %rd107;
	st.global.u32 	[%rd109], %r597;
	add.s32 	%r931, %r596, %r7;
	add.s32 	%r932, %r597, %r3;
	add.s32 	%r930, %r930, -4;
	setp.ne.s32 	%p97, %r930, 0;
	@%p97 bra 	$L__BB0_70;

$L__BB0_71:
	setp.eq.s32 	%p98, %r88, 0;
	@%p98 bra 	$L__BB0_75;

	setp.eq.s32 	%p99, %r88, 1;
	mul.wide.u32 	%rd110, %r931, 4;
	add.s64 	%rd111, %rd1, %rd110;
	st.global.u32 	[%rd111], %r107;
	add.s64 	%rd112, %rd4, %rd110;
	st.global.u32 	[%rd112], %r932;
	@%p99 bra 	$L__BB0_75;

	add.s32 	%r136, %r931, %r7;
	add.s32 	%r137, %r932, %r3;
	setp.eq.s32 	%p100, %r88, 2;
	mul.wide.u32 	%rd113, %r136, 4;
	add.s64 	%rd114, %rd1, %rd113;
	st.global.u32 	[%rd114], %r107;
	add.s64 	%rd115, %rd4, %rd113;
	st.global.u32 	[%rd115], %r137;
	@%p100 bra 	$L__BB0_75;

	add.s32 	%r598, %r136, %r7;
	mul.wide.u32 	%rd116, %r598, 4;
	add.s64 	%rd117, %rd1, %rd116;
	st.global.u32 	[%rd117], %r107;
	add.s64 	%rd118, %rd4, %rd116;
	add.s32 	%r599, %r137, %r3;
	st.global.u32 	[%rd118], %r599;

$L__BB0_75:
	add.s32 	%r927, %r927, %r228;
	add.s32 	%r926, %r926, %r228;
	add.s32 	%r925, %r925, 1;
	setp.lt.u32 	%p101, %r925, %r229;
	@%p101 bra 	$L__BB0_68;

$L__BB0_76:
	add.s32 	%r915, %r915, -1;
	sub.s32 	%r916, %r916, %r232;
	add.s32 	%r917, %r917, -1;
	setp.eq.s32 	%p102, %r917, 0;
	mov.u32 	%r936, 1;
	mov.u32 	%r933, %r910;
	mov.u32 	%r893, %r100;
	mov.u32 	%r935, %r908;
	@%p102 bra 	$L__BB0_78;
	bra.uni 	$L__BB0_49;

$L__BB0_78:
	add.s32 	%r910, %r910, -1;
	setp.ge.s32 	%p103, %r910, %r29;
	@%p103 bra 	$L__BB0_43;

$L__BB0_79:
	setp.lt.s32 	%p104, %r933, %r29;
	@%p104 bra 	$L__BB0_109;

	shr.u32 	%r154, %r3, 1;
	mov.u32 	%r601, 1;
	shr.u32 	%r602, %r234, 1;
	mul.lo.s32 	%r155, %r3, %r602;
	add.s32 	%r156, %r234, -1;
	add.s32 	%r157, %r229, -1;
	and.b32  	%r158, %r229, 3;
	mov.u32 	%r603, 3;
	sub.s32 	%r159, %r229, %r158;
	mul.lo.s32 	%r160, %r7, 3;
	shl.b32 	%r161, %r7, 2;
	mov.u32 	%r604, 2;
	shl.b32 	%r162, %r7, 1;
	sub.s32 	%r605, %r603, %r602;
	mul.lo.s32 	%r163, %r3, %r605;
	shl.b32 	%r164, %r3, 2;
	sub.s32 	%r606, %r604, %r602;
	mul.lo.s32 	%r165, %r3, %r606;
	sub.s32 	%r607, %r601, %r602;
	mul.lo.s32 	%r166, %r3, %r607;
	and.b32  	%r167, %r234, 3;
	sub.s32 	%r168, %r167, %r234;

$L__BB0_81:
	mul.wide.u32 	%rd119, %r933, 24;
	add.s64 	%rd120, %rd3, %rd119;
	ld.global.u64 	%rd30, [%rd120+16];
	and.b64  	%rd121, %rd30, -4294967296;
	setp.eq.s64 	%p105, %rd121, 0;
	@%p105 bra 	$L__BB0_83;

	rem.u64 	%rd187, %rd30, %rd21;
	bra.uni 	$L__BB0_84;

$L__BB0_83:
	cvt.u32.u64 	%r608, %rd21;
	cvt.u32.u64 	%r609, %rd30;
	rem.u32 	%r610, %r609, %r608;
	cvt.u64.u32 	%rd187, %r610;

$L__BB0_84:
	cvt.u32.u64 	%r171, %rd187;
	setp.eq.s32 	%p106, %r229, 0;
	@%p106 bra 	$L__BB0_108;

	ld.param.u32 	%r869, [sieve_kernel_trans_pp32_r32_param_10];
	shl.b32 	%r611, %r933, %r869;
	or.b32  	%r172, %r611, %r2;
	setp.eq.s32 	%p107, %r234, 1;
	@%p107 bra 	$L__BB0_92;

	setp.ne.s32 	%p108, %r234, 0;
	@%p108 bra 	$L__BB0_99;

	setp.lt.u32 	%p109, %r157, 3;
	@%p109 bra 	$L__BB0_90;

	mov.u32 	%r943, %r1;
	mov.u32 	%r944, %r159;

$L__BB0_89:
	add.s32 	%r664, %r943, %r228;
	add.s32 	%r665, %r664, %r228;
	add.s32 	%r666, %r665, %r228;
	add.s32 	%r943, %r666, %r228;
	add.s32 	%r944, %r944, -4;
	setp.ne.s32 	%p110, %r944, 0;
	@%p110 bra 	$L__BB0_89;

$L__BB0_90:
	setp.eq.s32 	%p111, %r158, 0;
	@%p111 bra 	$L__BB0_108;

	setp.eq.s32 	%p112, %r158, 1;
	bra.uni 	$L__BB0_108;

$L__BB0_92:
	setp.lt.u32 	%p114, %r157, 3;
	mov.u32 	%r948, %r1;
	mov.u32 	%r949, %r935;
	@%p114 bra 	$L__BB0_95;

	mov.u32 	%r948, %r1;
	mov.u32 	%r949, %r935;
	mov.u32 	%r947, %r159;

$L__BB0_94:
	mul.wide.u32 	%rd122, %r948, 4;
	add.s64 	%rd123, %rd2, %rd122;
	ld.global.u32 	%r707, [%rd123];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r706, %r707, %r171;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r706, %r706, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r710, %r706, %r937;
	mul.hi.u32 	%r711, %r706, %r937;
	mul.lo.s32 	%r758, %r710, %r4;
	mul.lo.s32 	%r713, %r758, %r3;
	mul.hi.u32 	%r714, %r758, %r3;
	mov.u32 	%r751, 0;
	mov.u32 	%r712, %r751;
	// begin inline asm
	add.cc.u32 %r710, %r710, %r713;   /* inline */   
	addc.cc.u32 %r711, %r711, %r714;   /* inline */   
	addc.u32 %r712, %r712, %r712;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p115, %r712, 0;
	setp.ge.u32 	%p116, %r711, %r3;
	or.pred  	%p117, %p116, %p115;
	selp.b32 	%r759, %r3, 0, %p117;
	sub.s32 	%r760, %r711, %r759;
	setp.gt.u32 	%p118, %r760, %r154;
	selp.b32 	%r761, %r3, 0, %p118;
	sub.s32 	%r762, %r760, %r761;
	mul.wide.u32 	%rd124, %r949, 4;
	add.s64 	%rd125, %rd1, %rd124;
	st.global.u32 	[%rd125], %r172;
	add.s64 	%rd126, %rd4, %rd124;
	st.global.u32 	[%rd126], %r762;
	add.s32 	%r763, %r948, %r228;
	mul.wide.u32 	%rd127, %r763, 4;
	add.s64 	%rd128, %rd2, %rd127;
	ld.global.u32 	%r720, [%rd128];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r719, %r720, %r171;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r719, %r719, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r723, %r719, %r937;
	mul.hi.u32 	%r724, %r719, %r937;
	mul.lo.s32 	%r764, %r723, %r4;
	mul.lo.s32 	%r726, %r764, %r3;
	mul.hi.u32 	%r727, %r764, %r3;
	mov.u32 	%r725, %r751;
	// begin inline asm
	add.cc.u32 %r723, %r723, %r726;   /* inline */   
	addc.cc.u32 %r724, %r724, %r727;   /* inline */   
	addc.u32 %r725, %r725, %r725;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p119, %r725, 0;
	setp.ge.u32 	%p120, %r724, %r3;
	or.pred  	%p121, %p120, %p119;
	selp.b32 	%r765, %r3, 0, %p121;
	sub.s32 	%r766, %r724, %r765;
	setp.gt.u32 	%p122, %r766, %r154;
	selp.b32 	%r767, %r3, 0, %p122;
	sub.s32 	%r768, %r766, %r767;
	add.s32 	%r769, %r949, %r228;
	mul.wide.u32 	%rd129, %r769, 4;
	add.s64 	%rd130, %rd1, %rd129;
	st.global.u32 	[%rd130], %r172;
	add.s64 	%rd131, %rd4, %rd129;
	st.global.u32 	[%rd131], %r768;
	add.s32 	%r770, %r769, %r228;
	add.s32 	%r771, %r763, %r228;
	mul.wide.u32 	%rd132, %r771, 4;
	add.s64 	%rd133, %rd2, %rd132;
	ld.global.u32 	%r733, [%rd133];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r732, %r733, %r171;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r732, %r732, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r736, %r732, %r937;
	mul.hi.u32 	%r737, %r732, %r937;
	mul.lo.s32 	%r772, %r736, %r4;
	mul.lo.s32 	%r739, %r772, %r3;
	mul.hi.u32 	%r740, %r772, %r3;
	mov.u32 	%r738, %r751;
	// begin inline asm
	add.cc.u32 %r736, %r736, %r739;   /* inline */   
	addc.cc.u32 %r737, %r737, %r740;   /* inline */   
	addc.u32 %r738, %r738, %r738;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p123, %r738, 0;
	setp.ge.u32 	%p124, %r737, %r3;
	or.pred  	%p125, %p124, %p123;
	selp.b32 	%r773, %r3, 0, %p125;
	sub.s32 	%r774, %r737, %r773;
	setp.gt.u32 	%p126, %r774, %r154;
	selp.b32 	%r775, %r3, 0, %p126;
	sub.s32 	%r776, %r774, %r775;
	mul.wide.u32 	%rd134, %r770, 4;
	add.s64 	%rd135, %rd1, %rd134;
	st.global.u32 	[%rd135], %r172;
	add.s64 	%rd136, %rd4, %rd134;
	st.global.u32 	[%rd136], %r776;
	add.s32 	%r777, %r770, %r228;
	add.s32 	%r778, %r771, %r228;
	mul.wide.u32 	%rd137, %r778, 4;
	add.s64 	%rd138, %rd2, %rd137;
	ld.global.u32 	%r746, [%rd138];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r745, %r746, %r171;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r745, %r745, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r749, %r745, %r937;
	mul.hi.u32 	%r750, %r745, %r937;
	mul.lo.s32 	%r779, %r749, %r4;
	mul.lo.s32 	%r752, %r779, %r3;
	mul.hi.u32 	%r753, %r779, %r3;
	// begin inline asm
	add.cc.u32 %r749, %r749, %r752;   /* inline */   
	addc.cc.u32 %r750, %r750, %r753;   /* inline */   
	addc.u32 %r751, %r751, %r751;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p127, %r751, 0;
	setp.ge.u32 	%p128, %r750, %r3;
	or.pred  	%p129, %p128, %p127;
	selp.b32 	%r780, %r3, 0, %p129;
	sub.s32 	%r781, %r750, %r780;
	setp.gt.u32 	%p130, %r781, %r154;
	selp.b32 	%r782, %r3, 0, %p130;
	sub.s32 	%r783, %r781, %r782;
	mul.wide.u32 	%rd139, %r777, 4;
	add.s64 	%rd140, %rd1, %rd139;
	st.global.u32 	[%rd140], %r172;
	add.s64 	%rd141, %rd4, %rd139;
	st.global.u32 	[%rd141], %r783;
	add.s32 	%r949, %r777, %r228;
	add.s32 	%r948, %r778, %r228;
	add.s32 	%r947, %r947, -4;
	setp.ne.s32 	%p131, %r947, 0;
	@%p131 bra 	$L__BB0_94;

$L__BB0_95:
	setp.eq.s32 	%p132, %r158, 0;
	@%p132 bra 	$L__BB0_108;

	setp.eq.s32 	%p133, %r158, 1;
	mul.wide.u32 	%rd142, %r948, 4;
	add.s64 	%rd143, %rd2, %rd142;
	ld.global.u32 	%r785, [%rd143];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r784, %r785, %r171;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r784, %r784, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r788, %r784, %r937;
	mul.hi.u32 	%r789, %r784, %r937;
	mul.lo.s32 	%r797, %r788, %r4;
	mul.lo.s32 	%r791, %r797, %r3;
	mul.hi.u32 	%r792, %r797, %r3;
	mov.u32 	%r808, 0;
	mov.u32 	%r790, %r808;
	// begin inline asm
	add.cc.u32 %r788, %r788, %r791;   /* inline */   
	addc.cc.u32 %r789, %r789, %r792;   /* inline */   
	addc.u32 %r790, %r790, %r790;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p134, %r790, 0;
	setp.ge.u32 	%p135, %r789, %r3;
	or.pred  	%p136, %p135, %p134;
	selp.b32 	%r798, %r3, 0, %p136;
	sub.s32 	%r799, %r789, %r798;
	setp.gt.u32 	%p137, %r799, %r154;
	selp.b32 	%r800, %r3, 0, %p137;
	sub.s32 	%r801, %r799, %r800;
	mul.wide.u32 	%rd144, %r949, 4;
	add.s64 	%rd145, %rd1, %rd144;
	st.global.u32 	[%rd145], %r172;
	add.s64 	%rd146, %rd4, %rd144;
	st.global.u32 	[%rd146], %r801;
	@%p133 bra 	$L__BB0_108;

	add.s32 	%r187, %r949, %r228;
	add.s32 	%r188, %r948, %r228;
	setp.eq.s32 	%p138, %r158, 2;
	mul.wide.u32 	%rd147, %r188, 4;
	add.s64 	%rd148, %rd2, %rd147;
	ld.global.u32 	%r803, [%rd148];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r802, %r803, %r171;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r802, %r802, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r806, %r802, %r937;
	mul.hi.u32 	%r807, %r802, %r937;
	mul.lo.s32 	%r815, %r806, %r4;
	mul.lo.s32 	%r809, %r815, %r3;
	mul.hi.u32 	%r810, %r815, %r3;
	// begin inline asm
	add.cc.u32 %r806, %r806, %r809;   /* inline */   
	addc.cc.u32 %r807, %r807, %r810;   /* inline */   
	addc.u32 %r808, %r808, %r808;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p139, %r808, 0;
	setp.ge.u32 	%p140, %r807, %r3;
	or.pred  	%p141, %p140, %p139;
	selp.b32 	%r816, %r3, 0, %p141;
	sub.s32 	%r817, %r807, %r816;
	setp.gt.u32 	%p142, %r817, %r154;
	selp.b32 	%r818, %r3, 0, %p142;
	sub.s32 	%r819, %r817, %r818;
	mul.wide.u32 	%rd149, %r187, 4;
	add.s64 	%rd150, %rd1, %rd149;
	st.global.u32 	[%rd150], %r172;
	add.s64 	%rd151, %rd4, %rd149;
	st.global.u32 	[%rd151], %r819;
	@%p138 bra 	$L__BB0_108;

	add.s32 	%r833, %r188, %r228;
	mul.wide.u32 	%rd152, %r833, 4;
	add.s64 	%rd153, %rd2, %rd152;
	ld.global.u32 	%r821, [%rd153];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r820, %r821, %r171;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r820, %r820, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r824, %r820, %r937;
	mul.hi.u32 	%r825, %r820, %r937;
	mul.lo.s32 	%r834, %r824, %r4;
	mul.lo.s32 	%r827, %r834, %r3;
	mul.hi.u32 	%r828, %r834, %r3;
	mov.u32 	%r826, 0;
	// begin inline asm
	add.cc.u32 %r824, %r824, %r827;   /* inline */   
	addc.cc.u32 %r825, %r825, %r828;   /* inline */   
	addc.u32 %r826, %r826, %r826;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p143, %r826, 0;
	setp.ge.u32 	%p144, %r825, %r3;
	or.pred  	%p145, %p144, %p143;
	selp.b32 	%r835, %r3, 0, %p145;
	sub.s32 	%r836, %r825, %r835;
	setp.gt.u32 	%p146, %r836, %r154;
	selp.b32 	%r837, %r3, 0, %p146;
	sub.s32 	%r838, %r836, %r837;
	add.s32 	%r839, %r187, %r228;
	mul.wide.u32 	%rd154, %r839, 4;
	add.s64 	%rd155, %rd1, %rd154;
	st.global.u32 	[%rd155], %r172;
	add.s64 	%rd156, %rd4, %rd154;
	st.global.u32 	[%rd156], %r838;
	bra.uni 	$L__BB0_108;

$L__BB0_99:
	mov.u32 	%r840, 0;
	mov.u32 	%r950, %r840;
	mov.u32 	%r951, %r1;
	mov.u32 	%r952, %r935;

$L__BB0_100:
	mul.wide.u32 	%rd157, %r951, 4;
	add.s64 	%rd158, %rd2, %rd157;
	ld.global.u32 	%r842, [%rd158];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r841, %r842, %r171;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r841, %r841, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r845, %r841, %r937;
	mul.hi.u32 	%r846, %r841, %r937;
	mul.lo.s32 	%r854, %r845, %r4;
	mul.lo.s32 	%r848, %r854, %r3;
	mul.hi.u32 	%r849, %r854, %r3;
	mov.u32 	%r847, %r840;
	// begin inline asm
	add.cc.u32 %r845, %r845, %r848;   /* inline */   
	addc.cc.u32 %r846, %r846, %r849;   /* inline */   
	addc.u32 %r847, %r847, %r847;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p147, %r847, 0;
	setp.ge.u32 	%p148, %r846, %r3;
	or.pred  	%p149, %p148, %p147;
	selp.b32 	%r193, %r3, 0, %p149;
	sub.s32 	%r855, %r846, %r193;
	sub.s32 	%r963, %r855, %r155;
	setp.lt.u32 	%p150, %r156, 3;
	mov.u32 	%r962, %r952;
	@%p150 bra 	$L__BB0_103;

	add.s32 	%r959, %r160, %r952;
	add.s32 	%r958, %r162, %r952;
	add.s32 	%r957, %r7, %r952;
	add.s32 	%r856, %r163, %r846;
	sub.s32 	%r956, %r856, %r193;
	add.s32 	%r857, %r165, %r846;
	sub.s32 	%r955, %r857, %r193;
	add.s32 	%r858, %r166, %r846;
	sub.s32 	%r954, %r858, %r193;
	mov.u32 	%r953, %r168;
	mov.u32 	%r962, %r952;

$L__BB0_102:
	mul.wide.u32 	%rd159, %r962, 4;
	add.s64 	%rd160, %rd1, %rd159;
	st.global.u32 	[%rd160], %r172;
	add.s64 	%rd161, %rd4, %rd159;
	st.global.u32 	[%rd161], %r963;
	mul.wide.u32 	%rd162, %r957, 4;
	add.s64 	%rd163, %rd1, %rd162;
	st.global.u32 	[%rd163], %r172;
	add.s64 	%rd164, %rd4, %rd162;
	st.global.u32 	[%rd164], %r954;
	add.s32 	%r859, %r962, %r7;
	add.s32 	%r860, %r859, %r7;
	add.s32 	%r861, %r963, %r3;
	add.s32 	%r862, %r861, %r3;
	mul.wide.u32 	%rd165, %r958, 4;
	add.s64 	%rd166, %rd1, %rd165;
	st.global.u32 	[%rd166], %r172;
	add.s64 	%rd167, %rd4, %rd165;
	st.global.u32 	[%rd167], %r955;
	add.s32 	%r863, %r860, %r7;
	add.s32 	%r864, %r862, %r3;
	mul.wide.u32 	%rd168, %r959, 4;
	add.s64 	%rd169, %rd1, %rd168;
	st.global.u32 	[%rd169], %r172;
	add.s64 	%rd170, %rd4, %rd168;
	st.global.u32 	[%rd170], %r956;
	add.s32 	%r962, %r863, %r7;
	add.s32 	%r963, %r864, %r3;
	add.s32 	%r959, %r959, %r161;
	add.s32 	%r958, %r958, %r161;
	add.s32 	%r957, %r957, %r161;
	add.s32 	%r956, %r956, %r164;
	add.s32 	%r955, %r955, %r164;
	add.s32 	%r954, %r954, %r164;
	add.s32 	%r953, %r953, 4;
	setp.ne.s32 	%p151, %r953, 0;
	@%p151 bra 	$L__BB0_102;

$L__BB0_103:
	setp.eq.s32 	%p152, %r167, 0;
	@%p152 bra 	$L__BB0_107;

	setp.eq.s32 	%p153, %r167, 1;
	mul.wide.u32 	%rd171, %r962, 4;
	add.s64 	%rd172, %rd1, %rd171;
	st.global.u32 	[%rd172], %r172;
	add.s64 	%rd173, %rd4, %rd171;
	st.global.u32 	[%rd173], %r963;
	@%p153 bra 	$L__BB0_107;

	add.s32 	%r221, %r962, %r7;
	add.s32 	%r222, %r963, %r3;
	setp.eq.s32 	%p154, %r167, 2;
	mul.wide.u32 	%rd174, %r221, 4;
	add.s64 	%rd175, %rd1, %rd174;
	st.global.u32 	[%rd175], %r172;
	add.s64 	%rd176, %rd4, %rd174;
	st.global.u32 	[%rd176], %r222;
	@%p154 bra 	$L__BB0_107;

	add.s32 	%r865, %r221, %r7;
	mul.wide.u32 	%rd177, %r865, 4;
	add.s64 	%rd178, %rd1, %rd177;
	st.global.u32 	[%rd178], %r172;
	add.s64 	%rd179, %rd4, %rd177;
	add.s32 	%r866, %r222, %r3;
	st.global.u32 	[%rd179], %r866;

$L__BB0_107:
	add.s32 	%r952, %r952, %r228;
	add.s32 	%r951, %r951, %r228;
	add.s32 	%r950, %r950, 1;
	setp.lt.u32 	%p155, %r950, %r229;
	@%p155 bra 	$L__BB0_100;

$L__BB0_108:
	ld.param.u32 	%r868, [sieve_kernel_trans_pp32_r32_param_9];
	add.s32 	%r933, %r933, -1;
	sub.s32 	%r935, %r935, %r868;
	setp.ge.s32 	%p156, %r933, %r29;
	@%p156 bra 	$L__BB0_81;

$L__BB0_109:
	ret;

}
	// .globl	sieve_kernel_trans_pp32_r64
.visible .entry sieve_kernel_trans_pp32_r64(
	.param .u64 sieve_kernel_trans_pp32_r64_param_0,
	.param .u32 sieve_kernel_trans_pp32_r64_param_1,
	.param .u64 sieve_kernel_trans_pp32_r64_param_2,
	.param .u32 sieve_kernel_trans_pp32_r64_param_3,
	.param .u64 sieve_kernel_trans_pp32_r64_param_4,
	.param .u64 sieve_kernel_trans_pp32_r64_param_5,
	.param .u64 sieve_kernel_trans_pp32_r64_param_6,
	.param .u32 sieve_kernel_trans_pp32_r64_param_7,
	.param .u32 sieve_kernel_trans_pp32_r64_param_8,
	.param .u32 sieve_kernel_trans_pp32_r64_param_9,
	.param .u32 sieve_kernel_trans_pp32_r64_param_10,
	.param .u32 sieve_kernel_trans_pp32_r64_param_11
)
{
	.reg .pred 	%p<159>;
	.reg .b32 	%r<915>;
	.reg .b64 	%rd<280>;


	ld.param.u64 	%rd60, [sieve_kernel_trans_pp32_r64_param_0];
	ld.param.u32 	%r201, [sieve_kernel_trans_pp32_r64_param_1];
	ld.param.u64 	%rd61, [sieve_kernel_trans_pp32_r64_param_2];
	ld.param.u32 	%r202, [sieve_kernel_trans_pp32_r64_param_3];
	ld.param.u64 	%rd62, [sieve_kernel_trans_pp32_r64_param_4];
	ld.param.u64 	%rd63, [sieve_kernel_trans_pp32_r64_param_5];
	ld.param.u64 	%rd64, [sieve_kernel_trans_pp32_r64_param_6];
	ld.param.u32 	%r203, [sieve_kernel_trans_pp32_r64_param_7];
	ld.param.u32 	%r204, [sieve_kernel_trans_pp32_r64_param_8];
	ld.param.u32 	%r205, [sieve_kernel_trans_pp32_r64_param_9];
	ld.param.u32 	%r207, [sieve_kernel_trans_pp32_r64_param_11];
	cvta.to.global.u64 	%rd1, %rd63;
	cvta.to.global.u64 	%rd2, %rd61;
	cvta.to.global.u64 	%rd3, %rd64;
	cvta.to.global.u64 	%rd4, %rd62;
	mov.u32 	%r208, %ntid.x;
	mov.u32 	%r209, %ctaid.x;
	mov.u32 	%r210, %tid.x;
	mad.lo.s32 	%r1, %r209, %r208, %r210;
	setp.ge.u32 	%p5, %r1, %r201;
	@%p5 bra 	$L__BB1_109;

	cvta.to.global.u64 	%rd65, %rd60;
	mul.wide.u32 	%rd66, %r1, 4;
	add.s64 	%rd67, %rd65, %rd66;
	ld.global.u32 	%r2, [%rd67];
	mul.lo.s32 	%r3, %r2, %r2;
	add.s32 	%r214, %r3, 2;
	mad.lo.s32 	%r215, %r214, %r3, 2;
	mul.lo.s32 	%r216, %r215, %r214;
	mad.lo.s32 	%r217, %r216, %r3, 2;
	mul.lo.s32 	%r218, %r217, %r216;
	mad.lo.s32 	%r219, %r218, %r3, 2;
	mul.lo.s32 	%r220, %r219, %r218;
	mad.lo.s32 	%r221, %r220, %r3, 2;
	mul.lo.s32 	%r4, %r221, %r220;
	mov.u32 	%r222, %ctaid.y;
	mul.lo.s32 	%r891, %r222, %r204;
	add.s32 	%r223, %r891, %r204;
	min.u32 	%r6, %r223, %r203;
	mul.lo.s32 	%r7, %r205, %r203;
	mad.lo.s32 	%r889, %r891, %r205, %r1;
	mul.wide.u32 	%rd68, %r891, 24;
	add.s64 	%rd267, %rd3, %rd68;
	setp.ge.u32 	%p7, %r891, %r6;
	mov.pred 	%p157, -1;
	mov.u32 	%r836, 0;
	mov.u32 	%r837, %r836;
	@%p7 bra 	$L__BB1_12;

	cvt.u64.u32 	%rd6, %r3;
	mov.u32 	%r224, 0;
	mov.u32 	%r836, %r224;

$L__BB1_3:
	ld.global.u32 	%r13, [%rd267];
	setp.eq.s32 	%p8, %r836, %r13;
	mov.u32 	%r837, %r224;
	@%p8 bra 	$L__BB1_11;

	mov.u32 	%r833, %r2;
	mov.u32 	%r834, %r13;

$L__BB1_5:
	neg.s32 	%r227, %r834;
	and.b32  	%r228, %r834, %r227;
	clz.b32 	%r229, %r228;
	mov.u32 	%r230, 31;
	sub.s32 	%r231, %r230, %r229;
	shr.u32 	%r232, %r834, %r231;
	min.u32 	%r16, %r833, %r232;
	max.u32 	%r233, %r833, %r232;
	sub.s32 	%r834, %r233, %r16;
	setp.ne.s32 	%p9, %r834, 0;
	mov.u32 	%r833, %r16;
	@%p9 bra 	$L__BB1_5;

	setp.ne.s32 	%p10, %r16, 1;
	mov.u32 	%r836, %r13;
	mov.u32 	%r837, %r224;
	@%p10 bra 	$L__BB1_11;

	ld.global.u64 	%rd8, [%rd267+8];
	and.b64  	%rd69, %rd8, -4294967296;
	setp.eq.s64 	%p11, %rd69, 0;
	@%p11 bra 	$L__BB1_9;

	rem.u64 	%rd266, %rd8, %rd6;
	bra.uni 	$L__BB1_10;

$L__BB1_9:
	cvt.u32.u64 	%r235, %rd6;
	cvt.u32.u64 	%r236, %rd8;
	rem.u32 	%r237, %r236, %r235;
	cvt.u64.u32 	%rd266, %r237;

$L__BB1_10:
	cvt.u32.u64 	%r850, %rd266;
	mov.u32 	%r836, %r13;
	mov.u32 	%r837, %r850;

$L__BB1_11:
	mul.wide.u32 	%rd70, %r889, 4;
	add.s64 	%rd71, %rd4, %rd70;
	st.global.u32 	[%rd71], %r837;
	add.s32 	%r889, %r889, %r205;
	add.s64 	%rd267, %rd267, 24;
	add.s32 	%r891, %r891, 1;
	setp.lt.u32 	%p12, %r891, %r6;
	setp.eq.s32 	%p157, %r837, 0;
	and.pred  	%p13, %p12, %p157;
	@%p13 bra 	$L__BB1_3;

$L__BB1_12:
	@%p157 bra 	$L__BB1_109;

	add.s32 	%r29, %r891, -1;
	setp.ge.u32 	%p14, %r891, %r6;
	@%p14 bra 	$L__BB1_24;

	cvt.u64.u32 	%rd14, %r3;

$L__BB1_15:
	ld.global.u32 	%r35, [%rd267];
	setp.eq.s32 	%p15, %r836, %r35;
	@%p15 bra 	$L__BB1_23;

	mov.u32 	%r848, %r2;
	mov.u32 	%r849, %r35;

$L__BB1_17:
	neg.s32 	%r238, %r849;
	and.b32  	%r239, %r849, %r238;
	clz.b32 	%r240, %r239;
	mov.u32 	%r241, 31;
	sub.s32 	%r242, %r241, %r240;
	shr.u32 	%r243, %r849, %r242;
	min.u32 	%r38, %r848, %r243;
	max.u32 	%r244, %r848, %r243;
	sub.s32 	%r849, %r244, %r38;
	setp.ne.s32 	%p16, %r849, 0;
	mov.u32 	%r848, %r38;
	@%p16 bra 	$L__BB1_17;

	setp.ne.s32 	%p17, %r38, 1;
	mov.u32 	%r837, 0;
	mov.u32 	%r836, %r35;
	@%p17 bra 	$L__BB1_23;

	ld.global.u64 	%rd16, [%rd267+8];
	and.b64  	%rd72, %rd16, -4294967296;
	setp.eq.s64 	%p18, %rd72, 0;
	@%p18 bra 	$L__BB1_21;

	rem.u64 	%rd269, %rd16, %rd14;
	bra.uni 	$L__BB1_22;

$L__BB1_21:
	cvt.u32.u64 	%r246, %rd14;
	cvt.u32.u64 	%r247, %rd16;
	rem.u32 	%r248, %r247, %r246;
	cvt.u64.u32 	%rd269, %r248;

$L__BB1_22:
	cvt.u32.u64 	%r258, %rd269;
	mul.lo.s32 	%r249, %r850, %r258;
	mul.hi.u32 	%r250, %r850, %r258;
	mul.lo.s32 	%r259, %r249, %r4;
	mul.lo.s32 	%r252, %r259, %r3;
	mul.hi.u32 	%r253, %r259, %r3;
	mov.u32 	%r251, 0;
	// begin inline asm
	add.cc.u32 %r249, %r249, %r252;   /* inline */   
	addc.cc.u32 %r250, %r250, %r253;   /* inline */   
	addc.u32 %r251, %r251, %r251;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p19, %r251, 0;
	setp.ge.u32 	%p20, %r250, %r3;
	or.pred  	%p21, %p20, %p19;
	selp.b32 	%r260, %r3, 0, %p21;
	sub.s32 	%r850, %r250, %r260;
	mov.u32 	%r836, %r35;
	mov.u32 	%r837, %r850;

$L__BB1_23:
	mul.wide.u32 	%rd73, %r889, 4;
	add.s64 	%rd74, %rd4, %rd73;
	st.global.u32 	[%rd74], %r837;
	add.s32 	%r889, %r889, %r205;
	add.s64 	%rd267, %rd267, 24;
	add.s32 	%r891, %r891, 1;
	setp.lt.u32 	%p22, %r891, %r6;
	@%p22 bra 	$L__BB1_15;

$L__BB1_24:
	setp.lt.u32 	%p24, %r850, 2;
	mov.pred 	%p158, -1;
	mov.u32 	%r863, 1;
	@%p24 bra 	$L__BB1_38;

	mov.u32 	%r863, 1;
	mov.u32 	%r856, 0;
	mov.u32 	%r862, %r850;
	mov.u32 	%r859, %r856;
	mov.u32 	%r860, %r3;

$L__BB1_26:
	mov.u32 	%r51, %r860;
	mov.u32 	%r50, %r859;
	mov.u32 	%r859, %r863;
	mov.u32 	%r860, %r862;
	mov.u32 	%r49, %r856;
	sub.s32 	%r862, %r51, %r860;
	sub.s32 	%r55, %r862, %r860;
	setp.lt.u32 	%p25, %r862, %r860;
	mov.u32 	%r861, %r859;
	@%p25 bra 	$L__BB1_36;

	shl.b32 	%r861, %r859, 1;
	sub.s32 	%r57, %r55, %r860;
	setp.lt.u32 	%p26, %r55, %r860;
	mov.u32 	%r862, %r55;
	@%p26 bra 	$L__BB1_36;

	mul.lo.s32 	%r861, %r859, 3;
	sub.s32 	%r59, %r57, %r860;
	setp.lt.u32 	%p27, %r57, %r860;
	mov.u32 	%r862, %r57;
	@%p27 bra 	$L__BB1_36;

	shl.b32 	%r861, %r859, 2;
	sub.s32 	%r61, %r59, %r860;
	setp.lt.u32 	%p28, %r59, %r860;
	mov.u32 	%r862, %r59;
	@%p28 bra 	$L__BB1_36;

	mul.lo.s32 	%r861, %r859, 5;
	sub.s32 	%r63, %r61, %r860;
	setp.lt.u32 	%p29, %r61, %r860;
	mov.u32 	%r862, %r61;
	@%p29 bra 	$L__BB1_36;

	mul.lo.s32 	%r861, %r859, 6;
	sub.s32 	%r65, %r63, %r860;
	setp.lt.u32 	%p30, %r63, %r860;
	mov.u32 	%r862, %r63;
	@%p30 bra 	$L__BB1_36;

	mul.lo.s32 	%r861, %r859, 7;
	sub.s32 	%r67, %r65, %r860;
	setp.lt.u32 	%p31, %r65, %r860;
	mov.u32 	%r862, %r65;
	@%p31 bra 	$L__BB1_36;

	shl.b32 	%r861, %r859, 3;
	sub.s32 	%r69, %r67, %r860;
	setp.lt.u32 	%p32, %r67, %r860;
	mov.u32 	%r862, %r67;
	@%p32 bra 	$L__BB1_36;

	mul.lo.s32 	%r861, %r859, 9;
	setp.lt.u32 	%p33, %r69, %r860;
	mov.u32 	%r862, %r69;
	@%p33 bra 	$L__BB1_36;

	div.u32 	%r265, %r51, %r860;
	mul.lo.s32 	%r266, %r265, %r860;
	sub.s32 	%r862, %r51, %r266;
	mul.lo.s32 	%r861, %r265, %r859;

$L__BB1_36:
	add.s32 	%r863, %r861, %r50;
	not.b32 	%r856, %r49;
	setp.gt.u32 	%p34, %r862, 1;
	@%p34 bra 	$L__BB1_26;

	setp.eq.s32 	%p158, %r49, -1;

$L__BB1_38:
	sub.s32 	%r280, %r3, %r863;
	selp.b32 	%r281, %r863, %r280, %p158;
	cvt.u64.u32 	%rd21, %r3;
	mov.u64 	%rd75, -9223372036854775808;
	rem.u64 	%rd76, %rd75, %rd21;
	cvt.u32.u64 	%r282, %rd76;
	shl.b32 	%r283, %r282, 1;
	setp.lt.u32 	%p35, %r283, %r282;
	selp.b32 	%r284, %r3, 0, %p35;
	mov.u32 	%r273, 0;
	sub.s32 	%r268, %r283, %r284;
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r267, %r268, %r3;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r267, %r267, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r271, %r267, %r281;
	mul.hi.u32 	%r272, %r281, %r267;
	mul.lo.s32 	%r285, %r271, %r4;
	mul.lo.s32 	%r274, %r285, %r3;
	mul.hi.u32 	%r275, %r285, %r3;
	// begin inline asm
	add.cc.u32 %r271, %r271, %r274;   /* inline */   
	addc.cc.u32 %r272, %r272, %r275;   /* inline */   
	addc.u32 %r273, %r273, %r273;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p36, %r273, 0;
	setp.ge.u32 	%p37, %r272, %r3;
	or.pred  	%p38, %p37, %p36;
	selp.b32 	%r286, %r3, 0, %p38;
	sub.s32 	%r893, %r272, %r286;

$L__BB1_39:
	mov.u32 	%r80, %r891;
	sub.s32 	%r889, %r889, %r205;
	add.s32 	%r891, %r80, -1;
	setp.le.u32 	%p39, %r891, %r29;
	@%p39 bra 	$L__BB1_41;

	mul.wide.u32 	%rd77, %r889, 4;
	add.s64 	%rd78, %rd4, %rd77;
	ld.global.u32 	%r287, [%rd78];
	setp.eq.s32 	%p40, %r287, 0;
	@%p40 bra 	$L__BB1_39;

$L__BB1_41:
	add.s32 	%r868, %r80, -2;
	setp.lt.s32 	%p41, %r868, %r29;
	@%p41 bra 	$L__BB1_79;

	shr.u32 	%r84, %r3, 1;
	mov.u32 	%r892, 1;
	shr.u32 	%r289, %r207, 1;
	cvt.u64.u32 	%rd79, %r289;
	mul.lo.s64 	%rd22, %rd21, %rd79;
	add.s32 	%r85, %r207, -1;
	add.s32 	%r86, %r202, -1;
	and.b32  	%r87, %r207, 3;
	sub.s32 	%r88, %r207, %r87;
	and.b32  	%r89, %r202, 3;
	sub.s32 	%r90, %r202, %r89;
	mov.u32 	%r866, %r889;

$L__BB1_43:
	sub.s32 	%r866, %r866, %r205;
	mul.wide.u32 	%rd80, %r866, 4;
	add.s64 	%rd81, %rd4, %rd80;
	ld.global.u32 	%r99, [%rd81];
	setp.eq.s32 	%p42, %r99, 0;
	@%p42 bra 	$L__BB1_78;

	setp.eq.s32 	%p43, %r99, %r850;
	@%p43 bra 	$L__BB1_77;
	bra.uni 	$L__BB1_45;

$L__BB1_77:
	add.s32 	%r892, %r892, 1;
	bra.uni 	$L__BB1_78;

$L__BB1_45:
	mul.lo.s32 	%r290, %r99, %r893;
	mul.lo.s32 	%r299, %r290, %r4;
	mul.lo.s32 	%r293, %r299, %r3;
	mul.hi.u32 	%r294, %r299, %r3;
	mul.hi.u32 	%r291, %r99, %r893;
	mov.u32 	%r292, 0;
	// begin inline asm
	add.cc.u32 %r290, %r290, %r293;   /* inline */   
	addc.cc.u32 %r291, %r291, %r294;   /* inline */   
	addc.u32 %r292, %r292, %r292;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p44, %r292, 0;
	setp.ge.u32 	%p45, %r291, %r3;
	or.pred  	%p46, %p45, %p44;
	selp.b32 	%r300, %r3, 0, %p46;
	sub.s32 	%r100, %r291, %r300;
	mul.wide.u32 	%rd82, %r891, 24;
	add.s64 	%rd83, %rd3, %rd82;
	ld.global.u64 	%rd23, [%rd83+8];
	and.b64  	%rd84, %rd23, -4294967296;
	setp.eq.s64 	%p47, %rd84, 0;
	@%p47 bra 	$L__BB1_47;

	rem.u64 	%rd270, %rd23, %rd21;
	bra.uni 	$L__BB1_48;

$L__BB1_47:
	cvt.u32.u64 	%r301, %rd21;
	cvt.u32.u64 	%r302, %rd23;
	rem.u32 	%r303, %r302, %r301;
	cvt.u64.u32 	%rd270, %r303;

$L__BB1_48:
	cvt.u32.u64 	%r313, %rd270;
	mul.lo.s32 	%r304, %r893, %r313;
	mul.hi.u32 	%r305, %r893, %r313;
	mul.lo.s32 	%r314, %r304, %r4;
	mul.lo.s32 	%r307, %r314, %r3;
	mul.hi.u32 	%r308, %r314, %r3;
	mov.u32 	%r306, 0;
	// begin inline asm
	add.cc.u32 %r304, %r304, %r307;   /* inline */   
	addc.cc.u32 %r305, %r305, %r308;   /* inline */   
	addc.u32 %r306, %r306, %r306;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p48, %r306, 0;
	setp.ge.u32 	%p49, %r305, %r3;
	or.pred  	%p50, %p49, %p48;
	selp.b32 	%r315, %r3, 0, %p50;
	sub.s32 	%r893, %r305, %r315;
	mov.u32 	%r873, %r889;
	mov.u32 	%r874, %r891;
	mov.u32 	%r875, %r892;

$L__BB1_49:
	mul.wide.u32 	%rd85, %r874, 24;
	add.s64 	%rd86, %rd3, %rd85;
	ld.global.u64 	%rd27, [%rd86+16];
	and.b64  	%rd87, %rd27, -4294967296;
	setp.eq.s64 	%p51, %rd87, 0;
	@%p51 bra 	$L__BB1_51;

	rem.u64 	%rd271, %rd27, %rd21;
	bra.uni 	$L__BB1_52;

$L__BB1_51:
	cvt.u32.u64 	%r316, %rd21;
	cvt.u32.u64 	%r317, %rd27;
	rem.u32 	%r318, %r317, %r316;
	cvt.u64.u32 	%rd271, %r318;

$L__BB1_52:
	cvt.u32.u64 	%r105, %rd271;
	setp.eq.s32 	%p52, %r202, 0;
	@%p52 bra 	$L__BB1_76;

	ld.param.u32 	%r828, [sieve_kernel_trans_pp32_r64_param_10];
	shl.b32 	%r319, %r874, %r828;
	or.b32  	%r106, %r319, %r2;
	setp.eq.s32 	%p53, %r207, 1;
	@%p53 bra 	$L__BB1_60;

	setp.ne.s32 	%p54, %r207, 0;
	@%p54 bra 	$L__BB1_67;

	setp.lt.u32 	%p55, %r86, 3;
	@%p55 bra 	$L__BB1_58;

	mov.u32 	%r876, %r1;
	mov.u32 	%r877, %r90;

$L__BB1_57:
	add.s32 	%r372, %r876, %r201;
	add.s32 	%r373, %r372, %r201;
	add.s32 	%r374, %r373, %r201;
	add.s32 	%r876, %r374, %r201;
	add.s32 	%r877, %r877, -4;
	setp.ne.s32 	%p56, %r877, 0;
	@%p56 bra 	$L__BB1_57;

$L__BB1_58:
	setp.eq.s32 	%p57, %r89, 0;
	@%p57 bra 	$L__BB1_76;

	setp.eq.s32 	%p58, %r89, 1;
	bra.uni 	$L__BB1_76;

$L__BB1_60:
	setp.lt.u32 	%p60, %r86, 3;
	mov.u32 	%r881, %r1;
	mov.u32 	%r882, %r873;
	@%p60 bra 	$L__BB1_63;

	mov.u32 	%r881, %r1;
	mov.u32 	%r882, %r873;
	mov.u32 	%r880, %r90;

$L__BB1_62:
	mul.wide.u32 	%rd88, %r881, 4;
	add.s64 	%rd89, %rd2, %rd88;
	ld.global.u32 	%r415, [%rd89];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r414, %r415, %r105;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r414, %r414, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r418, %r414, %r100;
	mul.hi.u32 	%r419, %r414, %r100;
	mul.lo.s32 	%r466, %r418, %r4;
	mul.lo.s32 	%r421, %r466, %r3;
	mul.hi.u32 	%r422, %r466, %r3;
	mov.u32 	%r459, 0;
	mov.u32 	%r420, %r459;
	// begin inline asm
	add.cc.u32 %r418, %r418, %r421;   /* inline */   
	addc.cc.u32 %r419, %r419, %r422;   /* inline */   
	addc.u32 %r420, %r420, %r420;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p61, %r420, 0;
	setp.ge.u32 	%p62, %r419, %r3;
	or.pred  	%p63, %p62, %p61;
	selp.b32 	%r467, %r3, 0, %p63;
	sub.s32 	%r468, %r419, %r467;
	setp.gt.u32 	%p64, %r468, %r84;
	selp.b32 	%r469, %r3, 0, %p64;
	sub.s32 	%r470, %r468, %r469;
	mul.wide.u32 	%rd90, %r882, 4;
	add.s64 	%rd91, %rd4, %rd90;
	st.global.u32 	[%rd91], %r106;
	cvt.u64.u32 	%rd92, %r470;
	mul.wide.u32 	%rd93, %r882, 8;
	add.s64 	%rd94, %rd1, %rd93;
	st.global.u64 	[%rd94], %rd92;
	add.s32 	%r471, %r881, %r201;
	mul.wide.u32 	%rd95, %r471, 4;
	add.s64 	%rd96, %rd2, %rd95;
	ld.global.u32 	%r428, [%rd96];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r427, %r428, %r105;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r427, %r427, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r431, %r427, %r100;
	mul.hi.u32 	%r432, %r427, %r100;
	mul.lo.s32 	%r472, %r431, %r4;
	mul.lo.s32 	%r434, %r472, %r3;
	mul.hi.u32 	%r435, %r472, %r3;
	mov.u32 	%r433, %r459;
	// begin inline asm
	add.cc.u32 %r431, %r431, %r434;   /* inline */   
	addc.cc.u32 %r432, %r432, %r435;   /* inline */   
	addc.u32 %r433, %r433, %r433;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p65, %r433, 0;
	setp.ge.u32 	%p66, %r432, %r3;
	or.pred  	%p67, %p66, %p65;
	selp.b32 	%r473, %r3, 0, %p67;
	sub.s32 	%r474, %r432, %r473;
	setp.gt.u32 	%p68, %r474, %r84;
	selp.b32 	%r475, %r3, 0, %p68;
	sub.s32 	%r476, %r474, %r475;
	add.s32 	%r477, %r882, %r201;
	mul.wide.u32 	%rd97, %r477, 4;
	add.s64 	%rd98, %rd4, %rd97;
	st.global.u32 	[%rd98], %r106;
	cvt.u64.u32 	%rd99, %r476;
	mul.wide.u32 	%rd100, %r477, 8;
	add.s64 	%rd101, %rd1, %rd100;
	st.global.u64 	[%rd101], %rd99;
	add.s32 	%r478, %r477, %r201;
	add.s32 	%r479, %r471, %r201;
	mul.wide.u32 	%rd102, %r479, 4;
	add.s64 	%rd103, %rd2, %rd102;
	ld.global.u32 	%r441, [%rd103];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r440, %r441, %r105;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r440, %r440, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r444, %r440, %r100;
	mul.hi.u32 	%r445, %r440, %r100;
	mul.lo.s32 	%r480, %r444, %r4;
	mul.lo.s32 	%r447, %r480, %r3;
	mul.hi.u32 	%r448, %r480, %r3;
	mov.u32 	%r446, %r459;
	// begin inline asm
	add.cc.u32 %r444, %r444, %r447;   /* inline */   
	addc.cc.u32 %r445, %r445, %r448;   /* inline */   
	addc.u32 %r446, %r446, %r446;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p69, %r446, 0;
	setp.ge.u32 	%p70, %r445, %r3;
	or.pred  	%p71, %p70, %p69;
	selp.b32 	%r481, %r3, 0, %p71;
	sub.s32 	%r482, %r445, %r481;
	setp.gt.u32 	%p72, %r482, %r84;
	selp.b32 	%r483, %r3, 0, %p72;
	sub.s32 	%r484, %r482, %r483;
	mul.wide.u32 	%rd104, %r478, 4;
	add.s64 	%rd105, %rd4, %rd104;
	st.global.u32 	[%rd105], %r106;
	cvt.u64.u32 	%rd106, %r484;
	mul.wide.u32 	%rd107, %r478, 8;
	add.s64 	%rd108, %rd1, %rd107;
	st.global.u64 	[%rd108], %rd106;
	add.s32 	%r485, %r478, %r201;
	add.s32 	%r486, %r479, %r201;
	mul.wide.u32 	%rd109, %r486, 4;
	add.s64 	%rd110, %rd2, %rd109;
	ld.global.u32 	%r454, [%rd110];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r453, %r454, %r105;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r453, %r453, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r457, %r453, %r100;
	mul.hi.u32 	%r458, %r453, %r100;
	mul.lo.s32 	%r487, %r457, %r4;
	mul.lo.s32 	%r460, %r487, %r3;
	mul.hi.u32 	%r461, %r487, %r3;
	// begin inline asm
	add.cc.u32 %r457, %r457, %r460;   /* inline */   
	addc.cc.u32 %r458, %r458, %r461;   /* inline */   
	addc.u32 %r459, %r459, %r459;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p73, %r459, 0;
	setp.ge.u32 	%p74, %r458, %r3;
	or.pred  	%p75, %p74, %p73;
	selp.b32 	%r488, %r3, 0, %p75;
	sub.s32 	%r489, %r458, %r488;
	setp.gt.u32 	%p76, %r489, %r84;
	selp.b32 	%r490, %r3, 0, %p76;
	sub.s32 	%r491, %r489, %r490;
	mul.wide.u32 	%rd111, %r485, 4;
	add.s64 	%rd112, %rd4, %rd111;
	st.global.u32 	[%rd112], %r106;
	cvt.u64.u32 	%rd113, %r491;
	mul.wide.u32 	%rd114, %r485, 8;
	add.s64 	%rd115, %rd1, %rd114;
	st.global.u64 	[%rd115], %rd113;
	add.s32 	%r882, %r485, %r201;
	add.s32 	%r881, %r486, %r201;
	add.s32 	%r880, %r880, -4;
	setp.ne.s32 	%p77, %r880, 0;
	@%p77 bra 	$L__BB1_62;

$L__BB1_63:
	setp.eq.s32 	%p78, %r89, 0;
	@%p78 bra 	$L__BB1_76;

	setp.eq.s32 	%p79, %r89, 1;
	mul.wide.u32 	%rd116, %r881, 4;
	add.s64 	%rd117, %rd2, %rd116;
	ld.global.u32 	%r493, [%rd117];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r492, %r493, %r105;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r492, %r492, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r496, %r492, %r100;
	mul.hi.u32 	%r497, %r492, %r100;
	mul.lo.s32 	%r505, %r496, %r4;
	mul.lo.s32 	%r499, %r505, %r3;
	mul.hi.u32 	%r500, %r505, %r3;
	mov.u32 	%r516, 0;
	mov.u32 	%r498, %r516;
	// begin inline asm
	add.cc.u32 %r496, %r496, %r499;   /* inline */   
	addc.cc.u32 %r497, %r497, %r500;   /* inline */   
	addc.u32 %r498, %r498, %r498;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p80, %r498, 0;
	setp.ge.u32 	%p81, %r497, %r3;
	or.pred  	%p82, %p81, %p80;
	selp.b32 	%r506, %r3, 0, %p82;
	sub.s32 	%r507, %r497, %r506;
	setp.gt.u32 	%p83, %r507, %r84;
	selp.b32 	%r508, %r3, 0, %p83;
	sub.s32 	%r509, %r507, %r508;
	mul.wide.u32 	%rd118, %r882, 4;
	add.s64 	%rd119, %rd4, %rd118;
	st.global.u32 	[%rd119], %r106;
	cvt.u64.u32 	%rd120, %r509;
	mul.wide.u32 	%rd121, %r882, 8;
	add.s64 	%rd122, %rd1, %rd121;
	st.global.u64 	[%rd122], %rd120;
	@%p79 bra 	$L__BB1_76;

	add.s32 	%r121, %r882, %r201;
	add.s32 	%r122, %r881, %r201;
	setp.eq.s32 	%p84, %r89, 2;
	mul.wide.u32 	%rd123, %r122, 4;
	add.s64 	%rd124, %rd2, %rd123;
	ld.global.u32 	%r511, [%rd124];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r510, %r511, %r105;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r510, %r510, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r514, %r510, %r100;
	mul.hi.u32 	%r515, %r510, %r100;
	mul.lo.s32 	%r523, %r514, %r4;
	mul.lo.s32 	%r517, %r523, %r3;
	mul.hi.u32 	%r518, %r523, %r3;
	// begin inline asm
	add.cc.u32 %r514, %r514, %r517;   /* inline */   
	addc.cc.u32 %r515, %r515, %r518;   /* inline */   
	addc.u32 %r516, %r516, %r516;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p85, %r516, 0;
	setp.ge.u32 	%p86, %r515, %r3;
	or.pred  	%p87, %p86, %p85;
	selp.b32 	%r524, %r3, 0, %p87;
	sub.s32 	%r525, %r515, %r524;
	setp.gt.u32 	%p88, %r525, %r84;
	selp.b32 	%r526, %r3, 0, %p88;
	sub.s32 	%r527, %r525, %r526;
	mul.wide.u32 	%rd125, %r121, 4;
	add.s64 	%rd126, %rd4, %rd125;
	st.global.u32 	[%rd126], %r106;
	cvt.u64.u32 	%rd127, %r527;
	mul.wide.u32 	%rd128, %r121, 8;
	add.s64 	%rd129, %rd1, %rd128;
	st.global.u64 	[%rd129], %rd127;
	@%p84 bra 	$L__BB1_76;

	add.s32 	%r541, %r122, %r201;
	mul.wide.u32 	%rd130, %r541, 4;
	add.s64 	%rd131, %rd2, %rd130;
	ld.global.u32 	%r529, [%rd131];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r528, %r529, %r105;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r528, %r528, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r532, %r528, %r100;
	mul.hi.u32 	%r533, %r528, %r100;
	mul.lo.s32 	%r542, %r532, %r4;
	mul.lo.s32 	%r535, %r542, %r3;
	mul.hi.u32 	%r536, %r542, %r3;
	mov.u32 	%r534, 0;
	// begin inline asm
	add.cc.u32 %r532, %r532, %r535;   /* inline */   
	addc.cc.u32 %r533, %r533, %r536;   /* inline */   
	addc.u32 %r534, %r534, %r534;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p89, %r534, 0;
	setp.ge.u32 	%p90, %r533, %r3;
	or.pred  	%p91, %p90, %p89;
	selp.b32 	%r543, %r3, 0, %p91;
	sub.s32 	%r544, %r533, %r543;
	setp.gt.u32 	%p92, %r544, %r84;
	selp.b32 	%r545, %r3, 0, %p92;
	sub.s32 	%r546, %r544, %r545;
	add.s32 	%r547, %r121, %r201;
	mul.wide.u32 	%rd132, %r547, 4;
	add.s64 	%rd133, %rd4, %rd132;
	st.global.u32 	[%rd133], %r106;
	cvt.u64.u32 	%rd134, %r546;
	mul.wide.u32 	%rd135, %r547, 8;
	add.s64 	%rd136, %rd1, %rd135;
	st.global.u64 	[%rd136], %rd134;
	bra.uni 	$L__BB1_76;

$L__BB1_67:
	mov.u32 	%r883, 0;
	mov.u32 	%r884, %r1;
	mov.u32 	%r885, %r873;

$L__BB1_68:
	mov.u32 	%r555, 0;
	mul.wide.u32 	%rd137, %r884, 4;
	add.s64 	%rd138, %rd2, %rd137;
	ld.global.u32 	%r550, [%rd138];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r549, %r550, %r105;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r549, %r549, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r553, %r549, %r100;
	mul.hi.u32 	%r554, %r549, %r100;
	mul.lo.s32 	%r562, %r553, %r4;
	mul.lo.s32 	%r556, %r562, %r3;
	mul.hi.u32 	%r557, %r562, %r3;
	// begin inline asm
	add.cc.u32 %r553, %r553, %r556;   /* inline */   
	addc.cc.u32 %r554, %r554, %r557;   /* inline */   
	addc.u32 %r555, %r555, %r555;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p93, %r555, 0;
	setp.ge.u32 	%p94, %r554, %r3;
	or.pred  	%p95, %p94, %p93;
	selp.b32 	%r563, %r3, 0, %p95;
	sub.s32 	%r564, %r554, %r563;
	cvt.u64.u32 	%rd139, %r564;
	sub.s64 	%rd273, %rd139, %rd22;
	setp.lt.u32 	%p96, %r85, 3;
	mov.u32 	%r888, %r885;
	@%p96 bra 	$L__BB1_71;

	mov.u32 	%r888, %r885;
	mov.u32 	%r887, %r88;

$L__BB1_70:
	mul.wide.u32 	%rd140, %r888, 4;
	add.s64 	%rd141, %rd4, %rd140;
	st.global.u32 	[%rd141], %r106;
	mul.wide.u32 	%rd142, %r888, 8;
	add.s64 	%rd143, %rd1, %rd142;
	st.global.u64 	[%rd143], %rd273;
	add.s32 	%r565, %r888, %r7;
	mul.wide.u32 	%rd144, %r565, 4;
	add.s64 	%rd145, %rd4, %rd144;
	st.global.u32 	[%rd145], %r106;
	mul.wide.u32 	%rd146, %r565, 8;
	add.s64 	%rd147, %rd1, %rd146;
	add.s64 	%rd148, %rd273, %rd21;
	st.global.u64 	[%rd147], %rd148;
	add.s32 	%r566, %r565, %r7;
	add.s64 	%rd149, %rd148, %rd21;
	mul.wide.u32 	%rd150, %r566, 4;
	add.s64 	%rd151, %rd4, %rd150;
	st.global.u32 	[%rd151], %r106;
	mul.wide.u32 	%rd152, %r566, 8;
	add.s64 	%rd153, %rd1, %rd152;
	st.global.u64 	[%rd153], %rd149;
	add.s32 	%r567, %r566, %r7;
	add.s64 	%rd154, %rd149, %rd21;
	mul.wide.u32 	%rd155, %r567, 4;
	add.s64 	%rd156, %rd4, %rd155;
	st.global.u32 	[%rd156], %r106;
	mul.wide.u32 	%rd157, %r567, 8;
	add.s64 	%rd158, %rd1, %rd157;
	st.global.u64 	[%rd158], %rd154;
	add.s32 	%r888, %r567, %r7;
	add.s64 	%rd273, %rd154, %rd21;
	add.s32 	%r887, %r887, -4;
	setp.ne.s32 	%p97, %r887, 0;
	@%p97 bra 	$L__BB1_70;

$L__BB1_71:
	setp.eq.s32 	%p98, %r87, 0;
	@%p98 bra 	$L__BB1_75;

	setp.eq.s32 	%p99, %r87, 1;
	mul.wide.u32 	%rd159, %r888, 4;
	add.s64 	%rd160, %rd4, %rd159;
	st.global.u32 	[%rd160], %r106;
	mul.wide.u32 	%rd161, %r888, 8;
	add.s64 	%rd162, %rd1, %rd161;
	st.global.u64 	[%rd162], %rd273;
	@%p99 bra 	$L__BB1_75;

	add.s32 	%r131, %r888, %r7;
	add.s64 	%rd35, %rd273, %rd21;
	setp.eq.s32 	%p100, %r87, 2;
	mul.wide.u32 	%rd163, %r131, 4;
	add.s64 	%rd164, %rd4, %rd163;
	st.global.u32 	[%rd164], %r106;
	mul.wide.u32 	%rd165, %r131, 8;
	add.s64 	%rd166, %rd1, %rd165;
	st.global.u64 	[%rd166], %rd35;
	@%p100 bra 	$L__BB1_75;

	add.s32 	%r568, %r131, %r7;
	mul.wide.u32 	%rd167, %r568, 4;
	add.s64 	%rd168, %rd4, %rd167;
	st.global.u32 	[%rd168], %r106;
	mul.wide.u32 	%rd169, %r568, 8;
	add.s64 	%rd170, %rd1, %rd169;
	add.s64 	%rd171, %rd35, %rd21;
	st.global.u64 	[%rd170], %rd171;

$L__BB1_75:
	add.s32 	%r885, %r885, %r201;
	add.s32 	%r884, %r884, %r201;
	add.s32 	%r883, %r883, 1;
	setp.lt.u32 	%p101, %r883, %r202;
	@%p101 bra 	$L__BB1_68;

$L__BB1_76:
	add.s32 	%r874, %r874, -1;
	sub.s32 	%r873, %r873, %r205;
	add.s32 	%r875, %r875, -1;
	setp.eq.s32 	%p102, %r875, 0;
	mov.u32 	%r892, 1;
	mov.u32 	%r889, %r866;
	mov.u32 	%r850, %r99;
	mov.u32 	%r891, %r868;
	@%p102 bra 	$L__BB1_78;
	bra.uni 	$L__BB1_49;

$L__BB1_78:
	add.s32 	%r868, %r868, -1;
	setp.ge.s32 	%p103, %r868, %r29;
	@%p103 bra 	$L__BB1_43;

$L__BB1_79:
	setp.lt.s32 	%p104, %r891, %r29;
	@%p104 bra 	$L__BB1_109;

	shr.u32 	%r148, %r3, 1;
	shr.u32 	%r570, %r207, 1;
	cvt.u64.u32 	%rd172, %r570;
	mul.lo.s64 	%rd36, %rd21, %rd172;
	add.s32 	%r149, %r207, -1;
	add.s32 	%r150, %r202, -1;
	and.b32  	%r151, %r202, 3;
	sub.s32 	%r152, %r202, %r151;
	mul.lo.s32 	%r153, %r7, 3;
	shl.b32 	%r154, %r7, 2;
	shl.b32 	%r155, %r7, 1;
	mov.u64 	%rd173, 3;
	sub.s64 	%rd174, %rd173, %rd172;
	mul.lo.s64 	%rd37, %rd174, %rd21;
	shl.b64 	%rd38, %rd21, 2;
	mov.u64 	%rd175, 2;
	sub.s64 	%rd176, %rd175, %rd172;
	mul.lo.s64 	%rd39, %rd176, %rd21;
	mov.u64 	%rd177, 1;
	sub.s64 	%rd178, %rd177, %rd172;
	mul.lo.s64 	%rd40, %rd178, %rd21;
	and.b32  	%r156, %r207, 3;
	sub.s32 	%r157, %r156, %r207;

$L__BB1_81:
	mul.wide.u32 	%rd179, %r891, 24;
	add.s64 	%rd180, %rd3, %rd179;
	ld.global.u64 	%rd41, [%rd180+16];
	and.b64  	%rd181, %rd41, -4294967296;
	setp.eq.s64 	%p105, %rd181, 0;
	@%p105 bra 	$L__BB1_83;

	rem.u64 	%rd274, %rd41, %rd21;
	bra.uni 	$L__BB1_84;

$L__BB1_83:
	cvt.u32.u64 	%r571, %rd21;
	cvt.u32.u64 	%r572, %rd41;
	rem.u32 	%r573, %r572, %r571;
	cvt.u64.u32 	%rd274, %r573;

$L__BB1_84:
	cvt.u32.u64 	%r160, %rd274;
	setp.eq.s32 	%p106, %r202, 0;
	@%p106 bra 	$L__BB1_108;

	ld.param.u32 	%r826, [sieve_kernel_trans_pp32_r64_param_10];
	shl.b32 	%r574, %r891, %r826;
	or.b32  	%r161, %r574, %r2;
	setp.eq.s32 	%p107, %r207, 1;
	@%p107 bra 	$L__BB1_92;

	setp.ne.s32 	%p108, %r207, 0;
	@%p108 bra 	$L__BB1_99;

	setp.lt.u32 	%p109, %r150, 3;
	@%p109 bra 	$L__BB1_90;

	mov.u32 	%r899, %r1;
	mov.u32 	%r900, %r152;

$L__BB1_89:
	add.s32 	%r627, %r899, %r201;
	add.s32 	%r628, %r627, %r201;
	add.s32 	%r629, %r628, %r201;
	add.s32 	%r899, %r629, %r201;
	add.s32 	%r900, %r900, -4;
	setp.ne.s32 	%p110, %r900, 0;
	@%p110 bra 	$L__BB1_89;

$L__BB1_90:
	setp.eq.s32 	%p111, %r151, 0;
	@%p111 bra 	$L__BB1_108;

	setp.eq.s32 	%p112, %r151, 1;
	bra.uni 	$L__BB1_108;

$L__BB1_92:
	setp.lt.u32 	%p114, %r150, 3;
	mov.u32 	%r904, %r1;
	mov.u32 	%r905, %r889;
	@%p114 bra 	$L__BB1_95;

	mov.u32 	%r904, %r1;
	mov.u32 	%r905, %r889;
	mov.u32 	%r903, %r152;

$L__BB1_94:
	mul.wide.u32 	%rd182, %r904, 4;
	add.s64 	%rd183, %rd2, %rd182;
	ld.global.u32 	%r670, [%rd183];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r669, %r670, %r160;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r669, %r669, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r673, %r669, %r893;
	mul.hi.u32 	%r674, %r669, %r893;
	mul.lo.s32 	%r721, %r673, %r4;
	mul.lo.s32 	%r676, %r721, %r3;
	mul.hi.u32 	%r677, %r721, %r3;
	mov.u32 	%r714, 0;
	mov.u32 	%r675, %r714;
	// begin inline asm
	add.cc.u32 %r673, %r673, %r676;   /* inline */   
	addc.cc.u32 %r674, %r674, %r677;   /* inline */   
	addc.u32 %r675, %r675, %r675;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p115, %r675, 0;
	setp.ge.u32 	%p116, %r674, %r3;
	or.pred  	%p117, %p116, %p115;
	selp.b32 	%r722, %r3, 0, %p117;
	sub.s32 	%r723, %r674, %r722;
	setp.gt.u32 	%p118, %r723, %r148;
	selp.b32 	%r724, %r3, 0, %p118;
	sub.s32 	%r725, %r723, %r724;
	mul.wide.u32 	%rd184, %r905, 4;
	add.s64 	%rd185, %rd4, %rd184;
	st.global.u32 	[%rd185], %r161;
	cvt.u64.u32 	%rd186, %r725;
	mul.wide.u32 	%rd187, %r905, 8;
	add.s64 	%rd188, %rd1, %rd187;
	st.global.u64 	[%rd188], %rd186;
	add.s32 	%r726, %r904, %r201;
	mul.wide.u32 	%rd189, %r726, 4;
	add.s64 	%rd190, %rd2, %rd189;
	ld.global.u32 	%r683, [%rd190];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r682, %r683, %r160;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r682, %r682, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r686, %r682, %r893;
	mul.hi.u32 	%r687, %r682, %r893;
	mul.lo.s32 	%r727, %r686, %r4;
	mul.lo.s32 	%r689, %r727, %r3;
	mul.hi.u32 	%r690, %r727, %r3;
	mov.u32 	%r688, %r714;
	// begin inline asm
	add.cc.u32 %r686, %r686, %r689;   /* inline */   
	addc.cc.u32 %r687, %r687, %r690;   /* inline */   
	addc.u32 %r688, %r688, %r688;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p119, %r688, 0;
	setp.ge.u32 	%p120, %r687, %r3;
	or.pred  	%p121, %p120, %p119;
	selp.b32 	%r728, %r3, 0, %p121;
	sub.s32 	%r729, %r687, %r728;
	setp.gt.u32 	%p122, %r729, %r148;
	selp.b32 	%r730, %r3, 0, %p122;
	sub.s32 	%r731, %r729, %r730;
	add.s32 	%r732, %r905, %r201;
	mul.wide.u32 	%rd191, %r732, 4;
	add.s64 	%rd192, %rd4, %rd191;
	st.global.u32 	[%rd192], %r161;
	cvt.u64.u32 	%rd193, %r731;
	mul.wide.u32 	%rd194, %r732, 8;
	add.s64 	%rd195, %rd1, %rd194;
	st.global.u64 	[%rd195], %rd193;
	add.s32 	%r733, %r732, %r201;
	add.s32 	%r734, %r726, %r201;
	mul.wide.u32 	%rd196, %r734, 4;
	add.s64 	%rd197, %rd2, %rd196;
	ld.global.u32 	%r696, [%rd197];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r695, %r696, %r160;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r695, %r695, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r699, %r695, %r893;
	mul.hi.u32 	%r700, %r695, %r893;
	mul.lo.s32 	%r735, %r699, %r4;
	mul.lo.s32 	%r702, %r735, %r3;
	mul.hi.u32 	%r703, %r735, %r3;
	mov.u32 	%r701, %r714;
	// begin inline asm
	add.cc.u32 %r699, %r699, %r702;   /* inline */   
	addc.cc.u32 %r700, %r700, %r703;   /* inline */   
	addc.u32 %r701, %r701, %r701;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p123, %r701, 0;
	setp.ge.u32 	%p124, %r700, %r3;
	or.pred  	%p125, %p124, %p123;
	selp.b32 	%r736, %r3, 0, %p125;
	sub.s32 	%r737, %r700, %r736;
	setp.gt.u32 	%p126, %r737, %r148;
	selp.b32 	%r738, %r3, 0, %p126;
	sub.s32 	%r739, %r737, %r738;
	mul.wide.u32 	%rd198, %r733, 4;
	add.s64 	%rd199, %rd4, %rd198;
	st.global.u32 	[%rd199], %r161;
	cvt.u64.u32 	%rd200, %r739;
	mul.wide.u32 	%rd201, %r733, 8;
	add.s64 	%rd202, %rd1, %rd201;
	st.global.u64 	[%rd202], %rd200;
	add.s32 	%r740, %r733, %r201;
	add.s32 	%r741, %r734, %r201;
	mul.wide.u32 	%rd203, %r741, 4;
	add.s64 	%rd204, %rd2, %rd203;
	ld.global.u32 	%r709, [%rd204];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r708, %r709, %r160;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r708, %r708, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r712, %r708, %r893;
	mul.hi.u32 	%r713, %r708, %r893;
	mul.lo.s32 	%r742, %r712, %r4;
	mul.lo.s32 	%r715, %r742, %r3;
	mul.hi.u32 	%r716, %r742, %r3;
	// begin inline asm
	add.cc.u32 %r712, %r712, %r715;   /* inline */   
	addc.cc.u32 %r713, %r713, %r716;   /* inline */   
	addc.u32 %r714, %r714, %r714;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p127, %r714, 0;
	setp.ge.u32 	%p128, %r713, %r3;
	or.pred  	%p129, %p128, %p127;
	selp.b32 	%r743, %r3, 0, %p129;
	sub.s32 	%r744, %r713, %r743;
	setp.gt.u32 	%p130, %r744, %r148;
	selp.b32 	%r745, %r3, 0, %p130;
	sub.s32 	%r746, %r744, %r745;
	mul.wide.u32 	%rd205, %r740, 4;
	add.s64 	%rd206, %rd4, %rd205;
	st.global.u32 	[%rd206], %r161;
	cvt.u64.u32 	%rd207, %r746;
	mul.wide.u32 	%rd208, %r740, 8;
	add.s64 	%rd209, %rd1, %rd208;
	st.global.u64 	[%rd209], %rd207;
	add.s32 	%r905, %r740, %r201;
	add.s32 	%r904, %r741, %r201;
	add.s32 	%r903, %r903, -4;
	setp.ne.s32 	%p131, %r903, 0;
	@%p131 bra 	$L__BB1_94;

$L__BB1_95:
	setp.eq.s32 	%p132, %r151, 0;
	@%p132 bra 	$L__BB1_108;

	setp.eq.s32 	%p133, %r151, 1;
	mul.wide.u32 	%rd210, %r904, 4;
	add.s64 	%rd211, %rd2, %rd210;
	ld.global.u32 	%r748, [%rd211];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r747, %r748, %r160;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r747, %r747, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r751, %r747, %r893;
	mul.hi.u32 	%r752, %r747, %r893;
	mul.lo.s32 	%r760, %r751, %r4;
	mul.lo.s32 	%r754, %r760, %r3;
	mul.hi.u32 	%r755, %r760, %r3;
	mov.u32 	%r771, 0;
	mov.u32 	%r753, %r771;
	// begin inline asm
	add.cc.u32 %r751, %r751, %r754;   /* inline */   
	addc.cc.u32 %r752, %r752, %r755;   /* inline */   
	addc.u32 %r753, %r753, %r753;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p134, %r753, 0;
	setp.ge.u32 	%p135, %r752, %r3;
	or.pred  	%p136, %p135, %p134;
	selp.b32 	%r761, %r3, 0, %p136;
	sub.s32 	%r762, %r752, %r761;
	setp.gt.u32 	%p137, %r762, %r148;
	selp.b32 	%r763, %r3, 0, %p137;
	sub.s32 	%r764, %r762, %r763;
	mul.wide.u32 	%rd212, %r905, 4;
	add.s64 	%rd213, %rd4, %rd212;
	st.global.u32 	[%rd213], %r161;
	cvt.u64.u32 	%rd214, %r764;
	mul.wide.u32 	%rd215, %r905, 8;
	add.s64 	%rd216, %rd1, %rd215;
	st.global.u64 	[%rd216], %rd214;
	@%p133 bra 	$L__BB1_108;

	add.s32 	%r176, %r905, %r201;
	add.s32 	%r177, %r904, %r201;
	setp.eq.s32 	%p138, %r151, 2;
	mul.wide.u32 	%rd217, %r177, 4;
	add.s64 	%rd218, %rd2, %rd217;
	ld.global.u32 	%r766, [%rd218];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r765, %r766, %r160;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r765, %r765, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r769, %r765, %r893;
	mul.hi.u32 	%r770, %r765, %r893;
	mul.lo.s32 	%r778, %r769, %r4;
	mul.lo.s32 	%r772, %r778, %r3;
	mul.hi.u32 	%r773, %r778, %r3;
	// begin inline asm
	add.cc.u32 %r769, %r769, %r772;   /* inline */   
	addc.cc.u32 %r770, %r770, %r773;   /* inline */   
	addc.u32 %r771, %r771, %r771;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p139, %r771, 0;
	setp.ge.u32 	%p140, %r770, %r3;
	or.pred  	%p141, %p140, %p139;
	selp.b32 	%r779, %r3, 0, %p141;
	sub.s32 	%r780, %r770, %r779;
	setp.gt.u32 	%p142, %r780, %r148;
	selp.b32 	%r781, %r3, 0, %p142;
	sub.s32 	%r782, %r780, %r781;
	mul.wide.u32 	%rd219, %r176, 4;
	add.s64 	%rd220, %rd4, %rd219;
	st.global.u32 	[%rd220], %r161;
	cvt.u64.u32 	%rd221, %r782;
	mul.wide.u32 	%rd222, %r176, 8;
	add.s64 	%rd223, %rd1, %rd222;
	st.global.u64 	[%rd223], %rd221;
	@%p138 bra 	$L__BB1_108;

	add.s32 	%r796, %r177, %r201;
	mul.wide.u32 	%rd224, %r796, 4;
	add.s64 	%rd225, %rd2, %rd224;
	ld.global.u32 	%r784, [%rd225];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r783, %r784, %r160;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r783, %r783, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r787, %r783, %r893;
	mul.hi.u32 	%r788, %r783, %r893;
	mul.lo.s32 	%r797, %r787, %r4;
	mul.lo.s32 	%r790, %r797, %r3;
	mul.hi.u32 	%r791, %r797, %r3;
	mov.u32 	%r789, 0;
	// begin inline asm
	add.cc.u32 %r787, %r787, %r790;   /* inline */   
	addc.cc.u32 %r788, %r788, %r791;   /* inline */   
	addc.u32 %r789, %r789, %r789;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p143, %r789, 0;
	setp.ge.u32 	%p144, %r788, %r3;
	or.pred  	%p145, %p144, %p143;
	selp.b32 	%r798, %r3, 0, %p145;
	sub.s32 	%r799, %r788, %r798;
	setp.gt.u32 	%p146, %r799, %r148;
	selp.b32 	%r800, %r3, 0, %p146;
	sub.s32 	%r801, %r799, %r800;
	add.s32 	%r802, %r176, %r201;
	mul.wide.u32 	%rd226, %r802, 4;
	add.s64 	%rd227, %rd4, %rd226;
	st.global.u32 	[%rd227], %r161;
	cvt.u64.u32 	%rd228, %r801;
	mul.wide.u32 	%rd229, %r802, 8;
	add.s64 	%rd230, %rd1, %rd229;
	st.global.u64 	[%rd230], %rd228;
	bra.uni 	$L__BB1_108;

$L__BB1_99:
	mov.u32 	%r906, 0;
	mov.u32 	%r907, %r1;
	mov.u32 	%r908, %r889;

$L__BB1_100:
	mov.u32 	%r810, 0;
	mul.wide.u32 	%rd231, %r907, 4;
	add.s64 	%rd232, %rd2, %rd231;
	ld.global.u32 	%r805, [%rd232];
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r804, %r805, %r160;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.u32 %r804, %r804, %r3; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r808, %r804, %r893;
	mul.hi.u32 	%r809, %r804, %r893;
	mul.lo.s32 	%r817, %r808, %r4;
	mul.lo.s32 	%r811, %r817, %r3;
	mul.hi.u32 	%r812, %r817, %r3;
	// begin inline asm
	add.cc.u32 %r808, %r808, %r811;   /* inline */   
	addc.cc.u32 %r809, %r809, %r812;   /* inline */   
	addc.u32 %r810, %r810, %r810;   /* inline */   
	
	// end inline asm
	setp.ne.s32 	%p147, %r810, 0;
	setp.ge.u32 	%p148, %r809, %r3;
	or.pred  	%p149, %p148, %p147;
	selp.b32 	%r818, %r3, 0, %p149;
	sub.s32 	%r819, %r809, %r818;
	cvt.u64.u32 	%rd45, %r819;
	sub.s64 	%rd279, %rd45, %rd36;
	setp.lt.u32 	%p150, %r149, 3;
	mov.u32 	%r914, %r908;
	@%p150 bra 	$L__BB1_103;

	add.s32 	%r912, %r153, %r908;
	add.s32 	%r911, %r155, %r908;
	add.s32 	%r910, %r7, %r908;
	add.s64 	%rd277, %rd37, %rd45;
	add.s64 	%rd276, %rd39, %rd45;
	add.s64 	%rd275, %rd40, %rd45;
	mov.u32 	%r909, %r157;
	mov.u32 	%r914, %r908;

$L__BB1_102:
	mul.wide.u32 	%rd233, %r914, 4;
	add.s64 	%rd234, %rd4, %rd233;
	st.global.u32 	[%rd234], %r161;
	mul.wide.u32 	%rd235, %r914, 8;
	add.s64 	%rd236, %rd1, %rd235;
	st.global.u64 	[%rd236], %rd279;
	mul.wide.u32 	%rd237, %r910, 4;
	add.s64 	%rd238, %rd4, %rd237;
	st.global.u32 	[%rd238], %r161;
	mul.wide.u32 	%rd239, %r910, 8;
	add.s64 	%rd240, %rd1, %rd239;
	st.global.u64 	[%rd240], %rd275;
	add.s32 	%r820, %r914, %r7;
	add.s32 	%r821, %r820, %r7;
	add.s64 	%rd241, %rd279, %rd21;
	add.s64 	%rd242, %rd241, %rd21;
	mul.wide.u32 	%rd243, %r911, 4;
	add.s64 	%rd244, %rd4, %rd243;
	st.global.u32 	[%rd244], %r161;
	mul.wide.u32 	%rd245, %r911, 8;
	add.s64 	%rd246, %rd1, %rd245;
	st.global.u64 	[%rd246], %rd276;
	add.s32 	%r822, %r821, %r7;
	add.s64 	%rd247, %rd242, %rd21;
	mul.wide.u32 	%rd248, %r912, 4;
	add.s64 	%rd249, %rd4, %rd248;
	st.global.u32 	[%rd249], %r161;
	mul.wide.u32 	%rd250, %r912, 8;
	add.s64 	%rd251, %rd1, %rd250;
	st.global.u64 	[%rd251], %rd277;
	add.s32 	%r914, %r822, %r7;
	add.s64 	%rd279, %rd247, %rd21;
	add.s32 	%r912, %r912, %r154;
	add.s32 	%r911, %r911, %r154;
	add.s32 	%r910, %r910, %r154;
	add.s64 	%rd277, %rd277, %rd38;
	add.s64 	%rd276, %rd276, %rd38;
	add.s64 	%rd275, %rd275, %rd38;
	add.s32 	%r909, %r909, 4;
	setp.ne.s32 	%p151, %r909, 0;
	@%p151 bra 	$L__BB1_102;

$L__BB1_103:
	setp.eq.s32 	%p152, %r156, 0;
	@%p152 bra 	$L__BB1_107;

	setp.eq.s32 	%p153, %r156, 1;
	mul.wide.u32 	%rd252, %r914, 4;
	add.s64 	%rd253, %rd4, %rd252;
	st.global.u32 	[%rd253], %r161;
	mul.wide.u32 	%rd254, %r914, 8;
	add.s64 	%rd255, %rd1, %rd254;
	st.global.u64 	[%rd255], %rd279;
	@%p153 bra 	$L__BB1_107;

	add.s32 	%r195, %r914, %r7;
	add.s64 	%rd59, %rd279, %rd21;
	setp.eq.s32 	%p154, %r156, 2;
	mul.wide.u32 	%rd256, %r195, 4;
	add.s64 	%rd257, %rd4, %rd256;
	st.global.u32 	[%rd257], %r161;
	mul.wide.u32 	%rd258, %r195, 8;
	add.s64 	%rd259, %rd1, %rd258;
	st.global.u64 	[%rd259], %rd59;
	@%p154 bra 	$L__BB1_107;

	add.s32 	%r823, %r195, %r7;
	mul.wide.u32 	%rd260, %r823, 4;
	add.s64 	%rd261, %rd4, %rd260;
	st.global.u32 	[%rd261], %r161;
	mul.wide.u32 	%rd262, %r823, 8;
	add.s64 	%rd263, %rd1, %rd262;
	add.s64 	%rd264, %rd59, %rd21;
	st.global.u64 	[%rd263], %rd264;

$L__BB1_107:
	add.s32 	%r908, %r908, %r201;
	add.s32 	%r907, %r907, %r201;
	add.s32 	%r906, %r906, 1;
	setp.lt.u32 	%p155, %r906, %r202;
	@%p155 bra 	$L__BB1_100;

$L__BB1_108:
	ld.param.u32 	%r825, [sieve_kernel_trans_pp32_r64_param_9];
	add.s32 	%r891, %r891, -1;
	sub.s32 	%r889, %r889, %r825;
	setp.ge.s32 	%p156, %r891, %r29;
	@%p156 bra 	$L__BB1_81;

$L__BB1_109:
	ret;

}
	// .globl	sieve_kernel_trans_pp64_r64
.visible .entry sieve_kernel_trans_pp64_r64(
	.param .u64 sieve_kernel_trans_pp64_r64_param_0,
	.param .u32 sieve_kernel_trans_pp64_r64_param_1,
	.param .u64 sieve_kernel_trans_pp64_r64_param_2,
	.param .u32 sieve_kernel_trans_pp64_r64_param_3,
	.param .u64 sieve_kernel_trans_pp64_r64_param_4,
	.param .u64 sieve_kernel_trans_pp64_r64_param_5,
	.param .u64 sieve_kernel_trans_pp64_r64_param_6,
	.param .u32 sieve_kernel_trans_pp64_r64_param_7,
	.param .u32 sieve_kernel_trans_pp64_r64_param_8,
	.param .u32 sieve_kernel_trans_pp64_r64_param_9,
	.param .u32 sieve_kernel_trans_pp64_r64_param_10,
	.param .u32 sieve_kernel_trans_pp64_r64_param_11
)
{
	.reg .pred 	%p<144>;
	.reg .b32 	%r<2320>;
	.reg .b64 	%rd<464>;


	ld.param.u64 	%rd119, [sieve_kernel_trans_pp64_r64_param_0];
	ld.param.u32 	%r172, [sieve_kernel_trans_pp64_r64_param_1];
	ld.param.u64 	%rd120, [sieve_kernel_trans_pp64_r64_param_2];
	ld.param.u32 	%r173, [sieve_kernel_trans_pp64_r64_param_3];
	ld.param.u64 	%rd121, [sieve_kernel_trans_pp64_r64_param_4];
	ld.param.u64 	%rd122, [sieve_kernel_trans_pp64_r64_param_5];
	ld.param.u64 	%rd123, [sieve_kernel_trans_pp64_r64_param_6];
	ld.param.u32 	%r174, [sieve_kernel_trans_pp64_r64_param_7];
	ld.param.u32 	%r175, [sieve_kernel_trans_pp64_r64_param_8];
	ld.param.u32 	%r176, [sieve_kernel_trans_pp64_r64_param_9];
	ld.param.u32 	%r177, [sieve_kernel_trans_pp64_r64_param_10];
	ld.param.u32 	%r178, [sieve_kernel_trans_pp64_r64_param_11];
	cvta.to.global.u64 	%rd1, %rd121;
	cvta.to.global.u64 	%rd2, %rd120;
	cvta.to.global.u64 	%rd3, %rd123;
	cvta.to.global.u64 	%rd4, %rd122;
	mov.u32 	%r179, %ntid.x;
	mov.u32 	%r180, %ctaid.x;
	mov.u32 	%r181, %tid.x;
	mad.lo.s32 	%r1, %r180, %r179, %r181;
	setp.ge.u32 	%p5, %r1, %r172;
	@%p5 bra 	$L__BB2_111;

	cvta.to.global.u64 	%rd126, %rd119;
	mul.wide.u32 	%rd127, %r1, 4;
	add.s64 	%rd128, %rd126, %rd127;
	ld.global.u32 	%r2, [%rd128];
	mov.u64 	%rd425, 0;
	mul.wide.u32 	%rd5, %r2, %r2;
	cvt.u32.u64 	%r3, %rd5;
	add.s32 	%r183, %r3, 2;
	mad.lo.s32 	%r184, %r183, %r3, 2;
	mul.lo.s32 	%r185, %r184, %r183;
	mad.lo.s32 	%r186, %r185, %r3, 2;
	mul.lo.s32 	%r187, %r186, %r185;
	mad.lo.s32 	%r188, %r187, %r3, 2;
	mul.lo.s32 	%r189, %r188, %r187;
	mad.lo.s32 	%r190, %r189, %r3, 2;
	mul.lo.s32 	%r4, %r190, %r189;
	mov.u32 	%r191, %ctaid.y;
	mul.lo.s32 	%r2297, %r191, %r175;
	add.s32 	%r192, %r2297, %r175;
	min.u32 	%r6, %r192, %r174;
	mul.lo.s32 	%r7, %r176, %r174;
	mad.lo.s32 	%r2298, %r2297, %r176, %r1;
	mul.wide.u32 	%rd129, %r2297, 24;
	add.s64 	%rd429, %rd3, %rd129;
	setp.ge.u32 	%p7, %r2297, %r6;
	mov.pred 	%p142, -1;
	mov.u32 	%r2257, 0;
	@%p7 bra 	$L__BB2_11;

	mov.u32 	%r2257, 0;

$L__BB2_3:
	ld.global.u32 	%r12, [%rd429];
	mov.u64 	%rd425, 0;
	setp.eq.s32 	%p8, %r2257, %r12;
	@%p8 bra 	$L__BB2_10;

	mov.u32 	%r2255, %r2;
	mov.u32 	%r2256, %r12;

$L__BB2_5:
	neg.s32 	%r194, %r2256;
	and.b32  	%r195, %r2256, %r194;
	clz.b32 	%r196, %r195;
	mov.u32 	%r197, 31;
	sub.s32 	%r198, %r197, %r196;
	shr.u32 	%r199, %r2256, %r198;
	min.u32 	%r15, %r2255, %r199;
	max.u32 	%r200, %r2255, %r199;
	sub.s32 	%r2256, %r200, %r15;
	setp.ne.s32 	%p9, %r2256, 0;
	mov.u32 	%r2255, %r15;
	@%p9 bra 	$L__BB2_5;

	setp.ne.s32 	%p10, %r15, 1;
	mov.u32 	%r2257, %r12;
	@%p10 bra 	$L__BB2_10;

	ld.global.u64 	%rd9, [%rd429+8];
	or.b64  	%rd133, %rd9, %rd5;
	and.b64  	%rd134, %rd133, -4294967296;
	setp.eq.s64 	%p11, %rd134, 0;
	@%p11 bra 	$L__BB2_9;

	rem.u64 	%rd425, %rd9, %rd5;
	mov.u64 	%rd426, %rd425;
	mov.u32 	%r2257, %r12;
	bra.uni 	$L__BB2_10;

$L__BB2_9:
	cvt.u32.u64 	%r202, %rd9;
	rem.u32 	%r203, %r202, %r3;
	cvt.u64.u32 	%rd425, %r203;
	mov.u64 	%rd426, %rd425;
	mov.u32 	%r2257, %r12;

$L__BB2_10:
	mul.wide.u32 	%rd135, %r2298, 8;
	add.s64 	%rd136, %rd4, %rd135;
	st.global.u64 	[%rd136], %rd425;
	add.s32 	%r2298, %r2298, %r176;
	add.s64 	%rd429, %rd429, 24;
	add.s32 	%r2297, %r2297, 1;
	setp.lt.u32 	%p12, %r2297, %r6;
	setp.eq.s64 	%p142, %rd425, 0;
	and.pred  	%p13, %p142, %p12;
	@%p13 bra 	$L__BB2_3;

$L__BB2_11:
	@%p142 bra 	$L__BB2_111;

	add.s32 	%r23, %r2297, -1;
	setp.ge.u32 	%p14, %r2297, %r6;
	@%p14 bra 	$L__BB2_23;

	shr.u64 	%rd137, %rd5, 32;
	cvt.u32.u64 	%r24, %rd137;

$L__BB2_14:
	ld.global.u32 	%r28, [%rd429];
	setp.eq.s32 	%p15, %r2257, %r28;
	@%p15 bra 	$L__BB2_22;

	mov.u32 	%r2264, %r2;
	mov.u32 	%r2265, %r28;

$L__BB2_16:
	neg.s32 	%r204, %r2265;
	and.b32  	%r205, %r2265, %r204;
	clz.b32 	%r206, %r205;
	mov.u32 	%r207, 31;
	sub.s32 	%r208, %r207, %r206;
	shr.u32 	%r209, %r2265, %r208;
	min.u32 	%r31, %r2264, %r209;
	max.u32 	%r210, %r2264, %r209;
	sub.s32 	%r2265, %r210, %r31;
	setp.ne.s32 	%p16, %r2265, 0;
	mov.u32 	%r2264, %r31;
	@%p16 bra 	$L__BB2_16;

	setp.ne.s32 	%p17, %r31, 1;
	mov.u64 	%rd425, 0;
	mov.u32 	%r2257, %r28;
	@%p17 bra 	$L__BB2_22;

	ld.global.u64 	%rd21, [%rd429+8];
	or.b64  	%rd139, %rd21, %rd5;
	and.b64  	%rd140, %rd139, -4294967296;
	setp.eq.s64 	%p18, %rd140, 0;
	@%p18 bra 	$L__BB2_20;

	rem.u64 	%rd433, %rd21, %rd5;
	bra.uni 	$L__BB2_21;

$L__BB2_20:
	cvt.u32.u64 	%r212, %rd21;
	rem.u32 	%r213, %r212, %r3;
	cvt.u64.u32 	%rd433, %r213;

$L__BB2_21:
	shr.u64 	%rd141, %rd426, 32;
	cvt.u32.u64 	%r275, %rd141;
	shr.u64 	%rd142, %rd433, 32;
	cvt.u32.u64 	%r276, %rd142;
	cvt.u32.u64 	%r277, %rd433;
	cvt.u32.u64 	%r278, %rd426;
	mul.lo.s32 	%r214, %r277, %r278;
	mul.hi.u32 	%r224, %r278, %r277;
	mul.lo.s32 	%r279, %r214, %r4;
	mul.lo.s32 	%r217, %r279, %r3;
	mul.hi.u32 	%r218, %r279, %r3;
	mov.u32 	%r271, 0;
	mov.u32 	%r225, %r271;
	// begin inline asm
	add.cc.u32 %r214, %r214, %r217;   /* inline */   
	addc.cc.u32 %r224, %r224, %r218;   /* inline */   
	addc.u32 %r225, %r225, %r225;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r226, %r276, %r278;
	mul.hi.u32 	%r227, %r278, %r276;
	// begin inline asm
	add.cc.u32 %r240, %r224, %r226;   /* inline */   
	addc.cc.u32 %r224, %r225, %r227;   /* inline */   
	addc.u32 %r225, %r271, %r271;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r234, %r277, %r275;
	mul.hi.u32 	%r235, %r275, %r277;
	// begin inline asm
	add.cc.u32 %r240, %r240, %r234;   /* inline */   
	addc.cc.u32 %r224, %r224, %r235;   /* inline */   
	addc.u32 %r225, %r225, %r271;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r243, %r279, %r24;
	mul.hi.u32 	%r244, %r279, %r24;
	// begin inline asm
	add.cc.u32 %r240, %r240, %r243;   /* inline */   
	addc.cc.u32 %r224, %r224, %r244;   /* inline */   
	addc.u32 %r225, %r225, %r271;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r280, %r240, %r4;
	mul.lo.s32 	%r252, %r280, %r3;
	mul.hi.u32 	%r253, %r280, %r3;
	// begin inline asm
	add.cc.u32 %r240, %r240, %r252;   /* inline */   
	addc.cc.u32 %r224, %r224, %r253;   /* inline */   
	addc.u32 %r225, %r225, %r271;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r261, %r276, %r275;
	mul.hi.u32 	%r262, %r275, %r276;
	// begin inline asm
	add.cc.u32 %r266, %r224, %r261;   /* inline */   
	addc.cc.u32 %r224, %r225, %r262;   /* inline */   
	addc.u32 %r225, %r271, %r271;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r269, %r280, %r24;
	mul.hi.u32 	%r270, %r280, %r24;
	// begin inline asm
	add.cc.u32 %r266, %r266, %r269;   /* inline */   
	addc.cc.u32 %r224, %r224, %r270;   /* inline */   
	addc.u32 %r225, %r225, %r271;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd143, %r224;
	cvt.u64.u32 	%rd144, %r266;
	bfi.b64 	%rd145, %rd143, %rd144, 32, 32;
	setp.ne.s32 	%p19, %r225, 0;
	setp.ge.u64 	%p20, %rd145, %rd5;
	or.pred  	%p21, %p19, %p20;
	selp.b64 	%rd146, %rd5, 0, %p21;
	sub.s64 	%rd425, %rd145, %rd146;
	mov.u64 	%rd426, %rd425;
	mov.u32 	%r2257, %r28;

$L__BB2_22:
	mul.wide.u32 	%rd147, %r2298, 8;
	add.s64 	%rd148, %rd4, %rd147;
	st.global.u64 	[%rd148], %rd425;
	add.s32 	%r2298, %r2298, %r176;
	add.s64 	%rd429, %rd429, 24;
	add.s32 	%r2297, %r2297, 1;
	setp.lt.u32 	%p22, %r2297, %r6;
	@%p22 bra 	$L__BB2_14;

$L__BB2_23:
	setp.lt.u64 	%p24, %rd426, 2;
	mov.pred 	%p143, -1;
	mov.u64 	%rd444, 1;
	@%p24 bra 	$L__BB2_40;

	mov.u64 	%rd444, 1;
	mov.u64 	%rd439, 0;
	mov.u32 	%r2269, 0;
	mov.u64 	%rd443, %rd426;
	mov.u64 	%rd440, %rd5;

$L__BB2_25:
	mov.u64 	%rd31, %rd440;
	mov.u64 	%rd30, %rd439;
	mov.u64 	%rd439, %rd444;
	mov.u64 	%rd440, %rd443;
	mov.u32 	%r38, %r2269;
	sub.s64 	%rd443, %rd31, %rd440;
	sub.s64 	%rd35, %rd443, %rd440;
	setp.lt.u64 	%p25, %rd443, %rd440;
	mov.u64 	%rd442, %rd439;
	@%p25 bra 	$L__BB2_38;

	shl.b64 	%rd442, %rd439, 1;
	sub.s64 	%rd37, %rd35, %rd440;
	setp.lt.u64 	%p26, %rd35, %rd440;
	mov.u64 	%rd443, %rd35;
	@%p26 bra 	$L__BB2_38;

	mul.lo.s64 	%rd442, %rd439, 3;
	sub.s64 	%rd39, %rd37, %rd440;
	setp.lt.u64 	%p27, %rd37, %rd440;
	mov.u64 	%rd443, %rd37;
	@%p27 bra 	$L__BB2_38;

	shl.b64 	%rd442, %rd439, 2;
	sub.s64 	%rd41, %rd39, %rd440;
	setp.lt.u64 	%p28, %rd39, %rd440;
	mov.u64 	%rd443, %rd39;
	@%p28 bra 	$L__BB2_38;

	mul.lo.s64 	%rd442, %rd439, 5;
	sub.s64 	%rd43, %rd41, %rd440;
	setp.lt.u64 	%p29, %rd41, %rd440;
	mov.u64 	%rd443, %rd41;
	@%p29 bra 	$L__BB2_38;

	mul.lo.s64 	%rd442, %rd439, 6;
	sub.s64 	%rd45, %rd43, %rd440;
	setp.lt.u64 	%p30, %rd43, %rd440;
	mov.u64 	%rd443, %rd43;
	@%p30 bra 	$L__BB2_38;

	mul.lo.s64 	%rd442, %rd439, 7;
	sub.s64 	%rd47, %rd45, %rd440;
	setp.lt.u64 	%p31, %rd45, %rd440;
	mov.u64 	%rd443, %rd45;
	@%p31 bra 	$L__BB2_38;

	shl.b64 	%rd442, %rd439, 3;
	sub.s64 	%rd49, %rd47, %rd440;
	setp.lt.u64 	%p32, %rd47, %rd440;
	mov.u64 	%rd443, %rd47;
	@%p32 bra 	$L__BB2_38;

	mul.lo.s64 	%rd442, %rd439, 9;
	setp.lt.u64 	%p33, %rd49, %rd440;
	mov.u64 	%rd443, %rd49;
	@%p33 bra 	$L__BB2_38;

	or.b64  	%rd152, %rd31, %rd440;
	and.b64  	%rd153, %rd152, -4294967296;
	setp.eq.s64 	%p34, %rd153, 0;
	@%p34 bra 	$L__BB2_36;

	div.u64 	%rd441, %rd31, %rd440;
	bra.uni 	$L__BB2_37;

$L__BB2_36:
	cvt.u32.u64 	%r282, %rd440;
	cvt.u32.u64 	%r283, %rd31;
	div.u32 	%r284, %r283, %r282;
	cvt.u64.u32 	%rd441, %r284;

$L__BB2_37:
	mul.lo.s64 	%rd154, %rd441, %rd440;
	sub.s64 	%rd443, %rd31, %rd154;
	mul.lo.s64 	%rd442, %rd441, %rd439;

$L__BB2_38:
	add.s64 	%rd444, %rd442, %rd30;
	not.b32 	%r2269, %r38;
	setp.gt.u64 	%p35, %rd443, 1;
	@%p35 bra 	$L__BB2_25;

	setp.eq.s32 	%p143, %r38, -1;

$L__BB2_40:
	sub.s64 	%rd155, %rd5, %rd444;
	selp.b64 	%rd60, %rd444, %rd155, %p143;
	clz.b64 	%r40, %rd5;
	cvt.u64.u32 	%rd61, %r40;
	shl.b64 	%rd62, %rd5, %r40;
	neg.s64 	%rd446, %rd62;
	mov.u32 	%r285, 64;
	sub.s32 	%r2272, %r285, %r40;
	setp.gt.u32 	%p36, %r2272, 71;
	@%p36 bra 	$L__BB2_44;

	and.b32  	%r2271, %r40, 3;
	setp.eq.s32 	%p37, %r2271, 0;
	@%p37 bra 	$L__BB2_43;

$L__BB2_42:
	.pragma "nounroll";
	shr.s64 	%rd156, %rd446, 63;
	and.b64  	%rd157, %rd156, %rd62;
	shl.b64 	%rd158, %rd446, 1;
	sub.s64 	%rd159, %rd158, %rd157;
	setp.lt.u64 	%p38, %rd159, %rd62;
	selp.b64 	%rd160, 0, %rd62, %p38;
	sub.s64 	%rd446, %rd159, %rd160;
	add.s32 	%r2272, %r2272, 1;
	add.s32 	%r2271, %r2271, -1;
	setp.ne.s32 	%p39, %r2271, 0;
	@%p39 bra 	$L__BB2_42;

$L__BB2_43:
	shr.s64 	%rd161, %rd446, 63;
	and.b64  	%rd162, %rd161, %rd62;
	shl.b64 	%rd163, %rd446, 1;
	sub.s64 	%rd164, %rd163, %rd162;
	setp.lt.u64 	%p40, %rd164, %rd62;
	selp.b64 	%rd165, 0, %rd62, %p40;
	sub.s64 	%rd166, %rd164, %rd165;
	shl.b64 	%rd167, %rd166, 1;
	shr.s64 	%rd168, %rd166, 63;
	and.b64  	%rd169, %rd168, %rd62;
	sub.s64 	%rd170, %rd167, %rd169;
	setp.lt.u64 	%p41, %rd170, %rd62;
	selp.b64 	%rd171, 0, %rd62, %p41;
	sub.s64 	%rd172, %rd170, %rd171;
	shl.b64 	%rd173, %rd172, 1;
	shr.s64 	%rd174, %rd172, 63;
	and.b64  	%rd175, %rd174, %rd62;
	sub.s64 	%rd176, %rd173, %rd175;
	setp.lt.u64 	%p42, %rd176, %rd62;
	selp.b64 	%rd177, 0, %rd62, %p42;
	sub.s64 	%rd178, %rd176, %rd177;
	shl.b64 	%rd179, %rd178, 1;
	shr.s64 	%rd180, %rd178, 63;
	and.b64  	%rd181, %rd180, %rd62;
	sub.s64 	%rd182, %rd179, %rd181;
	setp.lt.u64 	%p43, %rd182, %rd62;
	selp.b64 	%rd183, 0, %rd62, %p43;
	sub.s64 	%rd446, %rd182, %rd183;
	add.s32 	%r2272, %r2272, 4;
	setp.lt.u32 	%p44, %r2272, 72;
	@%p44 bra 	$L__BB2_43;

$L__BB2_44:
	cvt.u32.u64 	%r530, %rd61;
	shr.u64 	%rd184, %rd446, %r530;
	cvt.u32.u64 	%r531, %rd184;
	shr.u64 	%rd185, %rd184, 32;
	cvt.u32.u64 	%r532, %rd185;
	shr.u64 	%rd186, %rd5, 32;
	cvt.u32.u64 	%r50, %rd186;
	mul.lo.s32 	%r286, %r531, %r531;
	mul.hi.u32 	%r296, %r531, %r531;
	mul.lo.s32 	%r533, %r286, %r4;
	mul.lo.s32 	%r289, %r533, %r3;
	mul.hi.u32 	%r290, %r533, %r3;
	mov.u32 	%r526, 0;
	mov.u32 	%r297, %r526;
	// begin inline asm
	add.cc.u32 %r286, %r286, %r289;   /* inline */   
	addc.cc.u32 %r296, %r296, %r290;   /* inline */   
	addc.u32 %r297, %r297, %r297;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r306, %r532, %r531;
	mul.hi.u32 	%r299, %r531, %r532;
	// begin inline asm
	add.cc.u32 %r312, %r296, %r306;   /* inline */   
	addc.cc.u32 %r296, %r297, %r299;   /* inline */   
	addc.u32 %r297, %r526, %r526;   /* inline */   
	
	// end inline asm
	mul.hi.u32 	%r307, %r532, %r531;
	// begin inline asm
	add.cc.u32 %r312, %r312, %r306;   /* inline */   
	addc.cc.u32 %r296, %r296, %r307;   /* inline */   
	addc.u32 %r297, %r297, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r315, %r533, %r50;
	mul.hi.u32 	%r316, %r533, %r50;
	// begin inline asm
	add.cc.u32 %r312, %r312, %r315;   /* inline */   
	addc.cc.u32 %r296, %r296, %r316;   /* inline */   
	addc.u32 %r297, %r297, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r534, %r312, %r4;
	mul.lo.s32 	%r324, %r534, %r3;
	mul.hi.u32 	%r325, %r534, %r3;
	// begin inline asm
	add.cc.u32 %r312, %r312, %r324;   /* inline */   
	addc.cc.u32 %r296, %r296, %r325;   /* inline */   
	addc.u32 %r297, %r297, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r333, %r532, %r532;
	mul.hi.u32 	%r334, %r532, %r532;
	// begin inline asm
	add.cc.u32 %r338, %r296, %r333;   /* inline */   
	addc.cc.u32 %r296, %r297, %r334;   /* inline */   
	addc.u32 %r297, %r526, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r341, %r534, %r50;
	mul.hi.u32 	%r342, %r534, %r50;
	// begin inline asm
	add.cc.u32 %r338, %r338, %r341;   /* inline */   
	addc.cc.u32 %r296, %r296, %r342;   /* inline */   
	addc.u32 %r297, %r297, %r526;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd187, %r296;
	cvt.u64.u32 	%rd188, %r338;
	bfi.b64 	%rd189, %rd187, %rd188, 32, 32;
	setp.ne.s32 	%p45, %r297, 0;
	setp.ge.u64 	%p46, %rd189, %rd5;
	or.pred  	%p47, %p45, %p46;
	selp.b64 	%rd190, %rd5, 0, %p47;
	sub.s64 	%rd191, %rd189, %rd190;
	cvt.u32.u64 	%r535, %rd191;
	shr.u64 	%rd192, %rd191, 32;
	cvt.u32.u64 	%r536, %rd192;
	mul.lo.s32 	%r347, %r535, %r535;
	mul.hi.u32 	%r357, %r535, %r535;
	mul.lo.s32 	%r537, %r347, %r4;
	mul.lo.s32 	%r350, %r537, %r3;
	mul.hi.u32 	%r351, %r537, %r3;
	mov.u32 	%r358, %r526;
	// begin inline asm
	add.cc.u32 %r347, %r347, %r350;   /* inline */   
	addc.cc.u32 %r357, %r357, %r351;   /* inline */   
	addc.u32 %r358, %r358, %r358;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r367, %r536, %r535;
	mul.hi.u32 	%r360, %r535, %r536;
	// begin inline asm
	add.cc.u32 %r373, %r357, %r367;   /* inline */   
	addc.cc.u32 %r357, %r358, %r360;   /* inline */   
	addc.u32 %r358, %r526, %r526;   /* inline */   
	
	// end inline asm
	mul.hi.u32 	%r368, %r536, %r535;
	// begin inline asm
	add.cc.u32 %r373, %r373, %r367;   /* inline */   
	addc.cc.u32 %r357, %r357, %r368;   /* inline */   
	addc.u32 %r358, %r358, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r376, %r537, %r50;
	mul.hi.u32 	%r377, %r537, %r50;
	// begin inline asm
	add.cc.u32 %r373, %r373, %r376;   /* inline */   
	addc.cc.u32 %r357, %r357, %r377;   /* inline */   
	addc.u32 %r358, %r358, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r538, %r373, %r4;
	mul.lo.s32 	%r385, %r538, %r3;
	mul.hi.u32 	%r386, %r538, %r3;
	// begin inline asm
	add.cc.u32 %r373, %r373, %r385;   /* inline */   
	addc.cc.u32 %r357, %r357, %r386;   /* inline */   
	addc.u32 %r358, %r358, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r394, %r536, %r536;
	mul.hi.u32 	%r395, %r536, %r536;
	// begin inline asm
	add.cc.u32 %r399, %r357, %r394;   /* inline */   
	addc.cc.u32 %r357, %r358, %r395;   /* inline */   
	addc.u32 %r358, %r526, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r402, %r538, %r50;
	mul.hi.u32 	%r403, %r538, %r50;
	// begin inline asm
	add.cc.u32 %r399, %r399, %r402;   /* inline */   
	addc.cc.u32 %r357, %r357, %r403;   /* inline */   
	addc.u32 %r358, %r358, %r526;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd193, %r357;
	cvt.u64.u32 	%rd194, %r399;
	bfi.b64 	%rd195, %rd193, %rd194, 32, 32;
	setp.ne.s32 	%p48, %r358, 0;
	setp.ge.u64 	%p49, %rd195, %rd5;
	or.pred  	%p50, %p48, %p49;
	selp.b64 	%rd196, %rd5, 0, %p50;
	sub.s64 	%rd197, %rd195, %rd196;
	cvt.u32.u64 	%r539, %rd197;
	shr.u64 	%rd198, %rd197, 32;
	cvt.u32.u64 	%r540, %rd198;
	mul.lo.s32 	%r408, %r539, %r539;
	mul.hi.u32 	%r418, %r539, %r539;
	mul.lo.s32 	%r541, %r408, %r4;
	mul.lo.s32 	%r411, %r541, %r3;
	mul.hi.u32 	%r412, %r541, %r3;
	mov.u32 	%r419, %r526;
	// begin inline asm
	add.cc.u32 %r408, %r408, %r411;   /* inline */   
	addc.cc.u32 %r418, %r418, %r412;   /* inline */   
	addc.u32 %r419, %r419, %r419;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r428, %r540, %r539;
	mul.hi.u32 	%r421, %r539, %r540;
	// begin inline asm
	add.cc.u32 %r434, %r418, %r428;   /* inline */   
	addc.cc.u32 %r418, %r419, %r421;   /* inline */   
	addc.u32 %r419, %r526, %r526;   /* inline */   
	
	// end inline asm
	mul.hi.u32 	%r429, %r540, %r539;
	// begin inline asm
	add.cc.u32 %r434, %r434, %r428;   /* inline */   
	addc.cc.u32 %r418, %r418, %r429;   /* inline */   
	addc.u32 %r419, %r419, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r437, %r541, %r50;
	mul.hi.u32 	%r438, %r541, %r50;
	// begin inline asm
	add.cc.u32 %r434, %r434, %r437;   /* inline */   
	addc.cc.u32 %r418, %r418, %r438;   /* inline */   
	addc.u32 %r419, %r419, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r542, %r434, %r4;
	mul.lo.s32 	%r446, %r542, %r3;
	mul.hi.u32 	%r447, %r542, %r3;
	// begin inline asm
	add.cc.u32 %r434, %r434, %r446;   /* inline */   
	addc.cc.u32 %r418, %r418, %r447;   /* inline */   
	addc.u32 %r419, %r419, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r455, %r540, %r540;
	mul.hi.u32 	%r456, %r540, %r540;
	// begin inline asm
	add.cc.u32 %r460, %r418, %r455;   /* inline */   
	addc.cc.u32 %r418, %r419, %r456;   /* inline */   
	addc.u32 %r419, %r526, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r463, %r542, %r50;
	mul.hi.u32 	%r464, %r542, %r50;
	// begin inline asm
	add.cc.u32 %r460, %r460, %r463;   /* inline */   
	addc.cc.u32 %r418, %r418, %r464;   /* inline */   
	addc.u32 %r419, %r419, %r526;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd199, %r418;
	cvt.u64.u32 	%rd200, %r460;
	bfi.b64 	%rd201, %rd199, %rd200, 32, 32;
	setp.ne.s32 	%p51, %r419, 0;
	setp.ge.u64 	%p52, %rd201, %rd5;
	or.pred  	%p53, %p51, %p52;
	selp.b64 	%rd202, %rd5, 0, %p53;
	sub.s64 	%rd203, %rd201, %rd202;
	shr.u64 	%rd204, %rd60, 32;
	cvt.u32.u64 	%r543, %rd204;
	cvt.u32.u64 	%r544, %rd203;
	shr.u64 	%rd205, %rd203, 32;
	cvt.u32.u64 	%r545, %rd205;
	cvt.u32.u64 	%r546, %rd60;
	mul.lo.s32 	%r469, %r544, %r546;
	mul.hi.u32 	%r479, %r546, %r544;
	mul.lo.s32 	%r547, %r469, %r4;
	mul.lo.s32 	%r472, %r547, %r3;
	mul.hi.u32 	%r473, %r547, %r3;
	mov.u32 	%r480, %r526;
	// begin inline asm
	add.cc.u32 %r469, %r469, %r472;   /* inline */   
	addc.cc.u32 %r479, %r479, %r473;   /* inline */   
	addc.u32 %r480, %r480, %r480;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r481, %r545, %r546;
	mul.hi.u32 	%r482, %r546, %r545;
	// begin inline asm
	add.cc.u32 %r495, %r479, %r481;   /* inline */   
	addc.cc.u32 %r479, %r480, %r482;   /* inline */   
	addc.u32 %r480, %r526, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r489, %r544, %r543;
	mul.hi.u32 	%r490, %r543, %r544;
	// begin inline asm
	add.cc.u32 %r495, %r495, %r489;   /* inline */   
	addc.cc.u32 %r479, %r479, %r490;   /* inline */   
	addc.u32 %r480, %r480, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r498, %r547, %r50;
	mul.hi.u32 	%r499, %r547, %r50;
	// begin inline asm
	add.cc.u32 %r495, %r495, %r498;   /* inline */   
	addc.cc.u32 %r479, %r479, %r499;   /* inline */   
	addc.u32 %r480, %r480, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r548, %r495, %r4;
	mul.lo.s32 	%r507, %r548, %r3;
	mul.hi.u32 	%r508, %r548, %r3;
	// begin inline asm
	add.cc.u32 %r495, %r495, %r507;   /* inline */   
	addc.cc.u32 %r479, %r479, %r508;   /* inline */   
	addc.u32 %r480, %r480, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r516, %r545, %r543;
	mul.hi.u32 	%r517, %r543, %r545;
	// begin inline asm
	add.cc.u32 %r521, %r479, %r516;   /* inline */   
	addc.cc.u32 %r479, %r480, %r517;   /* inline */   
	addc.u32 %r480, %r526, %r526;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r524, %r548, %r50;
	mul.hi.u32 	%r525, %r548, %r50;
	// begin inline asm
	add.cc.u32 %r521, %r521, %r524;   /* inline */   
	addc.cc.u32 %r479, %r479, %r525;   /* inline */   
	addc.u32 %r480, %r480, %r526;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd206, %r479;
	cvt.u64.u32 	%rd207, %r521;
	bfi.b64 	%rd208, %rd206, %rd207, 32, 32;
	setp.ne.s32 	%p54, %r480, 0;
	setp.ge.u64 	%p55, %rd208, %rd5;
	or.pred  	%p56, %p54, %p55;
	selp.b64 	%rd209, %rd5, 0, %p56;
	sub.s64 	%rd456, %rd208, %rd209;

$L__BB2_45:
	mov.u32 	%r52, %r2297;
	sub.s32 	%r2298, %r2298, %r176;
	add.s32 	%r2297, %r52, -1;
	setp.le.u32 	%p57, %r2297, %r23;
	@%p57 bra 	$L__BB2_47;

	mul.wide.u32 	%rd210, %r2298, 8;
	add.s64 	%rd211, %rd4, %rd210;
	ld.global.u32 	%r549, [%rd211];
	setp.eq.s32 	%p58, %r549, 0;
	@%p58 bra 	$L__BB2_45;

$L__BB2_47:
	add.s32 	%r2277, %r52, -2;
	setp.lt.s32 	%p59, %r2277, %r23;
	@%p59 bra 	$L__BB2_83;

	shr.u64 	%rd71, %rd5, 1;
	mov.u32 	%r2299, 1;
	shr.u32 	%r551, %r178, 1;
	cvt.u64.u32 	%rd212, %r551;
	mul.lo.s64 	%rd72, %rd5, %rd212;
	add.s32 	%r56, %r178, -1;
	and.b32  	%r57, %r178, 3;
	sub.s32 	%r58, %r178, %r57;
	and.b32  	%r59, %r173, 3;
	sub.s32 	%r60, %r173, %r59;
	mov.u32 	%r2276, %r2298;

$L__BB2_49:
	sub.s32 	%r2276, %r2276, %r176;
	mul.wide.u32 	%rd213, %r2276, 8;
	add.s64 	%rd214, %rd4, %rd213;
	ld.global.u64 	%rd75, [%rd214];
	setp.eq.s64 	%p60, %rd75, 0;
	@%p60 bra 	$L__BB2_82;

	setp.eq.s64 	%p61, %rd75, %rd426;
	@%p61 bra 	$L__BB2_81;
	bra.uni 	$L__BB2_51;

$L__BB2_81:
	add.s32 	%r2299, %r2299, 1;
	bra.uni 	$L__BB2_82;

$L__BB2_51:
	cvt.u32.u64 	%r613, %rd75;
	shr.u64 	%rd215, %rd75, 32;
	cvt.u32.u64 	%r614, %rd215;
	shr.u64 	%rd216, %rd456, 32;
	cvt.u32.u64 	%r67, %rd216;
	cvt.u32.u64 	%r68, %rd456;
	mul.lo.s32 	%r552, %r613, %r68;
	mul.hi.u32 	%r562, %r613, %r68;
	mul.lo.s32 	%r615, %r552, %r4;
	mul.lo.s32 	%r555, %r615, %r3;
	mul.hi.u32 	%r556, %r615, %r3;
	mov.u32 	%r609, 0;
	mov.u32 	%r563, %r609;
	// begin inline asm
	add.cc.u32 %r552, %r552, %r555;   /* inline */   
	addc.cc.u32 %r562, %r562, %r556;   /* inline */   
	addc.u32 %r563, %r563, %r563;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r564, %r613, %r67;
	mul.hi.u32 	%r565, %r613, %r67;
	// begin inline asm
	add.cc.u32 %r578, %r562, %r564;   /* inline */   
	addc.cc.u32 %r562, %r563, %r565;   /* inline */   
	addc.u32 %r563, %r609, %r609;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r572, %r614, %r68;
	mul.hi.u32 	%r573, %r614, %r68;
	// begin inline asm
	add.cc.u32 %r578, %r578, %r572;   /* inline */   
	addc.cc.u32 %r562, %r562, %r573;   /* inline */   
	addc.u32 %r563, %r563, %r609;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r581, %r615, %r50;
	mul.hi.u32 	%r582, %r615, %r50;
	// begin inline asm
	add.cc.u32 %r578, %r578, %r581;   /* inline */   
	addc.cc.u32 %r562, %r562, %r582;   /* inline */   
	addc.u32 %r563, %r563, %r609;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r616, %r578, %r4;
	mul.lo.s32 	%r590, %r616, %r3;
	mul.hi.u32 	%r591, %r616, %r3;
	// begin inline asm
	add.cc.u32 %r578, %r578, %r590;   /* inline */   
	addc.cc.u32 %r562, %r562, %r591;   /* inline */   
	addc.u32 %r563, %r563, %r609;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r599, %r614, %r67;
	mul.hi.u32 	%r600, %r614, %r67;
	// begin inline asm
	add.cc.u32 %r604, %r562, %r599;   /* inline */   
	addc.cc.u32 %r562, %r563, %r600;   /* inline */   
	addc.u32 %r563, %r609, %r609;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r607, %r616, %r50;
	mul.hi.u32 	%r608, %r616, %r50;
	// begin inline asm
	add.cc.u32 %r604, %r604, %r607;   /* inline */   
	addc.cc.u32 %r562, %r562, %r608;   /* inline */   
	addc.u32 %r563, %r563, %r609;   /* inline */   
	
	// end inline asm
	mul.wide.u32 	%rd217, %r2297, 24;
	add.s64 	%rd218, %rd3, %rd217;
	ld.global.u64 	%rd76, [%rd218+8];
	or.b64  	%rd219, %rd76, %rd5;
	and.b64  	%rd220, %rd219, -4294967296;
	setp.eq.s64 	%p62, %rd220, 0;
	@%p62 bra 	$L__BB2_53;

	rem.u64 	%rd451, %rd76, %rd5;
	bra.uni 	$L__BB2_54;

$L__BB2_53:
	cvt.u32.u64 	%r618, %rd76;
	rem.u32 	%r619, %r618, %r3;
	cvt.u64.u32 	%rd451, %r619;

$L__BB2_54:
	cvt.u64.u32 	%rd221, %r604;
	cvt.u64.u32 	%rd222, %r562;
	bfi.b64 	%rd223, %rd222, %rd221, 32, 32;
	setp.ge.u64 	%p63, %rd223, %rd5;
	setp.ne.s32 	%p64, %r563, 0;
	mov.u32 	%r677, 0;
	or.pred  	%p65, %p64, %p63;
	selp.b64 	%rd224, %rd5, 0, %p65;
	sub.s64 	%rd225, %rd223, %rd224;
	shr.u64 	%rd226, %rd451, 32;
	cvt.u32.u64 	%r681, %rd226;
	cvt.u32.u64 	%r682, %rd451;
	mul.lo.s32 	%r620, %r682, %r68;
	mul.hi.u32 	%r630, %r68, %r682;
	mul.lo.s32 	%r683, %r620, %r4;
	mul.lo.s32 	%r623, %r683, %r3;
	mul.hi.u32 	%r624, %r683, %r3;
	mov.u32 	%r631, %r677;
	// begin inline asm
	add.cc.u32 %r620, %r620, %r623;   /* inline */   
	addc.cc.u32 %r630, %r630, %r624;   /* inline */   
	addc.u32 %r631, %r631, %r631;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r632, %r681, %r68;
	mul.hi.u32 	%r633, %r68, %r681;
	// begin inline asm
	add.cc.u32 %r646, %r630, %r632;   /* inline */   
	addc.cc.u32 %r630, %r631, %r633;   /* inline */   
	addc.u32 %r631, %r677, %r677;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r640, %r682, %r67;
	mul.hi.u32 	%r641, %r67, %r682;
	// begin inline asm
	add.cc.u32 %r646, %r646, %r640;   /* inline */   
	addc.cc.u32 %r630, %r630, %r641;   /* inline */   
	addc.u32 %r631, %r631, %r677;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r649, %r683, %r50;
	mul.hi.u32 	%r650, %r683, %r50;
	// begin inline asm
	add.cc.u32 %r646, %r646, %r649;   /* inline */   
	addc.cc.u32 %r630, %r630, %r650;   /* inline */   
	addc.u32 %r631, %r631, %r677;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r684, %r646, %r4;
	mul.lo.s32 	%r658, %r684, %r3;
	mul.hi.u32 	%r659, %r684, %r3;
	// begin inline asm
	add.cc.u32 %r646, %r646, %r658;   /* inline */   
	addc.cc.u32 %r630, %r630, %r659;   /* inline */   
	addc.u32 %r631, %r631, %r677;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r667, %r681, %r67;
	mul.hi.u32 	%r668, %r67, %r681;
	// begin inline asm
	add.cc.u32 %r672, %r630, %r667;   /* inline */   
	addc.cc.u32 %r630, %r631, %r668;   /* inline */   
	addc.u32 %r631, %r677, %r677;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r675, %r684, %r50;
	mul.hi.u32 	%r676, %r684, %r50;
	// begin inline asm
	add.cc.u32 %r672, %r672, %r675;   /* inline */   
	addc.cc.u32 %r630, %r630, %r676;   /* inline */   
	addc.u32 %r631, %r631, %r677;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd227, %r630;
	cvt.u64.u32 	%rd228, %r672;
	bfi.b64 	%rd229, %rd227, %rd228, 32, 32;
	setp.ne.s32 	%p66, %r631, 0;
	setp.ge.u64 	%p67, %rd229, %rd5;
	or.pred  	%p68, %p66, %p67;
	selp.b64 	%rd230, %rd5, 0, %p68;
	sub.s64 	%rd456, %rd229, %rd230;
	cvt.u32.u64 	%r72, %rd225;
	shr.u64 	%rd231, %rd225, 32;
	cvt.u32.u64 	%r73, %rd231;
	mov.u32 	%r2281, %r2297;
	mov.u32 	%r2282, %r2298;
	mov.u32 	%r2283, %r2299;

$L__BB2_55:
	mul.wide.u32 	%rd232, %r2281, 24;
	add.s64 	%rd233, %rd3, %rd232;
	ld.global.u64 	%rd81, [%rd233+16];
	or.b64  	%rd234, %rd81, %rd5;
	and.b64  	%rd235, %rd234, -4294967296;
	setp.eq.s64 	%p69, %rd235, 0;
	@%p69 bra 	$L__BB2_57;

	rem.u64 	%rd452, %rd81, %rd5;
	bra.uni 	$L__BB2_58;

$L__BB2_57:
	cvt.u32.u64 	%r686, %rd81;
	rem.u32 	%r687, %r686, %r3;
	cvt.u64.u32 	%rd452, %r687;

$L__BB2_58:
	setp.eq.s32 	%p70, %r173, 0;
	@%p70 bra 	$L__BB2_80;

	cvt.u32.u64 	%r77, %rd452;
	shr.u64 	%rd236, %rd452, 32;
	cvt.u32.u64 	%r78, %rd236;
	shl.b32 	%r688, %r2281, %r177;
	or.b32  	%r79, %r688, %r2;
	setp.eq.s32 	%p71, %r178, 1;
	@%p71 bra 	$L__BB2_66;

	setp.ne.s32 	%p72, %r178, 0;
	@%p72 bra 	$L__BB2_71;

	add.s32 	%r689, %r173, -1;
	setp.lt.u32 	%p73, %r689, 3;
	@%p73 bra 	$L__BB2_64;

	mov.u32 	%r2284, %r1;
	mov.u32 	%r2285, %r60;

$L__BB2_63:
	add.s32 	%r966, %r2284, %r172;
	add.s32 	%r967, %r966, %r172;
	add.s32 	%r968, %r967, %r172;
	add.s32 	%r2284, %r968, %r172;
	add.s32 	%r2285, %r2285, -4;
	setp.ne.s32 	%p74, %r2285, 0;
	@%p74 bra 	$L__BB2_63;

$L__BB2_64:
	setp.eq.s32 	%p75, %r59, 0;
	@%p75 bra 	$L__BB2_80;

	setp.eq.s32 	%p76, %r59, 1;
	bra.uni 	$L__BB2_80;

$L__BB2_66:
	setp.eq.s32 	%p78, %r173, 1;
	mov.u32 	%r2289, %r1;
	mov.u32 	%r2290, %r2282;
	@%p78 bra 	$L__BB2_69;

	and.b32  	%r1176, %r173, 1;
	sub.s32 	%r2288, %r173, %r1176;
	mov.u32 	%r2289, %r1;
	mov.u32 	%r2290, %r2282;

$L__BB2_68:
	mul.wide.u32 	%rd237, %r2289, 8;
	add.s64 	%rd238, %rd2, %rd237;
	ld.global.u64 	%rd239, [%rd238];
	cvt.u32.u64 	%r1179, %rd239;
	shr.u64 	%rd240, %rd239, 32;
	cvt.u32.u64 	%r1180, %rd240;
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r1177, %r1179, %r77;        
	subc.cc.u32 %r1178, %r1180, %r78;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.cc.u32 %r1177, %r1177, %r3; 
	@%pborrow addc.u32 %r1178, %r1178, %r50; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r1185, %r1177, %r72;
	mul.hi.u32 	%r1195, %r1177, %r72;
	mul.lo.s32 	%r1315, %r1185, %r4;
	mul.lo.s32 	%r1188, %r1315, %r3;
	mul.hi.u32 	%r1189, %r1315, %r3;
	mov.u32 	%r1311, 0;
	mov.u32 	%r1196, %r1311;
	// begin inline asm
	add.cc.u32 %r1185, %r1185, %r1188;   /* inline */   
	addc.cc.u32 %r1195, %r1195, %r1189;   /* inline */   
	addc.u32 %r1196, %r1196, %r1196;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1197, %r1177, %r73;
	mul.hi.u32 	%r1198, %r1177, %r73;
	// begin inline asm
	add.cc.u32 %r1211, %r1195, %r1197;   /* inline */   
	addc.cc.u32 %r1195, %r1196, %r1198;   /* inline */   
	addc.u32 %r1196, %r1311, %r1311;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1205, %r1178, %r72;
	mul.hi.u32 	%r1206, %r1178, %r72;
	// begin inline asm
	add.cc.u32 %r1211, %r1211, %r1205;   /* inline */   
	addc.cc.u32 %r1195, %r1195, %r1206;   /* inline */   
	addc.u32 %r1196, %r1196, %r1311;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1214, %r1315, %r50;
	mul.hi.u32 	%r1215, %r1315, %r50;
	// begin inline asm
	add.cc.u32 %r1211, %r1211, %r1214;   /* inline */   
	addc.cc.u32 %r1195, %r1195, %r1215;   /* inline */   
	addc.u32 %r1196, %r1196, %r1311;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1316, %r1211, %r4;
	mul.lo.s32 	%r1223, %r1316, %r3;
	mul.hi.u32 	%r1224, %r1316, %r3;
	// begin inline asm
	add.cc.u32 %r1211, %r1211, %r1223;   /* inline */   
	addc.cc.u32 %r1195, %r1195, %r1224;   /* inline */   
	addc.u32 %r1196, %r1196, %r1311;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1232, %r1178, %r73;
	mul.hi.u32 	%r1233, %r1178, %r73;
	// begin inline asm
	add.cc.u32 %r1237, %r1195, %r1232;   /* inline */   
	addc.cc.u32 %r1195, %r1196, %r1233;   /* inline */   
	addc.u32 %r1196, %r1311, %r1311;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1240, %r1316, %r50;
	mul.hi.u32 	%r1241, %r1316, %r50;
	// begin inline asm
	add.cc.u32 %r1237, %r1237, %r1240;   /* inline */   
	addc.cc.u32 %r1195, %r1195, %r1241;   /* inline */   
	addc.u32 %r1196, %r1196, %r1311;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd241, %r1195;
	cvt.u64.u32 	%rd242, %r1237;
	bfi.b64 	%rd243, %rd241, %rd242, 32, 32;
	setp.ne.s32 	%p79, %r1196, 0;
	setp.ge.u64 	%p80, %rd243, %rd5;
	or.pred  	%p81, %p79, %p80;
	selp.b64 	%rd244, %rd5, 0, %p81;
	sub.s64 	%rd245, %rd243, %rd244;
	setp.gt.u64 	%p82, %rd245, %rd71;
	selp.b64 	%rd246, %rd5, 0, %p82;
	sub.s64 	%rd247, %rd245, %rd246;
	mul.wide.u32 	%rd248, %r2290, 4;
	add.s64 	%rd249, %rd1, %rd248;
	st.global.u32 	[%rd249], %r79;
	mul.wide.u32 	%rd250, %r2290, 8;
	add.s64 	%rd251, %rd4, %rd250;
	st.global.u64 	[%rd251], %rd247;
	add.s32 	%r1317, %r2289, %r172;
	mul.wide.u32 	%rd252, %r1317, 8;
	add.s64 	%rd253, %rd2, %rd252;
	ld.global.u64 	%rd254, [%rd253];
	cvt.u32.u64 	%r1248, %rd254;
	shr.u64 	%rd255, %rd254, 32;
	cvt.u32.u64 	%r1249, %rd255;
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r1246, %r1248, %r77;        
	subc.cc.u32 %r1247, %r1249, %r78;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.cc.u32 %r1246, %r1246, %r3; 
	@%pborrow addc.u32 %r1247, %r1247, %r50; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r1254, %r1246, %r72;
	mul.hi.u32 	%r1264, %r1246, %r72;
	mul.lo.s32 	%r1318, %r1254, %r4;
	mul.lo.s32 	%r1257, %r1318, %r3;
	mul.hi.u32 	%r1258, %r1318, %r3;
	mov.u32 	%r1265, %r1311;
	// begin inline asm
	add.cc.u32 %r1254, %r1254, %r1257;   /* inline */   
	addc.cc.u32 %r1264, %r1264, %r1258;   /* inline */   
	addc.u32 %r1265, %r1265, %r1265;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1266, %r1246, %r73;
	mul.hi.u32 	%r1267, %r1246, %r73;
	// begin inline asm
	add.cc.u32 %r1280, %r1264, %r1266;   /* inline */   
	addc.cc.u32 %r1264, %r1265, %r1267;   /* inline */   
	addc.u32 %r1265, %r1311, %r1311;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1274, %r1247, %r72;
	mul.hi.u32 	%r1275, %r1247, %r72;
	// begin inline asm
	add.cc.u32 %r1280, %r1280, %r1274;   /* inline */   
	addc.cc.u32 %r1264, %r1264, %r1275;   /* inline */   
	addc.u32 %r1265, %r1265, %r1311;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1283, %r1318, %r50;
	mul.hi.u32 	%r1284, %r1318, %r50;
	// begin inline asm
	add.cc.u32 %r1280, %r1280, %r1283;   /* inline */   
	addc.cc.u32 %r1264, %r1264, %r1284;   /* inline */   
	addc.u32 %r1265, %r1265, %r1311;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1319, %r1280, %r4;
	mul.lo.s32 	%r1292, %r1319, %r3;
	mul.hi.u32 	%r1293, %r1319, %r3;
	// begin inline asm
	add.cc.u32 %r1280, %r1280, %r1292;   /* inline */   
	addc.cc.u32 %r1264, %r1264, %r1293;   /* inline */   
	addc.u32 %r1265, %r1265, %r1311;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1301, %r1247, %r73;
	mul.hi.u32 	%r1302, %r1247, %r73;
	// begin inline asm
	add.cc.u32 %r1306, %r1264, %r1301;   /* inline */   
	addc.cc.u32 %r1264, %r1265, %r1302;   /* inline */   
	addc.u32 %r1265, %r1311, %r1311;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1309, %r1319, %r50;
	mul.hi.u32 	%r1310, %r1319, %r50;
	// begin inline asm
	add.cc.u32 %r1306, %r1306, %r1309;   /* inline */   
	addc.cc.u32 %r1264, %r1264, %r1310;   /* inline */   
	addc.u32 %r1265, %r1265, %r1311;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd256, %r1264;
	cvt.u64.u32 	%rd257, %r1306;
	bfi.b64 	%rd258, %rd256, %rd257, 32, 32;
	setp.ne.s32 	%p83, %r1265, 0;
	setp.ge.u64 	%p84, %rd258, %rd5;
	or.pred  	%p85, %p83, %p84;
	selp.b64 	%rd259, %rd5, 0, %p85;
	sub.s64 	%rd260, %rd258, %rd259;
	setp.gt.u64 	%p86, %rd260, %rd71;
	selp.b64 	%rd261, %rd5, 0, %p86;
	sub.s64 	%rd262, %rd260, %rd261;
	add.s32 	%r1320, %r2290, %r172;
	mul.wide.u32 	%rd263, %r1320, 4;
	add.s64 	%rd264, %rd1, %rd263;
	st.global.u32 	[%rd264], %r79;
	mul.wide.u32 	%rd265, %r1320, 8;
	add.s64 	%rd266, %rd4, %rd265;
	st.global.u64 	[%rd266], %rd262;
	add.s32 	%r2290, %r1320, %r172;
	add.s32 	%r2289, %r1317, %r172;
	add.s32 	%r2288, %r2288, -2;
	setp.ne.s32 	%p87, %r2288, 0;
	@%p87 bra 	$L__BB2_68;

$L__BB2_69:
	and.b32  	%r1321, %r173, 1;
	setp.eq.b32 	%p88, %r1321, 1;
	mov.pred 	%p89, 0;
	xor.pred  	%p90, %p88, %p89;
	not.pred 	%p91, %p90;
	@%p91 bra 	$L__BB2_80;

	mul.wide.u32 	%rd267, %r2289, 8;
	add.s64 	%rd268, %rd2, %rd267;
	ld.global.u64 	%rd269, [%rd268];
	cvt.u32.u64 	%r1324, %rd269;
	shr.u64 	%rd270, %rd269, 32;
	cvt.u32.u64 	%r1325, %rd270;
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r1322, %r1324, %r77;        
	subc.cc.u32 %r1323, %r1325, %r78;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.cc.u32 %r1322, %r1322, %r3; 
	@%pborrow addc.u32 %r1323, %r1323, %r50; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r1330, %r1322, %r72;
	mul.hi.u32 	%r1340, %r1322, %r72;
	mul.lo.s32 	%r1391, %r1330, %r4;
	mul.lo.s32 	%r1333, %r1391, %r3;
	mul.hi.u32 	%r1334, %r1391, %r3;
	mov.u32 	%r1387, 0;
	mov.u32 	%r1341, %r1387;
	// begin inline asm
	add.cc.u32 %r1330, %r1330, %r1333;   /* inline */   
	addc.cc.u32 %r1340, %r1340, %r1334;   /* inline */   
	addc.u32 %r1341, %r1341, %r1341;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1342, %r1322, %r73;
	mul.hi.u32 	%r1343, %r1322, %r73;
	// begin inline asm
	add.cc.u32 %r1356, %r1340, %r1342;   /* inline */   
	addc.cc.u32 %r1340, %r1341, %r1343;   /* inline */   
	addc.u32 %r1341, %r1387, %r1387;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1350, %r1323, %r72;
	mul.hi.u32 	%r1351, %r1323, %r72;
	// begin inline asm
	add.cc.u32 %r1356, %r1356, %r1350;   /* inline */   
	addc.cc.u32 %r1340, %r1340, %r1351;   /* inline */   
	addc.u32 %r1341, %r1341, %r1387;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1359, %r1391, %r50;
	mul.hi.u32 	%r1360, %r1391, %r50;
	// begin inline asm
	add.cc.u32 %r1356, %r1356, %r1359;   /* inline */   
	addc.cc.u32 %r1340, %r1340, %r1360;   /* inline */   
	addc.u32 %r1341, %r1341, %r1387;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1392, %r1356, %r4;
	mul.lo.s32 	%r1368, %r1392, %r3;
	mul.hi.u32 	%r1369, %r1392, %r3;
	// begin inline asm
	add.cc.u32 %r1356, %r1356, %r1368;   /* inline */   
	addc.cc.u32 %r1340, %r1340, %r1369;   /* inline */   
	addc.u32 %r1341, %r1341, %r1387;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1377, %r1323, %r73;
	mul.hi.u32 	%r1378, %r1323, %r73;
	// begin inline asm
	add.cc.u32 %r1382, %r1340, %r1377;   /* inline */   
	addc.cc.u32 %r1340, %r1341, %r1378;   /* inline */   
	addc.u32 %r1341, %r1387, %r1387;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1385, %r1392, %r50;
	mul.hi.u32 	%r1386, %r1392, %r50;
	// begin inline asm
	add.cc.u32 %r1382, %r1382, %r1385;   /* inline */   
	addc.cc.u32 %r1340, %r1340, %r1386;   /* inline */   
	addc.u32 %r1341, %r1341, %r1387;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd271, %r1340;
	cvt.u64.u32 	%rd272, %r1382;
	bfi.b64 	%rd273, %rd271, %rd272, 32, 32;
	setp.ne.s32 	%p92, %r1341, 0;
	setp.ge.u64 	%p93, %rd273, %rd5;
	or.pred  	%p94, %p92, %p93;
	selp.b64 	%rd274, %rd5, 0, %p94;
	sub.s64 	%rd275, %rd273, %rd274;
	setp.gt.u64 	%p95, %rd275, %rd71;
	selp.b64 	%rd276, %rd5, 0, %p95;
	sub.s64 	%rd277, %rd275, %rd276;
	mul.wide.u32 	%rd278, %r2290, 4;
	add.s64 	%rd279, %rd1, %rd278;
	st.global.u32 	[%rd279], %r79;
	mul.wide.u32 	%rd280, %r2290, 8;
	add.s64 	%rd281, %rd4, %rd280;
	st.global.u64 	[%rd281], %rd277;
	bra.uni 	$L__BB2_80;

$L__BB2_71:
	mov.u32 	%r1393, 0;
	mov.u32 	%r2291, %r1393;
	mov.u32 	%r2292, %r1;
	mov.u32 	%r2293, %r2282;

$L__BB2_72:
	mul.wide.u32 	%rd282, %r2292, 8;
	add.s64 	%rd283, %rd2, %rd282;
	ld.global.u64 	%rd284, [%rd283];
	cvt.u32.u64 	%r1396, %rd284;
	shr.u64 	%rd285, %rd284, 32;
	cvt.u32.u64 	%r1397, %rd285;
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r1394, %r1396, %r77;        
	subc.cc.u32 %r1395, %r1397, %r78;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.cc.u32 %r1394, %r1394, %r3; 
	@%pborrow addc.u32 %r1395, %r1395, %r50; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r1402, %r1394, %r72;
	mul.hi.u32 	%r1412, %r1394, %r72;
	mul.lo.s32 	%r1463, %r1402, %r4;
	mul.lo.s32 	%r1405, %r1463, %r3;
	mul.hi.u32 	%r1406, %r1463, %r3;
	mov.u32 	%r1413, %r1393;
	// begin inline asm
	add.cc.u32 %r1402, %r1402, %r1405;   /* inline */   
	addc.cc.u32 %r1412, %r1412, %r1406;   /* inline */   
	addc.u32 %r1413, %r1413, %r1413;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1414, %r1394, %r73;
	mul.hi.u32 	%r1415, %r1394, %r73;
	// begin inline asm
	add.cc.u32 %r1428, %r1412, %r1414;   /* inline */   
	addc.cc.u32 %r1412, %r1413, %r1415;   /* inline */   
	addc.u32 %r1413, %r1393, %r1393;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1422, %r1395, %r72;
	mul.hi.u32 	%r1423, %r1395, %r72;
	// begin inline asm
	add.cc.u32 %r1428, %r1428, %r1422;   /* inline */   
	addc.cc.u32 %r1412, %r1412, %r1423;   /* inline */   
	addc.u32 %r1413, %r1413, %r1393;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1431, %r1463, %r50;
	mul.hi.u32 	%r1432, %r1463, %r50;
	// begin inline asm
	add.cc.u32 %r1428, %r1428, %r1431;   /* inline */   
	addc.cc.u32 %r1412, %r1412, %r1432;   /* inline */   
	addc.u32 %r1413, %r1413, %r1393;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1464, %r1428, %r4;
	mul.lo.s32 	%r1440, %r1464, %r3;
	mul.hi.u32 	%r1441, %r1464, %r3;
	// begin inline asm
	add.cc.u32 %r1428, %r1428, %r1440;   /* inline */   
	addc.cc.u32 %r1412, %r1412, %r1441;   /* inline */   
	addc.u32 %r1413, %r1413, %r1393;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1449, %r1395, %r73;
	mul.hi.u32 	%r1450, %r1395, %r73;
	// begin inline asm
	add.cc.u32 %r1454, %r1412, %r1449;   /* inline */   
	addc.cc.u32 %r1412, %r1413, %r1450;   /* inline */   
	addc.u32 %r1413, %r1393, %r1393;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1457, %r1464, %r50;
	mul.hi.u32 	%r1458, %r1464, %r50;
	// begin inline asm
	add.cc.u32 %r1454, %r1454, %r1457;   /* inline */   
	addc.cc.u32 %r1412, %r1412, %r1458;   /* inline */   
	addc.u32 %r1413, %r1413, %r1393;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd286, %r1412;
	cvt.u64.u32 	%rd287, %r1454;
	bfi.b64 	%rd288, %rd286, %rd287, 32, 32;
	setp.ne.s32 	%p96, %r1413, 0;
	setp.ge.u64 	%p97, %rd288, %rd5;
	or.pred  	%p98, %p96, %p97;
	selp.b64 	%rd289, %rd5, 0, %p98;
	sub.s64 	%rd290, %rd288, %rd289;
	sub.s64 	%rd454, %rd290, %rd72;
	setp.lt.u32 	%p99, %r56, 3;
	mov.u32 	%r2296, %r2293;
	@%p99 bra 	$L__BB2_75;

	mov.u32 	%r2296, %r2293;
	mov.u32 	%r2295, %r58;

$L__BB2_74:
	mul.wide.u32 	%rd291, %r2296, 4;
	add.s64 	%rd292, %rd1, %rd291;
	st.global.u32 	[%rd292], %r79;
	mul.wide.u32 	%rd293, %r2296, 8;
	add.s64 	%rd294, %rd4, %rd293;
	st.global.u64 	[%rd294], %rd454;
	add.s32 	%r1465, %r2296, %r7;
	mul.wide.u32 	%rd295, %r1465, 4;
	add.s64 	%rd296, %rd1, %rd295;
	st.global.u32 	[%rd296], %r79;
	mul.wide.u32 	%rd297, %r1465, 8;
	add.s64 	%rd298, %rd4, %rd297;
	add.s64 	%rd299, %rd454, %rd5;
	st.global.u64 	[%rd298], %rd299;
	add.s32 	%r1466, %r1465, %r7;
	add.s64 	%rd300, %rd299, %rd5;
	mul.wide.u32 	%rd301, %r1466, 4;
	add.s64 	%rd302, %rd1, %rd301;
	st.global.u32 	[%rd302], %r79;
	mul.wide.u32 	%rd303, %r1466, 8;
	add.s64 	%rd304, %rd4, %rd303;
	st.global.u64 	[%rd304], %rd300;
	add.s32 	%r1467, %r1466, %r7;
	add.s64 	%rd305, %rd300, %rd5;
	mul.wide.u32 	%rd306, %r1467, 4;
	add.s64 	%rd307, %rd1, %rd306;
	st.global.u32 	[%rd307], %r79;
	mul.wide.u32 	%rd308, %r1467, 8;
	add.s64 	%rd309, %rd4, %rd308;
	st.global.u64 	[%rd309], %rd305;
	add.s32 	%r2296, %r1467, %r7;
	add.s64 	%rd454, %rd305, %rd5;
	add.s32 	%r2295, %r2295, -4;
	setp.ne.s32 	%p100, %r2295, 0;
	@%p100 bra 	$L__BB2_74;

$L__BB2_75:
	setp.eq.s32 	%p101, %r57, 0;
	@%p101 bra 	$L__BB2_79;

	setp.eq.s32 	%p102, %r57, 1;
	mul.wide.u32 	%rd310, %r2296, 4;
	add.s64 	%rd311, %rd1, %rd310;
	st.global.u32 	[%rd311], %r79;
	mul.wide.u32 	%rd312, %r2296, 8;
	add.s64 	%rd313, %rd4, %rd312;
	st.global.u64 	[%rd313], %rd454;
	@%p102 bra 	$L__BB2_79;

	add.s32 	%r103, %r2296, %r7;
	add.s64 	%rd89, %rd454, %rd5;
	setp.eq.s32 	%p103, %r57, 2;
	mul.wide.u32 	%rd314, %r103, 4;
	add.s64 	%rd315, %rd1, %rd314;
	st.global.u32 	[%rd315], %r79;
	mul.wide.u32 	%rd316, %r103, 8;
	add.s64 	%rd317, %rd4, %rd316;
	st.global.u64 	[%rd317], %rd89;
	@%p103 bra 	$L__BB2_79;

	add.s32 	%r1468, %r103, %r7;
	mul.wide.u32 	%rd318, %r1468, 4;
	add.s64 	%rd319, %rd1, %rd318;
	st.global.u32 	[%rd319], %r79;
	mul.wide.u32 	%rd320, %r1468, 8;
	add.s64 	%rd321, %rd4, %rd320;
	add.s64 	%rd322, %rd89, %rd5;
	st.global.u64 	[%rd321], %rd322;

$L__BB2_79:
	add.s32 	%r2293, %r2293, %r172;
	add.s32 	%r2292, %r2292, %r172;
	add.s32 	%r2291, %r2291, 1;
	setp.lt.u32 	%p104, %r2291, %r173;
	@%p104 bra 	$L__BB2_72;

$L__BB2_80:
	add.s32 	%r2281, %r2281, -1;
	sub.s32 	%r2282, %r2282, %r176;
	add.s32 	%r2283, %r2283, -1;
	setp.eq.s32 	%p105, %r2283, 0;
	mov.u32 	%r2299, 1;
	mov.u64 	%rd426, %rd75;
	mov.u32 	%r2297, %r2277;
	mov.u32 	%r2298, %r2276;
	@%p105 bra 	$L__BB2_82;
	bra.uni 	$L__BB2_55;

$L__BB2_82:
	add.s32 	%r2277, %r2277, -1;
	setp.ge.s32 	%p106, %r2277, %r23;
	@%p106 bra 	$L__BB2_49;

$L__BB2_83:
	setp.lt.s32 	%p107, %r2297, %r23;
	@%p107 bra 	$L__BB2_111;

	cvt.u32.u64 	%r117, %rd456;
	shr.u64 	%rd323, %rd456, 32;
	cvt.u32.u64 	%r118, %rd323;
	shr.u64 	%rd93, %rd5, 1;
	mov.u64 	%rd324, 1;
	shr.u32 	%r1470, %r178, 1;
	cvt.u64.u32 	%rd325, %r1470;
	mul.lo.s64 	%rd94, %rd5, %rd325;
	add.s32 	%r119, %r178, -1;
	add.s32 	%r120, %r173, -1;
	and.b32  	%r121, %r173, 1;
	sub.s32 	%r122, %r173, %r121;
	and.b32  	%r123, %r173, 3;
	sub.s32 	%r124, %r173, %r123;
	mul.lo.s32 	%r125, %r7, 3;
	shl.b32 	%r126, %r7, 2;
	shl.b32 	%r127, %r7, 1;
	mov.u64 	%rd326, 3;
	sub.s64 	%rd327, %rd326, %rd325;
	mul.lo.s64 	%rd95, %rd327, %rd5;
	shl.b64 	%rd96, %rd5, 2;
	mov.u64 	%rd328, 2;
	sub.s64 	%rd329, %rd328, %rd325;
	mul.lo.s64 	%rd97, %rd329, %rd5;
	sub.s64 	%rd330, %rd324, %rd325;
	mul.lo.s64 	%rd98, %rd330, %rd5;
	and.b32  	%r128, %r178, 3;
	sub.s32 	%r129, %r128, %r178;

$L__BB2_85:
	mul.wide.u32 	%rd331, %r2297, 24;
	add.s64 	%rd332, %rd3, %rd331;
	ld.global.u64 	%rd99, [%rd332+16];
	or.b64  	%rd333, %rd99, %rd5;
	and.b64  	%rd334, %rd333, -4294967296;
	setp.eq.s64 	%p108, %rd334, 0;
	@%p108 bra 	$L__BB2_87;

	rem.u64 	%rd458, %rd99, %rd5;
	bra.uni 	$L__BB2_88;

$L__BB2_87:
	cvt.u32.u64 	%r1472, %rd99;
	rem.u32 	%r1473, %r1472, %r3;
	cvt.u64.u32 	%rd458, %r1473;

$L__BB2_88:
	setp.eq.s32 	%p109, %r173, 0;
	@%p109 bra 	$L__BB2_110;

	cvt.u32.u64 	%r132, %rd458;
	shr.u64 	%rd335, %rd458, 32;
	cvt.u32.u64 	%r133, %rd335;
	shl.b32 	%r1474, %r2297, %r177;
	or.b32  	%r134, %r1474, %r2;
	setp.eq.s32 	%p110, %r178, 1;
	@%p110 bra 	$L__BB2_96;

	setp.ne.s32 	%p111, %r178, 0;
	@%p111 bra 	$L__BB2_101;

	setp.lt.u32 	%p112, %r120, 3;
	@%p112 bra 	$L__BB2_94;

	mov.u32 	%r2304, %r1;
	mov.u32 	%r2305, %r124;

$L__BB2_93:
	add.s32 	%r1751, %r2304, %r172;
	add.s32 	%r1752, %r1751, %r172;
	add.s32 	%r1753, %r1752, %r172;
	add.s32 	%r2304, %r1753, %r172;
	add.s32 	%r2305, %r2305, -4;
	setp.ne.s32 	%p113, %r2305, 0;
	@%p113 bra 	$L__BB2_93;

$L__BB2_94:
	setp.eq.s32 	%p114, %r123, 0;
	@%p114 bra 	$L__BB2_110;

	setp.eq.s32 	%p115, %r123, 1;
	bra.uni 	$L__BB2_110;

$L__BB2_96:
	setp.eq.s32 	%p117, %r120, 0;
	mov.u32 	%r2309, %r1;
	mov.u32 	%r2310, %r2298;
	@%p117 bra 	$L__BB2_99;

	mov.u32 	%r2309, %r1;
	mov.u32 	%r2310, %r2298;
	mov.u32 	%r2308, %r122;

$L__BB2_98:
	mul.wide.u32 	%rd336, %r2309, 8;
	add.s64 	%rd337, %rd2, %rd336;
	ld.global.u64 	%rd338, [%rd337];
	cvt.u32.u64 	%r1963, %rd338;
	shr.u64 	%rd339, %rd338, 32;
	cvt.u32.u64 	%r1964, %rd339;
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r1961, %r1963, %r132;        
	subc.cc.u32 %r1962, %r1964, %r133;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.cc.u32 %r1961, %r1961, %r3; 
	@%pborrow addc.u32 %r1962, %r1962, %r50; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r1969, %r1961, %r117;
	mul.hi.u32 	%r1979, %r1961, %r117;
	mul.lo.s32 	%r2099, %r1969, %r4;
	mul.lo.s32 	%r1972, %r2099, %r3;
	mul.hi.u32 	%r1973, %r2099, %r3;
	mov.u32 	%r2095, 0;
	mov.u32 	%r1980, %r2095;
	// begin inline asm
	add.cc.u32 %r1969, %r1969, %r1972;   /* inline */   
	addc.cc.u32 %r1979, %r1979, %r1973;   /* inline */   
	addc.u32 %r1980, %r1980, %r1980;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1981, %r1961, %r118;
	mul.hi.u32 	%r1982, %r1961, %r118;
	// begin inline asm
	add.cc.u32 %r1995, %r1979, %r1981;   /* inline */   
	addc.cc.u32 %r1979, %r1980, %r1982;   /* inline */   
	addc.u32 %r1980, %r2095, %r2095;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1989, %r1962, %r117;
	mul.hi.u32 	%r1990, %r1962, %r117;
	// begin inline asm
	add.cc.u32 %r1995, %r1995, %r1989;   /* inline */   
	addc.cc.u32 %r1979, %r1979, %r1990;   /* inline */   
	addc.u32 %r1980, %r1980, %r2095;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r1998, %r2099, %r50;
	mul.hi.u32 	%r1999, %r2099, %r50;
	// begin inline asm
	add.cc.u32 %r1995, %r1995, %r1998;   /* inline */   
	addc.cc.u32 %r1979, %r1979, %r1999;   /* inline */   
	addc.u32 %r1980, %r1980, %r2095;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2100, %r1995, %r4;
	mul.lo.s32 	%r2007, %r2100, %r3;
	mul.hi.u32 	%r2008, %r2100, %r3;
	// begin inline asm
	add.cc.u32 %r1995, %r1995, %r2007;   /* inline */   
	addc.cc.u32 %r1979, %r1979, %r2008;   /* inline */   
	addc.u32 %r1980, %r1980, %r2095;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2016, %r1962, %r118;
	mul.hi.u32 	%r2017, %r1962, %r118;
	// begin inline asm
	add.cc.u32 %r2021, %r1979, %r2016;   /* inline */   
	addc.cc.u32 %r1979, %r1980, %r2017;   /* inline */   
	addc.u32 %r1980, %r2095, %r2095;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2024, %r2100, %r50;
	mul.hi.u32 	%r2025, %r2100, %r50;
	// begin inline asm
	add.cc.u32 %r2021, %r2021, %r2024;   /* inline */   
	addc.cc.u32 %r1979, %r1979, %r2025;   /* inline */   
	addc.u32 %r1980, %r1980, %r2095;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd340, %r1979;
	cvt.u64.u32 	%rd341, %r2021;
	bfi.b64 	%rd342, %rd340, %rd341, 32, 32;
	setp.ne.s32 	%p118, %r1980, 0;
	setp.ge.u64 	%p119, %rd342, %rd5;
	or.pred  	%p120, %p118, %p119;
	selp.b64 	%rd343, %rd5, 0, %p120;
	sub.s64 	%rd344, %rd342, %rd343;
	setp.gt.u64 	%p121, %rd344, %rd93;
	selp.b64 	%rd345, %rd5, 0, %p121;
	sub.s64 	%rd346, %rd344, %rd345;
	mul.wide.u32 	%rd347, %r2310, 4;
	add.s64 	%rd348, %rd1, %rd347;
	st.global.u32 	[%rd348], %r134;
	mul.wide.u32 	%rd349, %r2310, 8;
	add.s64 	%rd350, %rd4, %rd349;
	st.global.u64 	[%rd350], %rd346;
	add.s32 	%r2101, %r2309, %r172;
	mul.wide.u32 	%rd351, %r2101, 8;
	add.s64 	%rd352, %rd2, %rd351;
	ld.global.u64 	%rd353, [%rd352];
	cvt.u32.u64 	%r2032, %rd353;
	shr.u64 	%rd354, %rd353, 32;
	cvt.u32.u64 	%r2033, %rd354;
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r2030, %r2032, %r132;        
	subc.cc.u32 %r2031, %r2033, %r133;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.cc.u32 %r2030, %r2030, %r3; 
	@%pborrow addc.u32 %r2031, %r2031, %r50; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r2038, %r2030, %r117;
	mul.hi.u32 	%r2048, %r2030, %r117;
	mul.lo.s32 	%r2102, %r2038, %r4;
	mul.lo.s32 	%r2041, %r2102, %r3;
	mul.hi.u32 	%r2042, %r2102, %r3;
	mov.u32 	%r2049, %r2095;
	// begin inline asm
	add.cc.u32 %r2038, %r2038, %r2041;   /* inline */   
	addc.cc.u32 %r2048, %r2048, %r2042;   /* inline */   
	addc.u32 %r2049, %r2049, %r2049;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2050, %r2030, %r118;
	mul.hi.u32 	%r2051, %r2030, %r118;
	// begin inline asm
	add.cc.u32 %r2064, %r2048, %r2050;   /* inline */   
	addc.cc.u32 %r2048, %r2049, %r2051;   /* inline */   
	addc.u32 %r2049, %r2095, %r2095;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2058, %r2031, %r117;
	mul.hi.u32 	%r2059, %r2031, %r117;
	// begin inline asm
	add.cc.u32 %r2064, %r2064, %r2058;   /* inline */   
	addc.cc.u32 %r2048, %r2048, %r2059;   /* inline */   
	addc.u32 %r2049, %r2049, %r2095;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2067, %r2102, %r50;
	mul.hi.u32 	%r2068, %r2102, %r50;
	// begin inline asm
	add.cc.u32 %r2064, %r2064, %r2067;   /* inline */   
	addc.cc.u32 %r2048, %r2048, %r2068;   /* inline */   
	addc.u32 %r2049, %r2049, %r2095;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2103, %r2064, %r4;
	mul.lo.s32 	%r2076, %r2103, %r3;
	mul.hi.u32 	%r2077, %r2103, %r3;
	// begin inline asm
	add.cc.u32 %r2064, %r2064, %r2076;   /* inline */   
	addc.cc.u32 %r2048, %r2048, %r2077;   /* inline */   
	addc.u32 %r2049, %r2049, %r2095;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2085, %r2031, %r118;
	mul.hi.u32 	%r2086, %r2031, %r118;
	// begin inline asm
	add.cc.u32 %r2090, %r2048, %r2085;   /* inline */   
	addc.cc.u32 %r2048, %r2049, %r2086;   /* inline */   
	addc.u32 %r2049, %r2095, %r2095;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2093, %r2103, %r50;
	mul.hi.u32 	%r2094, %r2103, %r50;
	// begin inline asm
	add.cc.u32 %r2090, %r2090, %r2093;   /* inline */   
	addc.cc.u32 %r2048, %r2048, %r2094;   /* inline */   
	addc.u32 %r2049, %r2049, %r2095;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd355, %r2048;
	cvt.u64.u32 	%rd356, %r2090;
	bfi.b64 	%rd357, %rd355, %rd356, 32, 32;
	setp.ne.s32 	%p122, %r2049, 0;
	setp.ge.u64 	%p123, %rd357, %rd5;
	or.pred  	%p124, %p122, %p123;
	selp.b64 	%rd358, %rd5, 0, %p124;
	sub.s64 	%rd359, %rd357, %rd358;
	setp.gt.u64 	%p125, %rd359, %rd93;
	selp.b64 	%rd360, %rd5, 0, %p125;
	sub.s64 	%rd361, %rd359, %rd360;
	add.s32 	%r2104, %r2310, %r172;
	mul.wide.u32 	%rd362, %r2104, 4;
	add.s64 	%rd363, %rd1, %rd362;
	st.global.u32 	[%rd363], %r134;
	mul.wide.u32 	%rd364, %r2104, 8;
	add.s64 	%rd365, %rd4, %rd364;
	st.global.u64 	[%rd365], %rd361;
	add.s32 	%r2310, %r2104, %r172;
	add.s32 	%r2309, %r2101, %r172;
	add.s32 	%r2308, %r2308, -2;
	setp.ne.s32 	%p126, %r2308, 0;
	@%p126 bra 	$L__BB2_98;

$L__BB2_99:
	setp.eq.s32 	%p127, %r121, 0;
	@%p127 bra 	$L__BB2_110;

	mul.wide.u32 	%rd366, %r2309, 8;
	add.s64 	%rd367, %rd2, %rd366;
	ld.global.u64 	%rd368, [%rd367];
	cvt.u32.u64 	%r2107, %rd368;
	shr.u64 	%rd369, %rd368, 32;
	cvt.u32.u64 	%r2108, %rd369;
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r2105, %r2107, %r132;        
	subc.cc.u32 %r2106, %r2108, %r133;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.cc.u32 %r2105, %r2105, %r3; 
	@%pborrow addc.u32 %r2106, %r2106, %r50; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r2113, %r2105, %r117;
	mul.hi.u32 	%r2123, %r2105, %r117;
	mul.lo.s32 	%r2174, %r2113, %r4;
	mul.lo.s32 	%r2116, %r2174, %r3;
	mul.hi.u32 	%r2117, %r2174, %r3;
	mov.u32 	%r2170, 0;
	mov.u32 	%r2124, %r2170;
	// begin inline asm
	add.cc.u32 %r2113, %r2113, %r2116;   /* inline */   
	addc.cc.u32 %r2123, %r2123, %r2117;   /* inline */   
	addc.u32 %r2124, %r2124, %r2124;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2125, %r2105, %r118;
	mul.hi.u32 	%r2126, %r2105, %r118;
	// begin inline asm
	add.cc.u32 %r2139, %r2123, %r2125;   /* inline */   
	addc.cc.u32 %r2123, %r2124, %r2126;   /* inline */   
	addc.u32 %r2124, %r2170, %r2170;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2133, %r2106, %r117;
	mul.hi.u32 	%r2134, %r2106, %r117;
	// begin inline asm
	add.cc.u32 %r2139, %r2139, %r2133;   /* inline */   
	addc.cc.u32 %r2123, %r2123, %r2134;   /* inline */   
	addc.u32 %r2124, %r2124, %r2170;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2142, %r2174, %r50;
	mul.hi.u32 	%r2143, %r2174, %r50;
	// begin inline asm
	add.cc.u32 %r2139, %r2139, %r2142;   /* inline */   
	addc.cc.u32 %r2123, %r2123, %r2143;   /* inline */   
	addc.u32 %r2124, %r2124, %r2170;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2175, %r2139, %r4;
	mul.lo.s32 	%r2151, %r2175, %r3;
	mul.hi.u32 	%r2152, %r2175, %r3;
	// begin inline asm
	add.cc.u32 %r2139, %r2139, %r2151;   /* inline */   
	addc.cc.u32 %r2123, %r2123, %r2152;   /* inline */   
	addc.u32 %r2124, %r2124, %r2170;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2160, %r2106, %r118;
	mul.hi.u32 	%r2161, %r2106, %r118;
	// begin inline asm
	add.cc.u32 %r2165, %r2123, %r2160;   /* inline */   
	addc.cc.u32 %r2123, %r2124, %r2161;   /* inline */   
	addc.u32 %r2124, %r2170, %r2170;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2168, %r2175, %r50;
	mul.hi.u32 	%r2169, %r2175, %r50;
	// begin inline asm
	add.cc.u32 %r2165, %r2165, %r2168;   /* inline */   
	addc.cc.u32 %r2123, %r2123, %r2169;   /* inline */   
	addc.u32 %r2124, %r2124, %r2170;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd370, %r2123;
	cvt.u64.u32 	%rd371, %r2165;
	bfi.b64 	%rd372, %rd370, %rd371, 32, 32;
	setp.ne.s32 	%p128, %r2124, 0;
	setp.ge.u64 	%p129, %rd372, %rd5;
	or.pred  	%p130, %p128, %p129;
	selp.b64 	%rd373, %rd5, 0, %p130;
	sub.s64 	%rd374, %rd372, %rd373;
	setp.gt.u64 	%p131, %rd374, %rd93;
	selp.b64 	%rd375, %rd5, 0, %p131;
	sub.s64 	%rd376, %rd374, %rd375;
	mul.wide.u32 	%rd377, %r2310, 4;
	add.s64 	%rd378, %rd1, %rd377;
	st.global.u32 	[%rd378], %r134;
	mul.wide.u32 	%rd379, %r2310, 8;
	add.s64 	%rd380, %rd4, %rd379;
	st.global.u64 	[%rd380], %rd376;
	bra.uni 	$L__BB2_110;

$L__BB2_101:
	mov.u32 	%r2176, 0;
	mov.u32 	%r2311, %r2176;
	mov.u32 	%r2312, %r1;
	mov.u32 	%r2313, %r2298;

$L__BB2_102:
	mul.wide.u32 	%rd381, %r2312, 8;
	add.s64 	%rd382, %rd2, %rd381;
	ld.global.u64 	%rd383, [%rd382];
	cvt.u32.u64 	%r2179, %rd383;
	shr.u64 	%rd384, %rd383, 32;
	cvt.u32.u64 	%r2180, %rd384;
	// begin inline asm
	{  
	.reg .pred %pborrow;           
	.reg .u32 %borrow;           
	mov.b32 %borrow, 0;           
	sub.cc.u32 %r2177, %r2179, %r132;        
	subc.cc.u32 %r2178, %r2180, %r133;        
	subc.u32 %borrow, %borrow, 0; 
	setp.ne.u32 %pborrow, %borrow, 0;  
	@%pborrow add.cc.u32 %r2177, %r2177, %r3; 
	@%pborrow addc.u32 %r2178, %r2178, %r50; 
	} 
	
	// end inline asm
	mul.lo.s32 	%r2185, %r2177, %r117;
	mul.hi.u32 	%r2195, %r2177, %r117;
	mul.lo.s32 	%r2246, %r2185, %r4;
	mul.lo.s32 	%r2188, %r2246, %r3;
	mul.hi.u32 	%r2189, %r2246, %r3;
	mov.u32 	%r2196, %r2176;
	// begin inline asm
	add.cc.u32 %r2185, %r2185, %r2188;   /* inline */   
	addc.cc.u32 %r2195, %r2195, %r2189;   /* inline */   
	addc.u32 %r2196, %r2196, %r2196;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2197, %r2177, %r118;
	mul.hi.u32 	%r2198, %r2177, %r118;
	// begin inline asm
	add.cc.u32 %r2211, %r2195, %r2197;   /* inline */   
	addc.cc.u32 %r2195, %r2196, %r2198;   /* inline */   
	addc.u32 %r2196, %r2176, %r2176;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2205, %r2178, %r117;
	mul.hi.u32 	%r2206, %r2178, %r117;
	// begin inline asm
	add.cc.u32 %r2211, %r2211, %r2205;   /* inline */   
	addc.cc.u32 %r2195, %r2195, %r2206;   /* inline */   
	addc.u32 %r2196, %r2196, %r2176;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2214, %r2246, %r50;
	mul.hi.u32 	%r2215, %r2246, %r50;
	// begin inline asm
	add.cc.u32 %r2211, %r2211, %r2214;   /* inline */   
	addc.cc.u32 %r2195, %r2195, %r2215;   /* inline */   
	addc.u32 %r2196, %r2196, %r2176;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2247, %r2211, %r4;
	mul.lo.s32 	%r2223, %r2247, %r3;
	mul.hi.u32 	%r2224, %r2247, %r3;
	// begin inline asm
	add.cc.u32 %r2211, %r2211, %r2223;   /* inline */   
	addc.cc.u32 %r2195, %r2195, %r2224;   /* inline */   
	addc.u32 %r2196, %r2196, %r2176;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2232, %r2178, %r118;
	mul.hi.u32 	%r2233, %r2178, %r118;
	// begin inline asm
	add.cc.u32 %r2237, %r2195, %r2232;   /* inline */   
	addc.cc.u32 %r2195, %r2196, %r2233;   /* inline */   
	addc.u32 %r2196, %r2176, %r2176;   /* inline */   
	
	// end inline asm
	mul.lo.s32 	%r2240, %r2247, %r50;
	mul.hi.u32 	%r2241, %r2247, %r50;
	// begin inline asm
	add.cc.u32 %r2237, %r2237, %r2240;   /* inline */   
	addc.cc.u32 %r2195, %r2195, %r2241;   /* inline */   
	addc.u32 %r2196, %r2196, %r2176;   /* inline */   
	
	// end inline asm
	cvt.u64.u32 	%rd385, %r2195;
	cvt.u64.u32 	%rd386, %r2237;
	bfi.b64 	%rd103, %rd385, %rd386, 32, 32;
	setp.ne.s32 	%p132, %r2196, 0;
	setp.ge.u64 	%p133, %rd103, %rd5;
	or.pred  	%p134, %p132, %p133;
	selp.b64 	%rd104, %rd5, 0, %p134;
	sub.s64 	%rd387, %rd103, %rd104;
	sub.s64 	%rd463, %rd387, %rd94;
	setp.lt.u32 	%p135, %r119, 3;
	mov.u32 	%r2319, %r2313;
	@%p135 bra 	$L__BB2_105;

	add.s32 	%r2317, %r125, %r2313;
	add.s32 	%r2316, %r127, %r2313;
	add.s32 	%r2315, %r7, %r2313;
	add.s64 	%rd388, %rd95, %rd103;
	sub.s64 	%rd461, %rd388, %rd104;
	add.s64 	%rd389, %rd97, %rd103;
	sub.s64 	%rd460, %rd389, %rd104;
	add.s64 	%rd390, %rd98, %rd103;
	sub.s64 	%rd459, %rd390, %rd104;
	mov.u32 	%r2314, %r129;
	mov.u32 	%r2319, %r2313;

$L__BB2_104:
	mul.wide.u32 	%rd391, %r2319, 4;
	add.s64 	%rd392, %rd1, %rd391;
	st.global.u32 	[%rd392], %r134;
	mul.wide.u32 	%rd393, %r2319, 8;
	add.s64 	%rd394, %rd4, %rd393;
	st.global.u64 	[%rd394], %rd463;
	mul.wide.u32 	%rd395, %r2315, 4;
	add.s64 	%rd396, %rd1, %rd395;
	st.global.u32 	[%rd396], %r134;
	mul.wide.u32 	%rd397, %r2315, 8;
	add.s64 	%rd398, %rd4, %rd397;
	st.global.u64 	[%rd398], %rd459;
	add.s32 	%r2248, %r2319, %r7;
	add.s32 	%r2249, %r2248, %r7;
	add.s64 	%rd399, %rd463, %rd5;
	add.s64 	%rd400, %rd399, %rd5;
	mul.wide.u32 	%rd401, %r2316, 4;
	add.s64 	%rd402, %rd1, %rd401;
	st.global.u32 	[%rd402], %r134;
	mul.wide.u32 	%rd403, %r2316, 8;
	add.s64 	%rd404, %rd4, %rd403;
	st.global.u64 	[%rd404], %rd460;
	add.s32 	%r2250, %r2249, %r7;
	add.s64 	%rd405, %rd400, %rd5;
	mul.wide.u32 	%rd406, %r2317, 4;
	add.s64 	%rd407, %rd1, %rd406;
	st.global.u32 	[%rd407], %r134;
	mul.wide.u32 	%rd408, %r2317, 8;
	add.s64 	%rd409, %rd4, %rd408;
	st.global.u64 	[%rd409], %rd461;
	add.s32 	%r2319, %r2250, %r7;
	add.s64 	%rd463, %rd405, %rd5;
	add.s32 	%r2317, %r2317, %r126;
	add.s32 	%r2316, %r2316, %r126;
	add.s32 	%r2315, %r2315, %r126;
	add.s64 	%rd461, %rd461, %rd96;
	add.s64 	%rd460, %rd460, %rd96;
	add.s64 	%rd459, %rd459, %rd96;
	add.s32 	%r2314, %r2314, 4;
	setp.ne.s32 	%p136, %r2314, 0;
	@%p136 bra 	$L__BB2_104;

$L__BB2_105:
	setp.eq.s32 	%p137, %r128, 0;
	@%p137 bra 	$L__BB2_109;

	setp.eq.s32 	%p138, %r128, 1;
	mul.wide.u32 	%rd410, %r2319, 4;
	add.s64 	%rd411, %rd1, %rd410;
	st.global.u32 	[%rd411], %r134;
	mul.wide.u32 	%rd412, %r2319, 8;
	add.s64 	%rd413, %rd4, %rd412;
	st.global.u64 	[%rd413], %rd463;
	@%p138 bra 	$L__BB2_109;

	add.s32 	%r166, %r2319, %r7;
	add.s64 	%rd118, %rd463, %rd5;
	setp.eq.s32 	%p139, %r128, 2;
	mul.wide.u32 	%rd414, %r166, 4;
	add.s64 	%rd415, %rd1, %rd414;
	st.global.u32 	[%rd415], %r134;
	mul.wide.u32 	%rd416, %r166, 8;
	add.s64 	%rd417, %rd4, %rd416;
	st.global.u64 	[%rd417], %rd118;
	@%p139 bra 	$L__BB2_109;

	add.s32 	%r2251, %r166, %r7;
	mul.wide.u32 	%rd418, %r2251, 4;
	add.s64 	%rd419, %rd1, %rd418;
	st.global.u32 	[%rd419], %r134;
	mul.wide.u32 	%rd420, %r2251, 8;
	add.s64 	%rd421, %rd4, %rd420;
	add.s64 	%rd422, %rd118, %rd5;
	st.global.u64 	[%rd421], %rd422;

$L__BB2_109:
	add.s32 	%r2313, %r2313, %r172;
	add.s32 	%r2312, %r2312, %r172;
	add.s32 	%r2311, %r2311, 1;
	setp.lt.u32 	%p140, %r2311, %r173;
	@%p140 bra 	$L__BB2_102;

$L__BB2_110:
	add.s32 	%r2297, %r2297, -1;
	sub.s32 	%r2298, %r2298, %r176;
	setp.ge.s32 	%p141, %r2297, %r23;
	@%p141 bra 	$L__BB2_85;

$L__BB2_111:
	ret;

}
	// .globl	sieve_kernel_final_32
.visible .entry sieve_kernel_final_32(
	.param .u64 sieve_kernel_final_32_param_0,
	.param .u64 sieve_kernel_final_32_param_1,
	.param .u32 sieve_kernel_final_32_param_2,
	.param .u64 sieve_kernel_final_32_param_3,
	.param .u64 sieve_kernel_final_32_param_4,
	.param .u32 sieve_kernel_final_32_param_5
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd8, [sieve_kernel_final_32_param_0];
	ld.param.u64 	%rd9, [sieve_kernel_final_32_param_1];
	ld.param.u32 	%r20, [sieve_kernel_final_32_param_2];
	ld.param.u64 	%rd7, [sieve_kernel_final_32_param_3];
	ld.param.u64 	%rd10, [sieve_kernel_final_32_param_4];
	ld.param.u32 	%r21, [sieve_kernel_final_32_param_5];
	cvta.to.global.u64 	%rd1, %rd10;
	cvta.to.global.u64 	%rd2, %rd8;
	cvta.to.global.u64 	%rd3, %rd9;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r22, %ctaid.x;
	mov.u32 	%r23, %tid.x;
	mad.lo.s32 	%r39, %r22, %r1, %r23;
	mov.u32 	%r24, -1;
	shl.b32 	%r25, %r24, %r21;
	not.b32 	%r3, %r25;
	add.s32 	%r4, %r20, -1;
	setp.ge.u32 	%p1, %r39, %r4;
	@%p1 bra 	$L__BB3_14;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r5, %r26, %r1;
	cvta.to.global.u64 	%rd15, %rd7;

$L__BB3_2:
	cvt.u64.u32 	%rd4, %r39;
	mul.wide.u32 	%rd11, %r39, 4;
	add.s64 	%rd12, %rd3, %rd11;
	ld.global.s32 	%rd5, [%rd12];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB3_13;

	add.s32 	%r40, %r39, 1;
	setp.ge.u32 	%p3, %r40, %r20;
	@%p3 bra 	$L__BB3_13;

	shl.b64 	%rd13, %rd4, 2;
	add.s64 	%rd14, %rd2, %rd13;
	ld.global.u32 	%r27, [%rd14];
	shr.u32 	%r8, %r27, %r21;
	and.b32  	%r9, %r27, %r3;
	mul.wide.u32 	%rd16, %r8, 24;
	add.s64 	%rd6, %rd15, %rd16;

$L__BB3_5:
	mul.wide.u32 	%rd17, %r40, 4;
	add.s64 	%rd18, %rd3, %rd17;
	add.s64 	%rd19, %rd2, %rd17;
	ld.global.u32 	%r11, [%rd19];
	ld.global.u32 	%r28, [%rd18];
	cvt.u32.u64 	%r29, %rd5;
	setp.ne.s32 	%p4, %r29, %r28;
	@%p4 bra 	$L__BB3_13;

	shr.u32 	%r30, %r11, %r21;
	setp.ne.s32 	%p5, %r8, %r30;
	@%p5 bra 	$L__BB3_12;

	and.b32  	%r12, %r11, %r3;
	mov.u32 	%r41, %r9;
	mov.u32 	%r42, %r12;

$L__BB3_8:
	neg.s32 	%r31, %r42;
	and.b32  	%r32, %r42, %r31;
	clz.b32 	%r33, %r32;
	mov.u32 	%r34, 31;
	sub.s32 	%r35, %r34, %r33;
	shr.u32 	%r36, %r42, %r35;
	min.u32 	%r15, %r41, %r36;
	max.u32 	%r37, %r41, %r36;
	sub.s32 	%r42, %r37, %r15;
	setp.ne.s32 	%p6, %r42, 0;
	mov.u32 	%r41, %r15;
	@%p6 bra 	$L__BB3_8;

	setp.ne.s32 	%p7, %r15, 1;
	@%p7 bra 	$L__BB3_12;

	atom.global.add.u32 	%r17, [%rd1], 1;
	setp.gt.u32 	%p8, %r17, 998;
	@%p8 bra 	$L__BB3_12;

	mul.wide.u32 	%rd20, %r17, 32;
	add.s64 	%rd21, %rd1, %rd20;
	st.global.v2.u32 	[%rd21+32], {%r9, %r12};
	ld.global.u32 	%r38, [%rd6];
	st.global.u32 	[%rd21+40], %r38;
	ld.global.u64 	%rd22, [%rd6+16];
	st.global.u64 	[%rd21+48], %rd22;
	st.global.u64 	[%rd21+56], %rd5;

$L__BB3_12:
	add.s32 	%r40, %r40, 1;
	setp.lt.u32 	%p9, %r40, %r20;
	@%p9 bra 	$L__BB3_5;

$L__BB3_13:
	add.s32 	%r39, %r39, %r5;
	setp.lt.u32 	%p10, %r39, %r4;
	@%p10 bra 	$L__BB3_2;

$L__BB3_14:
	ret;

}
	// .globl	sieve_kernel_final_64
.visible .entry sieve_kernel_final_64(
	.param .u64 sieve_kernel_final_64_param_0,
	.param .u64 sieve_kernel_final_64_param_1,
	.param .u32 sieve_kernel_final_64_param_2,
	.param .u64 sieve_kernel_final_64_param_3,
	.param .u64 sieve_kernel_final_64_param_4,
	.param .u32 sieve_kernel_final_64_param_5
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd8, [sieve_kernel_final_64_param_0];
	ld.param.u64 	%rd9, [sieve_kernel_final_64_param_1];
	ld.param.u32 	%r20, [sieve_kernel_final_64_param_2];
	ld.param.u64 	%rd7, [sieve_kernel_final_64_param_3];
	ld.param.u64 	%rd10, [sieve_kernel_final_64_param_4];
	ld.param.u32 	%r21, [sieve_kernel_final_64_param_5];
	cvta.to.global.u64 	%rd1, %rd10;
	cvta.to.global.u64 	%rd2, %rd8;
	cvta.to.global.u64 	%rd3, %rd9;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r22, %ctaid.x;
	mov.u32 	%r23, %tid.x;
	mad.lo.s32 	%r37, %r22, %r1, %r23;
	mov.u32 	%r24, -1;
	shl.b32 	%r25, %r24, %r21;
	not.b32 	%r3, %r25;
	add.s32 	%r4, %r20, -1;
	setp.ge.u32 	%p1, %r37, %r4;
	@%p1 bra 	$L__BB4_14;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r5, %r26, %r1;
	cvta.to.global.u64 	%rd15, %rd7;

$L__BB4_2:
	cvt.u64.u32 	%rd4, %r37;
	mul.wide.u32 	%rd11, %r37, 8;
	add.s64 	%rd12, %rd3, %rd11;
	ld.global.u64 	%rd5, [%rd12];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB4_13;

	add.s32 	%r38, %r37, 1;
	setp.ge.u32 	%p3, %r38, %r20;
	@%p3 bra 	$L__BB4_13;

	shl.b64 	%rd13, %rd4, 2;
	add.s64 	%rd14, %rd2, %rd13;
	ld.global.u32 	%r27, [%rd14];
	shr.u32 	%r8, %r27, %r21;
	and.b32  	%r9, %r27, %r3;
	mul.wide.u32 	%rd16, %r8, 24;
	add.s64 	%rd6, %rd15, %rd16;

$L__BB4_5:
	mul.wide.u32 	%rd17, %r38, 8;
	add.s64 	%rd18, %rd3, %rd17;
	mul.wide.u32 	%rd19, %r38, 4;
	add.s64 	%rd20, %rd2, %rd19;
	ld.global.u32 	%r11, [%rd20];
	ld.global.u64 	%rd21, [%rd18];
	setp.ne.s64 	%p4, %rd5, %rd21;
	@%p4 bra 	$L__BB4_13;

	shr.u32 	%r28, %r11, %r21;
	setp.ne.s32 	%p5, %r8, %r28;
	@%p5 bra 	$L__BB4_12;

	and.b32  	%r12, %r11, %r3;
	mov.u32 	%r39, %r9;
	mov.u32 	%r40, %r12;

$L__BB4_8:
	neg.s32 	%r29, %r40;
	and.b32  	%r30, %r40, %r29;
	clz.b32 	%r31, %r30;
	mov.u32 	%r32, 31;
	sub.s32 	%r33, %r32, %r31;
	shr.u32 	%r34, %r40, %r33;
	min.u32 	%r15, %r39, %r34;
	max.u32 	%r35, %r39, %r34;
	sub.s32 	%r40, %r35, %r15;
	setp.ne.s32 	%p6, %r40, 0;
	mov.u32 	%r39, %r15;
	@%p6 bra 	$L__BB4_8;

	setp.ne.s32 	%p7, %r15, 1;
	@%p7 bra 	$L__BB4_12;

	atom.global.add.u32 	%r17, [%rd1], 1;
	setp.gt.u32 	%p8, %r17, 998;
	@%p8 bra 	$L__BB4_12;

	mul.wide.u32 	%rd22, %r17, 32;
	add.s64 	%rd23, %rd1, %rd22;
	st.global.v2.u32 	[%rd23+32], {%r9, %r12};
	ld.global.u32 	%r36, [%rd6];
	st.global.u32 	[%rd23+40], %r36;
	ld.global.u64 	%rd24, [%rd6+16];
	st.global.u64 	[%rd23+48], %rd24;
	st.global.u64 	[%rd23+56], %rd5;

$L__BB4_12:
	add.s32 	%r38, %r38, 1;
	setp.lt.u32 	%p9, %r38, %r20;
	@%p9 bra 	$L__BB4_5;

$L__BB4_13:
	add.s32 	%r37, %r37, %r5;
	setp.lt.u32 	%p10, %r37, %r4;
	@%p10 bra 	$L__BB4_2;

$L__BB4_14:
	ret;

}

